{
    "Name": "Paul Iusztin",
    "Posts": {
        "Post_0": {
            "text": "When ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—¶ğ—»ğ—´ ğŸ­ğŸ¬ğŸ¬+ ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—²ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜ğ˜€, I often got ğ—¼ğ˜ƒğ—²ğ—¿ğ˜„ğ—µğ—²ğ—¹ğ—ºğ—²ğ—± by which one to ğ—½ğ—¶ğ—°ğ—¸.\nUntil I started ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ these ğŸ¯ ğ˜€ğ—¶ğ—ºğ—½ğ—¹ğ—² ğ˜ğ—¿ğ—¶ğ—°ğ—¸ğ˜€ â†“\n#ğŸ­. ğ—•ğ—®ğ˜€ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹\nYou need a reference point to compare your model results with.\nOtherwise, the computed metrics are hard to interpret.\n-> For example, you are training a time series forecaster.\nThe base model always predicts the last value. If your \"smart\" model can't outperform that, you are better off without it.\n#ğŸ®. ğ—¦ğ—¹ğ—¶ğ—°ğ—¶ğ—»ğ—´\nAggregated metrics are often misleading (the mean over all the testing samples).\nSlicing your testing dataset by features of interest such as gender, age, demographics, etc., can bring to the surface issues such as:\n- bias\n- weakness points\n- relationships between inputs & outputs (aka explainability), etc.\n-> For example, your model can have extraordinary results in the [18, 30] age range but terrible ones in [30+, inf].\nThe aggregated metrics look great because most data samples are within the [18, 30] range. But in reality, your model fails the minority groups.\nThus, even though the aggregated metrics look great, you may deploy a broken model.\nTools: Snorkel\n#ğŸ¯. ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜ ğ—§ğ—¿ğ—®ğ—°ğ—¸ğ—²ğ—¿\nYou just run 100+ experiments using different models and hyperparameters.\nYou already have your base model and slicing techniques set in place.\nHow can you easily compare these experiments?\nYou can quickly aggregate the results using an experiment tracker in a single graph(s).\nThus, you have the big picture to pick the best experiment and its metadata easily.\nTools: Comet ML, W&B, MLFlow, Neptune\n.\nTo conclude...\nTo quickly compare many experiments, you need:\n- a base model\n- to slice your testing split\n- an experiment tracker\nDo you recommend other tricks to improve your evaluation process?\n.\nIf you are curious to read more, check out my hands-on article:\nâ†³ğŸ”— ğ˜ğ˜¶ğ˜ªğ˜¥ğ˜¦ ğ˜µğ˜° ğ˜‰ğ˜¶ğ˜ªğ˜­ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜Œğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜—ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜”ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® ğ˜™ğ˜¦ğ˜´ğ˜¶ğ˜­ğ˜µğ˜´:\nhttps://lnkd.in/dXhmT9aV\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQGQ_AZb7aHRMA/feedshare-shrink_800/0/1704439990187?e=1707350400&v=beta&t=rn4-kvvz0aN-LWd5Teob7rUtKfRBP4GX6I01vNETI0c"
        },
        "Post_1": {
            "text": "Ever thought about how ğ—® ğ˜†ğ—²ğ—®ğ—¿ can ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜ğ—²ğ—¹ğ˜† ğ—°ğ—µğ—®ğ—»ğ—´ğ—² your ğ˜ƒğ—¶ğ˜€ğ—¶ğ—¼ğ—»? ğŸ®ğŸ¬ğŸ®ğŸ¯ did just that for me...\nI don't really care about New Year's Eve. It is just another trip around the sun.\n...but it is a great moment to pause and think about the last year and the future.\nPersonally, I love planning things and learning from my past. It helps me gain perspective and prioritize the important things in my life.\n.\nğ˜šğ˜° ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜¢ğ˜³ğ˜¦ 6 ğ˜µğ˜©ğ˜ªğ˜¯ğ˜¨ğ˜´ ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜©ğ˜¢ğ˜±ğ˜±ğ˜¦ğ˜¯ğ˜¦ğ˜¥ ğ˜ªğ˜¯ 2023 ğ˜µğ˜©ğ˜¢ğ˜µ ğ˜ ğ˜¢ğ˜® ğ˜¨ğ˜³ğ˜¢ğ˜µğ˜¦ğ˜§ğ˜¶ğ˜­ ğ˜§ğ˜°ğ˜³:\n1. Enough discipline to work out almost daily & eat healthy.\n2. Grew my MLE & MLOps content creation business.\n3. Created x2 open-source MLE & MLOps free courses that accumulated >1800 stars on GitHub.\n4. Met many incredible MLE & MLOps people from Europe and the US.\n5. Had the chance to work on awesome deep fake projects in production at Metaphysic\n6. Adopted my second cat: Arthur. I love this little fellow. Now we are a happy 2 people - 2 cat family.\n...ğ˜¢ğ˜¯ğ˜¥ ğ˜®ğ˜º ğ˜µğ˜°ğ˜± 6 ğ˜µğ˜©ğ˜ªğ˜¯ğ˜¨ğ˜´ ğ˜ ğ˜±ğ˜­ğ˜¢ğ˜¯ ğ˜µğ˜° ğ˜¥ğ˜° ğ˜ªğ˜¯ 2024:\n1. Grow the ğ——ğ—²ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ— ğ—Ÿ ğ—½ğ˜‚ğ—¯ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—». I have some exciting news here that I will share in the following days.\n2. Travel & work from a different country every 3-4 months to level up my social and emotional skills while exploring the world.\n3. Level up my MLE & MLOps production-ready skills on real-world projects to share better insights with you.\n4. Better understand myself and be 100% true to myself.\n5. Start creating video content.\n6. This is a grand one: Move to the city center to leave my house more.\n.\nMy final take is that you should always take care of your mind & body as much as your tech skills.\nIf you degrade, your skills degrade with you.\nLet the games begin!\nWhat are your main goals for 2024, or what are you grateful for from last year?\n.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQGUsi3lpqdeWQ/feedshare-shrink_800/0/1704353488146?e=1707350400&v=beta&t=U3svlKOKn7BQOSiGE3lMXJMOHQUhCPfx_hejoRpxqU8"
        },
        "Post_2": {
            "text": "Do you want to ğ—¹ğ—²ğ˜ƒğ—²ğ—¹ ğ˜‚ğ—½ your ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—»-ğ—¿ğ—²ğ—®ğ—±ğ˜† ğ— ğ—Ÿğ—˜ & ğ— ğ—Ÿğ—¢ğ—½ğ˜€ game?\nThen I have some great news ğŸ”¥\nSince I started creating content, I learned one crucial thing: \"ğ˜Œğ˜·ğ˜¦ğ˜³ğ˜ºğ˜£ğ˜°ğ˜¥ğ˜º ğ˜­ğ˜ªğ˜¬ğ˜¦ğ˜´ ğ˜µğ˜° ğ˜³ğ˜¦ğ˜¢ğ˜¥ ğ˜¢ğ˜¯ğ˜¥ ğ˜­ğ˜¦ğ˜¢ğ˜³ğ˜¯ ğ˜¥ğ˜ªğ˜§ğ˜§ğ˜¦ğ˜³ğ˜¦ğ˜¯ğ˜µğ˜­ğ˜º.\"\nThat is why I ğ˜€ğ˜ğ—®ğ—¿ğ˜ğ—²ğ—± my own ğ— ğ—²ğ—±ğ—¶ğ˜‚ğ—º ğ—½ğ˜‚ğ—¯ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—», called after my newsletter: \"ğ˜‹ğ˜¦ğ˜¤ğ˜°ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜”ğ˜“\"\nStarting from now, all my articles can be found under this Medium publication: ğŸ”—\nhttps://lnkd.in/dTizqHG7\nCurrently, it is empty, but in January 2024, I plan to drop ğŸµ ğ˜€ğ˜‚ğ—¿ğ—½ğ—¿ğ—¶ğ˜€ğ—²ğ˜€ in my new ğ——ğ—²ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ— ğ—Ÿ ğ— ğ—²ğ—±ğ—¶ğ˜‚ğ—º ğ—½ğ˜‚ğ—¯ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—».\nIf you have liked my content so far...\n-> Support me and follow my new publication as you will enjoy what I prepared: ğŸ”—\nhttps://lnkd.in/dTizqHG7\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQEmn1alF-aNGw/feedshare-shrink_800/0/1704267126276?e=1707350400&v=beta&t=Nv2P0OMYi5bO9kBY6RZzjX5OpG172UdYeR5fnPVoI8U"
        },
        "Post_3": {
            "text": "2023 was crazy. Here are my ğ—¿ğ—²ğ˜€ğ˜‚ğ—¹ğ˜ğ˜€ after ğ—¼ğ—»ğ—² ğ˜†ğ—²ğ—®ğ—¿ of ğ˜„ğ—¿ğ—¶ğ˜ğ—¶ğ—»ğ—´ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜ about ğ— ğ—Ÿğ—˜ and ğ— ğ—Ÿğ—¢ğ—½ğ˜€ and starting as a nobody.\n...not that now I am somebody ğŸ˜‚\nI had some humble goals. At the beginning of 2022, I hadn't imagined I would plan to make a living out of creating content:\n- ğ—Ÿğ—¶ğ—»ğ—¸ğ—²ğ—±ğ—œğ—»: 3k -> 22.5k followers (goal 10k)\n- ğ— ğ—²ğ—±ğ—¶ğ˜‚ğ—º: 200 -> 1.3k followers (goal 1k)\n...but I stumbled on some great sources of inspiration and people who helped me grow more than I could have imagined.\nThus, during 2023, I got excited and experimented with quite a few things, such as:\n- ğ˜…ğŸ® ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—²ğ˜€ about MLOps, LLMOps, and MLE that accumulated over 1800 GitHub stars:\n-> ğ˜ğ˜¢ğ˜¯ğ˜¥ğ˜´-ğ˜°ğ˜¯ ğ˜“ğ˜“ğ˜”ğ˜´:\nhttps://lnkd.in/dZgqtf8f\n-> ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¶ğ˜­ğ˜­ ğ˜šğ˜µğ˜¢ğ˜¤ğ˜¬ 7-ğ˜šğ˜µğ˜¦ğ˜±ğ˜´ ğ˜”ğ˜“ğ˜–ğ˜±ğ˜´ ğ˜ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜¸ğ˜°ğ˜³ğ˜¬:\nhttps://lnkd.in/d5HUN39Q\n- I started my newsletter: \"ğ——ğ—²ğ—°ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ— ğ—Ÿ\", which grew to 2.8k followers\n- I tried to grow my ğ—§ğ˜„ğ—¶ğ˜ğ˜ğ—²ğ—¿/ğ—« account (only 233 followers): this was quite a mess as I haven't    tweaked my posts well enough to satisfy the\n.\n2023 ğ™¬ğ™–ğ™¨ ğ™–ğ™£ ğ™šğ™­ğ™˜ğ™ğ™©ğ™ğ™£ğ™œ ğ™®ğ™šğ™–ğ™§, ğ™–ğ™£ğ™™ ğ™„ ğ™ğ™–ğ™«ğ™š ğ™¨ğ™¤ğ™¢ğ™š ğ™©ğ™ğ™§ğ™ğ™¡ğ™¡ğ™ğ™£ğ™œ ğ™¥ğ™¡ğ™–ğ™£ğ™¨ ğ™›ğ™¤ğ™§ 2024.\nBut...\nI want to thank everybody who followed me and engaged with my content. You are one of the first drivers that keep me going. So... Thank you ğŸ™\nSecondly, setting such goals isn't that essential. What if I haven't reached out to any of them? That would have demoralized me entirely. But, I think they are critical in giving a clear direction.\nRemember: \"Direction is more important than... anything.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQHhZmHkyXrrsQ/feedshare-shrink_800/0/1704180681017?e=1707350400&v=beta&t=gcNoiHATosFLEJeLjcvHQMLWTcHonktjxdcij57Qbpw"
        },
        "Post_4": {
            "text": "If anyone told you that ğ— ğ—Ÿ or ğ— ğ—Ÿğ—¢ğ—½ğ˜€ is ğ—²ğ—®ğ˜€ğ˜†, they were ğ—¿ğ—¶ğ—´ğ—µğ˜.\nHere is a simple trick that I learned the hard way â†“\nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is â†’ \"ğ—™ğ—¼ğ—°ğ˜‚ğ˜€ ğ—¼ğ—» ğ˜ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ğ˜€.\"\n.\nLet me explain â†“\nWhen you constantly chase the latest models (aka FOMO), you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQG1WqQlxxnj7A/image-shrink_800/0/1703057415998?e=1705082400&v=beta&t=N1j36QNO1kWqukgpJt1giHoIjlb7A6jOUTfEOmrtcWs"
        },
        "Post_5": {
            "text": "ğ—§ğ—µğ—² ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ FREE ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² just ğ—½ğ—®ğ˜€ğ˜€ğ—²ğ—± 750+ ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ â­ğŸŒŸâ­\nIf you want to ğ¥ğğšğ«ğ§ for FREE to ğ›ğ®ğ¢ğ¥ğ ğ¡ğšğ§ğğ¬-ğ¨ğ§ ğ‹ğ‹ğŒ ğ¬ğ²ğ¬ğ­ğğ¦ğ¬ using good LLMOps principles, this might be something for you.\n.\nA big ğ—§ğ—µğ—®ğ—»ğ—¸ ğ˜†ğ—¼ğ˜‚! ...for everyone who supported the GitHub repo. This means a lot to me.\nAlso, I want to thank Pau Labarta and Alexandru Razvant for this fantastic collaboration and for making this course possible.\n.\nğ˜ğ˜°ğ˜³ ğ˜µğ˜©ğ˜¦ ğ˜±ğ˜¦ğ˜°ğ˜±ğ˜­ğ˜¦ ğ˜¸ğ˜©ğ˜° ğ˜¥ğ˜°ğ˜¯'ğ˜µ ğ˜¬ğ˜¯ğ˜°ğ˜¸, ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜ªğ˜´ ğ˜´ğ˜°ğ˜®ğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜¢ğ˜£ğ˜°ğ˜¶ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜°ğ˜¶ğ˜³ğ˜´ğ˜¦ â†“\nğ—§ğ—µğ—² ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¢ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ course is not just another demo of how to make a few predictions in a notebook.\nWe will primarily focus on the engineering & MLOps aspects.\nYou'll walk away with a ğ—³ğ˜‚ğ—¹ğ—¹ğ˜† ğ—¼ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜, leveraging Large Language Models (LLMs), LLMOps, and the 3-pipeline design to build a chatbot for financial investment advice.\nThese are ğ­ğ¡ğ 3 ğœğ¨ğ¦ğ©ğ¨ğ§ğğ§ğ­ğ¬ ğ²ğ¨ğ® ğ°ğ¢ğ¥ğ¥ ğ¥ğğšğ«ğ§ ğ­ğ¨ ğ›ğ®ğ¢ğ¥ğ during the course â†“\n1. a ğ«ğğšğ¥-ğ­ğ¢ğ¦ğ ğ¬ğ­ğ«ğğšğ¦ğ¢ğ§ğ  ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DB\n2. a ğŸğ¢ğ§ğ-ğ­ğ®ğ§ğ¢ğ§ğ  ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n3. an ğ¢ğ§ğŸğğ«ğğ§ğœğ ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nWe will also show you how to integrate various serverless tools, such as:\nâ€¢ Comet ML as your ML Platform;\nâ€¢ Qdrant as your vector DB;\nâ€¢ Beam as your infrastructure.\n.\nğ–ğ¡ğ¨ ğ¢ğ¬ ğ­ğ¡ğ¢ğ¬ ğŸğ¨ğ«?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\nğ‡ğ¨ğ° ğ°ğ¢ğ¥ğ¥ ğ²ğ¨ğ® ğ¥ğğšğ«ğ§?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\n.\nğ‚ğ®ğ«ğ¢ğ¨ğ®ğ¬?\nâ†³ Check out the ğ—§ğ—µğ—² ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ FREE course and support us with a â­: ğŸ”—\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFJY7MXbxX_gA/image-shrink_800/0/1702971017234?e=1705082400&v=beta&t=zvhexAVrC7qkRWhYmvV2qRfnFqe5LpIiUqwRVcn38Ao"
        },
        "Post_6": {
            "text": "ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ—°ğ—µğ—®ğ—¿ğ—´ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ— ğ—Ÿ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º: ğ˜‚ğ˜€ğ—² ğ—® ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¿ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\nA model registry is the holy grail of any production-ready ML system.\nThe model registry is the critical component that decouples your offline pipeline (experimental/research phase) from your production pipeline.\nğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—¢ğ—³ğ—³ğ—¹ğ—¶ğ—»ğ—² ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€\nUsually, when training your model, you use a static data source.\nUsing a feature engineering pipeline, you compute the necessary features used to train the model.\nThese features will be stored inside a features store.\nAfter processing your data, your training pipeline creates the training & testing splits and starts training the model.\nThe output of your training pipeline is the trained weights, also known as the model artifact.\nğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜„ğ—µğ—²ğ—¿ğ—² ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¿ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜† ğ—¸ğ—¶ğ—°ğ—¸ğ˜€ ğ—¶ğ—» â†“\nThis artifact will be pushed into the model registry under a new version that can easily be tracked.\nSince this point, the new model artifact version can be pulled by any serving strategy:\n#1. batch\n#2. request-response\n#3. streaming\nYour inference pipeline doesnâ€™t care how the model artifact was generated. It just has to know what model to use and how to transform the data into features.\nNote that this strategy is independent of the type of model & hardware you use:\n- classic model (Sklearn, XGboost),\n- distributed system (Spark),\n- deep learning model (PyTorch)\nTo summarize...\nUsing a model registry is a simple and effective method to:\n-> detach your experimentation from your production environment,\nregardless of what framework or hardware you use.\n.\nTo learn more, check out my hands-on article:\nâ†³ğŸ”— ğ˜ğ˜¶ğ˜ªğ˜¥ğ˜¦ ğ˜µğ˜° ğ˜‰ğ˜¶ğ˜ªğ˜­ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜Œğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜—ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜”ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® ğ˜™ğ˜¦ğ˜´ğ˜¶ğ˜­ğ˜µğ˜´:\nhttps://lnkd.in/d85u_wwN\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQHdEuVbu2i35g/feedshare-shrink_800/0/1702633568332?e=1707350400&v=beta&t=R031162rqsBad0glmrfjFZeZcAPIDHkp6ZR-YyW4324"
        },
        "Post_7": {
            "text": "Have you ever presented your MLOps ideas to upper management just to get ghosted?\nIn that case...\nRaphaÃ«l Hoogvliets\n,\nBaÅŸak TuÄŸÃ§e Eskili\n, and\nMaria Vechtomova\nfrom\nMarvelous MLOps\npresented a great step-by-step strategy for pitching your MLOps ideas to your upper management and getting attention and resources to implement them.\nHere are the 6 steps you have to know â†“\n1. ğ‚ğ¨ğ¥ğ¥ğğœğ­ ğšğ¥ğ¥ ğ­ğ¡ğ ğ©ğšğ¢ğ§ ğ©ğ¨ğ¢ğ§ğ­ğ¬\nTalk to data scientists, product owners, and stakeholders in your organization to gather issues such as:\n- time to deployment\n- poor quality deployment\n- non-existing monitoring\n- lack of collaboration\n- external parties\n2. ğ„ğğ®ğœğšğ­ğ ğ©ğğ¨ğ©ğ¥ğ\nOrganize workshops, meetings, etc., to present what MLOps is and how it can help.\nI think it's critical to present it to your target audience. For example, an engineer looks at the problem differently than the business stakeholders.\n3. ğğ«ğğ¬ğğ§ğ­ ğ›ğğŸğ¨ğ«ğ ğšğ§ğ ğšğŸğ­ğğ« ğ¬ğœğğ§ğšğ«ğ¢ğ¨ğ¬\nShow how MLOps can solve the company's challenges and deliver tangible benefits to the organization, such as:\n- less cost\n- fast deployment\n- better collaboration\n- less risk\n4. ğğ«ğ¨ğ¯ğ ğ¢ğ­\nUse concrete examples to support your ideas, such as:\n- how a competitor or an organization in the same or related field benefited from introducing MLOps\n- build a PoC within your organization\n5. ğ’ğğ­ ğ®ğ© ğ²ğ¨ğ®ğ« ğ­ğğšğ¦\nChoose 2-3 experienced individuals (not juniors) to set up the foundations in your team/organization.\nWith an emphasis on starting with experienced engineers and only later bringing more juniors to the party.\n6. ğŠğğğ© ğ¨ğ§ ğ¤ğğğ©ğ¢ğ§' ğ¨ğ§\nOnce you successfully apply MLOps to one use case, you can bring in more responsibility by growing your team and taking on more projects.\n.\nAll of these are great tips for integrating MLOps in your organization.\nI love their \"Present before and after scenarios\" approach.\nYou can extrapolate this strategy for any other new processes (not only MLOps).\n.\nâ†³ To learn the details, check out Marvelous MLOps full article at ğŸ”—\nhttps://lnkd.in/dBesYGBv\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFmfyzSQixw0Q/image-shrink_800/0/1702625411189?e=1705082400&v=beta&t=xhqh9DVW5UD6gkaiL40ITlUpZIGrunobF6XJEuFDtSo"
        },
        "Post_8": {
            "text": "Next Tuesday, 19 December, at 17:30 CET, I am pleased to participate in the 2nd edition of ğ—”ğ˜€ğ—¸\nMarvelous MLOps\nğ—”ğ—»ğ˜†ğ˜ğ—µğ—¶ğ—»ğ—´: a live Q&A session where we try to answer all your questions around\nhashtag\n#\nmlops\nand\nhashtag\n#\nllmops\n.\nIn the spirit of Christmas, you can ask my magic floating head anything, and I will grant you hands-on answers.\nâ†³ Join the event here: ğŸ”—\nhttps://lnkd.in/d_bu2RJv\nSee you there ğŸ‘€\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQF1V9ProdQ5nA/image-shrink_800/0/1702539012444?e=1705082400&v=beta&t=Y0GV9S91L-gVcFMbupr9qEbB2dAn-vMBsgmZuwsJXCc"
        },
        "Post_9": {
            "text": "Here are 3 techniques you must know to evaluate your LLMs quickly.\nManually testing the output of your LLMs is a tedious and painful process â†’ you need to automate it.\nIn generative AI, most of the time, you cannot leverage standard metrics.\nThus, the real question is, how do you evaluate the outputs of an LLM?\nDepending on your problem, here is what you can do â†“\n#ğŸ­. ğ—¦ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ—± ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ğ˜€ - ğ˜†ğ—¼ğ˜‚ ğ—¸ğ—»ğ—¼ğ˜„ ğ—²ğ˜…ğ—®ğ—°ğ˜ğ—¹ğ˜† ğ˜„ğ—µğ—®ğ˜ ğ˜†ğ—¼ğ˜‚ ğ˜„ğ—®ğ—»ğ˜ ğ˜ğ—¼ ğ—´ğ—²ğ˜\nEven if you use an LLM to generate text, you can ask it to generate a response in a structured format (e.g., JSON) that can be parsed.\nYou know exactly what you want (e.g., a list of products extracted from the user's question).\nThus, you can easily compare the generated and ideal answers using classic approaches.\nFor example, when extracting the list of products from the user's input, you can do the following:\n- check if the LLM outputs a valid JSON structure\n- use a classic method to compare the generated and real answers\n#ğŸ®. ğ—¡ğ—¼ \"ğ—¿ğ—¶ğ—´ğ—µğ˜\" ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ (ğ—².ğ—´., ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—»ğ—´ ğ—±ğ—²ğ˜€ğ—°ğ—¿ğ—¶ğ—½ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ˜€ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ—¶ğ—²ğ˜€, ğ—²ğ˜ğ—°.)\nWhen generating sentences, the LLM can use different styles, words, etc. Thus, traditional metrics (e.g., BLUE score) are too rigid to be useful.\nYou can leverage another LLM to test the output of our initial LLM. The trick is in what questions to ask.\nWhen testing LLMs, you won't have a big testing split size as you are used to. A set of 10-100 tricky examples usually do the job (it won't be costly).\nHere, we have another 2 sub scenarios:\nâ†³ ğŸ®.ğŸ­ ğ—ªğ—µğ—²ğ—» ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ğ—»'ğ˜ ğ—µğ—®ğ˜ƒğ—² ğ—®ğ—» ğ—¶ğ—±ğ—²ğ—®ğ—¹ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ (ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ğ—»'ğ˜ ğ—µğ—®ğ˜ƒğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ)\nYou don't have access to an expert to write an ideal answer for a given question to compare it to.\nBased on the initial prompt and generated answer, you can compile a set of questions and pass them to an LLM. Usually, these are Y/N questions that you can easily quantify and check the validity of the generated answer.\nThis is known as \"Rubric Evaluation\"\nFor example:\n\"\"\"\n- Is there any disagreement between the response and the context? (Y or N)\n- Count how many questions the user asked. (output a number)\n...\n\"\"\"\nThis strategy is intuitive, as you can ask the LLM any question you are interested in as long it can output a quantifiable answer (Y/N or a number).\nâ†³ ğŸ®.ğŸ®. ğ—ªğ—µğ—²ğ—» ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ ğ—µğ—®ğ˜ƒğ—² ğ—®ğ—» ğ—¶ğ—±ğ—²ğ—®ğ—¹ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—¿ğ—²ğ˜€ğ—½ğ—¼ğ—»ğ˜€ğ—² ğ˜ğ—¼ (ğ˜†ğ—¼ğ˜‚ ğ—µğ—®ğ˜ƒğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ)\nWhen you have access to an answer manually created by a group of experts, things are easier.\nYou will use an LLM to compare the generated and ideal answers based on semantics, not structure.\nFor example:\n\"\"\"\n(A) The submitted answer is a subset of the expert answer and entirely consistent.\n...\n(E) The answers differ, but these differences don't matter.\n\"\"\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D5610AQFoO9A1JvNRJw/image-shrink_800/0/1702452603385?e=1705082400&v=beta&t=AK8KePlsAkCVzWKPcGzCVisrINAayNL9hS9aOG_eyrw"
        },
        "Post_10": {
            "text": "What is the ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² between your ğ— ğ—Ÿ ğ—±ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—ºğ—²ğ—»ğ˜ and ğ—°ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—²ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜ğ˜€?\nThey might do the same thing, but their design is entirely different â†“\nğ— ğ—Ÿ ğ——ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—ºğ—²ğ—»ğ˜ ğ—˜ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜\nAt this point, your main goal is to ingest the raw and preprocessed data through versioned artifacts (or a feature store), analyze it & generate as many experiments as possible to find the best:\n- model\n- hyperparameters\n- augmentations\nBased on your business requirements, you must maximize some specific metrics, find the best latency-accuracy trade-offs, etc.\nYou will use an experiment tracker to compare all these experiments.\nAfter you settle on the best one, the output of your ML development environment will be:\n- a new version of the code\n- a new version of the configuration artifact\nHere is where the research happens. Thus, you need flexibility.\nThat is why we decouple it from the rest of the ML systems through artifacts (data, config, & code artifacts).\nğ—–ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—˜ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜\nHere is where you want to take the data, code, and config artifacts and:\n- train the model on all the required data\n- output a staging versioned model artifact\n- test the staging model artifact\n- if the test passes, label it as the new production model artifact\n- deploy it to the inference services\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub Actions):\n- builds a docker image from the code artifact (e.g., triggered manually or when a new artifact version is created)\n- start the training pipeline inside the docker container that pulls the feature and config artifacts and outputs the staging model artifact\n- manually look over the training report -> If everything went fine, manually trigger the testing pipeline\n- manually look over the testing report -> if everything worked fine (e.g., the model is better than the previous one), manually trigger the CD pipeline that deploys the new model to your inference services\nNote how the model registry quickly helps you to decouple all the components.\nAlso, because training and testing metrics are not always black & white, it is tough to 100% automate the CI/CD pipeline.\nThus, you need a human in the loop when deploying ML models.\nTo conclude...\nThe ML development environment is where you do your research to find better models:\n- ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ: data artifact\n- ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µ: code & config artifacts\nThe continuous training environment is used to train & test the production model at scale:\n- ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ: data, code, config artifacts\n- ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µ: model artifact\n.\nâ†³ See this strategy in action in my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course: ğŸ”—\nhttps://lnkd.in/d_GVpZ9X\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQEdpFdpJSlDKQ/image-shrink_800/0/1701156624205?e=1705082400&v=beta&t=jxPE3kyWPThjTX_XDPpcMOeSnaBplBodZgaU5ukMN3c"
        },
        "Post_11": {
            "text": "I am excited to announce that I will participate in a live Q&A session hosted by\nMarvelous MLOps\n.\nIn case you want to ask me anything. See you on Tuesday (19th of December) ğŸ‘€\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQGA4rQNkKGyfw/image-shrink_800/0/1701070219962?e=1705082400&v=beta&t=g6vq_5Wkf8n8BkWmqMlXpViOvcdwHPfFBRO6zacGl8g"
        },
        "Post_12": {
            "text": "Next Tuesday we'll have the 2nd edition of ğ—”ğ˜€ğ—¸ ğ— ğ—®ğ—¿ğ˜ƒğ—²ğ—¹ğ—¼ğ˜‚ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—”ğ—»ğ˜†ğ˜ğ—µğ—¶ğ—»ğ—´:\nA live Q&A sessions where we try to answer all your questions ğŸ¤—\nThis time\nPaul Iusztin\nwill join us, sign up here:\nhttps://lnkd.in/eVk7e8k5",
            "image": "https://media.licdn.com/dms/image/D4D10AQG1TheTmWHOVQ/image-shrink_800/0/1700724626687?e=1705082400&v=beta&t=SMspEABg6mO8NCZxwfZGsLYx-KoRkXdNTyZYrCRTshE"
        },
        "Post_13": {
            "text": "If you want to ğ¥ğğšğ«ğ§ for FREE to ğ›ğ®ğ¢ğ¥ğ ğ¡ğšğ§ğğ¬-ğ¨ğ§ ğ‹ğ‹ğŒ ğ¬ğ²ğ¬ğ­ğğ¦ğ¬ using good LLMOps principles, we want to announce that we just ğŸğ¢ğ§ğ¢ğ¬ğ¡ğğ the code & video lessons for the \"ğ‡ğšğ§ğğ¬-ğ¨ğ§ ğ‹ğ‹ğŒğ¬\" ğœğ¨ğ®ğ«ğ¬ğ.\nBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.\nWe will primarily focus on the engineering & MLOps aspects.\nThus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks.\nğŒğ¨ğ«ğ ğ©ğ«ğğœğ¢ğ¬ğğ¥ğ², ğ­ğ¡ğğ¬ğ ğšğ«ğ ğ­ğ¡ğ 3 ğœğ¨ğ¦ğ©ğ¨ğ§ğğ§ğ­ğ¬ ğ²ğ¨ğ® ğ°ğ¢ğ¥ğ¥ ğ¥ğğšğ«ğ§ ğ­ğ¨ ğ›ğ®ğ¢ğ¥ğ:\n1.  a ğ«ğğšğ¥-ğ­ğ¢ğ¦ğ ğ¬ğ­ğ«ğğšğ¦ğ¢ğ§ğ  ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DB\n2.  a ğŸğ¢ğ§ğ-ğ­ğ®ğ§ğ¢ğ§ğ  ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n3.  an ğ¢ğ§ğŸğğ«ğğ§ğœğ ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)\nWe will also show you how to integrate various serverless tools, such as:\nâ€¢ Comet ML as your ML Platform;\nâ€¢ Qdrant as your vector DB;\nâ€¢ Beam as your infrastructure.\nğ–ğ¡ğ¨ ğ¢ğ¬ ğ­ğ¡ğ¢ğ¬ ğŸğ¨ğ«?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\nğ‡ğ¨ğ° ğ°ğ¢ğ¥ğ¥ ğ²ğ¨ğ® ğ¥ğğšğ«ğ§?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\nğ‚ğ®ğ«ğ¢ğ¨ğ®ğ¬?\nâ†³ Check it out and support us with a â­:  ğŸ”—\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGFh8AbplRcZQ/image-shrink_800/0/1700638220231?e=1705082400&v=beta&t=M3cW_Kzw-1gWhLQEYoYkiJRIdfktmlp700RZWry_SCc"
        },
        "Post_14": {
            "text": "How do you ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² a ğ˜€ğ˜†ğ—»ğ˜ğ—µğ—²ğ˜ğ—¶ğ—° ğ—±ğ—¼ğ—ºğ—®ğ—¶ğ—»-ğ˜€ğ—½ğ—²ğ—°ğ—¶ğ—³ğ—¶ğ—° ğ—¤&ğ—” ğ—±ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜ in <ğŸ¯ğŸ¬ ğ—ºğ—¶ğ—»ğ˜‚ğ˜ğ—²ğ˜€ to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² your ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ— ?\nThis method is also known as ğ—³ğ—¶ğ—»ğ—²ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ˜„ğ—¶ğ˜ğ—µ ğ—±ğ—¶ğ˜€ğ˜ğ—¶ğ—¹ğ—¹ğ—®ğ˜ğ—¶ğ—¼ğ—». Here are its 3 ğ˜®ğ˜¢ğ˜ªğ˜¯ ğ˜´ğ˜µğ˜¦ğ˜±ğ˜´ â†“\nğ˜ğ˜°ğ˜³ ğ˜¦ğ˜¹ğ˜¢ğ˜®ğ˜±ğ˜­ğ˜¦, ğ˜­ğ˜¦ğ˜µ'ğ˜´ ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜³ğ˜¢ğ˜µğ˜¦ ğ˜¢ ğ˜˜&ğ˜ˆ ğ˜§ğ˜ªğ˜¯ğ˜¦-ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¥ğ˜¢ğ˜µğ˜¢ğ˜´ğ˜¦ğ˜µ ğ˜¶ğ˜´ğ˜¦ğ˜¥ ğ˜µğ˜° ğ˜§ğ˜ªğ˜¯ğ˜¦-ğ˜µğ˜¶ğ˜¯ğ˜¦ ğ˜¢ ğ˜§ğ˜ªğ˜¯ğ˜¢ğ˜¯ğ˜¤ğ˜ªğ˜¢ğ˜­ ğ˜¢ğ˜¥ğ˜·ğ˜ªğ˜´ğ˜°ğ˜³ ğ˜“ğ˜“ğ˜”.\nğ—¦ğ˜ğ—²ğ—½ ğŸ­: ğ— ğ—®ğ—»ğ˜‚ğ—®ğ—¹ğ—¹ğ˜† ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² ğ—® ğ—³ğ—²ğ˜„ ğ—¶ğ—»ğ—½ğ˜‚ğ˜ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€\nGenerate a few input samples (~3) that have the following structure:\n- ğ˜¶ğ˜´ğ˜¦ğ˜³_ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ: describe the type of investor (e.g., \"I am a 28-year-old marketing professional\")\n- ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µğ˜ªğ˜°ğ˜¯: describe the user's intention (e.g., \"Is Bitcoin a good investment option?\")\nğ—¦ğ˜ğ—²ğ—½ ğŸ®: ğ—˜ğ˜…ğ—½ğ—®ğ—»ğ—± ğ˜ğ—µğ—² ğ—¶ğ—»ğ—½ğ˜‚ğ˜ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€ ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—µğ—²ğ—¹ğ—½ ğ—¼ğ—³ ğ—® ğ˜ğ—²ğ—®ğ—°ğ—µğ—²ğ—¿ ğ—Ÿğ—Ÿğ— \nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up to +N similar input examples.\nWe generated 100 input examples in our use case, but you can generate more.\nYou will use the manually filled input examples to do few-shot prompting.\nThis will guide the LLM to give you domain-specific samples.\nğ˜›ğ˜©ğ˜¦ ğ˜±ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µ ğ˜¸ğ˜ªğ˜­ğ˜­ ğ˜­ğ˜°ğ˜°ğ˜¬ ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜µğ˜©ğ˜ªğ˜´:\n\"\"\"\n...\nGenerate 100 more examples with the following pattern:\n# USER CONTEXT 1\n...\n# QUESTION 1\n...\n# USER CONTEXT 2\n...\n\"\"\"\nğ—¦ğ˜ğ—²ğ—½ ğŸ¯: ğ—¨ğ˜€ğ—² ğ˜ğ—µğ—² ğ˜ğ—²ğ—®ğ—°ğ—µğ—²ğ—¿ ğ—Ÿğ—Ÿğ—  ğ˜ğ—¼ ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² ğ—¼ğ˜‚ğ˜ğ—½ğ˜‚ğ˜ğ˜€ ğ—³ğ—¼ğ—¿ ğ—®ğ—¹ğ—¹ ğ˜ğ—µğ—² ğ—¶ğ—»ğ—½ğ˜‚ğ˜ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€\nNow, you will have the same powerful LLM as a teacher, but this time, it will answer all your N input examples.\nBut first, to introduce more variance, we will use RAG to enrich the input examples with news context.\nAfterward, we will use the teacher LLM to answer all N input examples.\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual work.\n.\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a niched task, such as financial advising.\nThis technique is known as finetuning with distillation because you use a powerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data, which will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as the student.\nâœ’ï¸ ğ˜•ğ˜°ğ˜µğ˜¦: To ensure that the generated data is of high quality, you can hire a domain expert to check & refine it.\n.\nâ†³ To learn more about this technique, check out this article from Pau Labarta's RLML newsletter: ğŸ”—\nhttps://lnkd.in/d4m9pbn5\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGEejtz6vXI9w/image-shrink_800/0/1700551829710?e=1705082400&v=beta&t=aeWfGt6XqnDaSULDRD71dv9Rzaeli0QOFk7hpMKf3Sg"
        },
        "Post_15": {
            "text": "We just released the 3rd Lesson of the free Hands-on LLMs course. If you want to learn how to build a streaming pipeline for real-time text embedding, check it out â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\nmlops\nhashtag\n#\nllm",
            "image": "https://media.licdn.com/dms/image/D4D22AQGi5fXZelxn5A/feedshare-shrink_800/0/1700299851370?e=1707350400&v=beta&t=6HjAC5cnp7ExqbkCbQ1k7u87840_LKbHXwiqxtdf5Hs"
        },
        "Post_16": {
            "text": "ğŸ¬ ğ—Ÿğ—²ğ˜€ğ˜€ğ—¼ğ—» ğŸ¯ of the ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ—  ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² is ğ—¢ğ—¨ğ—§ ğŸ¬\nğ—–ğ—¼ğ—»ğ˜ğ—²ğ˜…ğ˜\nLLMs are as good as the data you embed in your prompts. And for many real-world problems, this means the data needs to be both\nâ†’ good, and\nâ†’ fresh\nğ—˜ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²Â ğŸ’\nImagine you build a great LLM-based financial advisorâ€¦ but you only feed it with outdated data.\nNo matter how good your model is, the predictions it will generate will be rubbish ğŸ«£\nSo, the question is\nğ™ƒğ™¤ğ™¬ ğ™™ğ™¤ ğ™›ğ™šğ™šğ™™ ğ™›ğ™§ğ™šğ™¨ğ™ ğ™™ğ™–ğ™©ğ™– ğ™©ğ™¤ ğ™®ğ™¤ğ™ªğ™§ ğ™‡ğ™‡ğ™ˆ â“\nğ—§ğ—µğ—² ğ˜€ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ğŸ§ \nYou need to build aÂ ğ—¿ğ—²ğ—®ğ—¹-ğ˜ğ—¶ğ—ºğ—² ğ˜ğ—²ğ˜…ğ˜ ğ—²ğ—ºğ—¯ğ—²ğ—±ğ—±ğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—², that\nâ†’ ğ—œğ—»ğ—´ğ—²ğ˜€ğ˜ğ˜€Â raw text from your data source, in real-time\nâ†’ ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—ºğ˜€Â this raw text into vector embeddings, and\nâ†’ ğ—¦ğ˜ğ—¼ğ—¿ğ—²ğ˜€Â these embeddings in a VectorDB, so your LLM can fetch and use them for Retrieval Augmented Generation (RAG) at inference time.\nğ—™ğ˜‚ğ—¹ğ—¹ ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—°ğ—¼ğ—±ğ—² ğŸ‘¨ğŸ»â€ğŸ’»\nIn Lesson 3 of the ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ—  ğ—–ğ—¼ğ˜‚ğ—¿ğ˜€ğ—², you will find a full source code implementation of a text embedding pipeline, that uses:\nâ†’\nAlpaca\nNews APIÂ as our real-time data source,\nâ†’\nBytewax\nto transform raw text into vector embeddings, and\nâ†’\nQdrant\nas a Serverless Vector DB, to store and retrieve embeddings at inference time.\nYou will find the link to the ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ ğ—¿ğ—²ğ—½ğ—¼ in the comment section below â¬‡ï¸\nğ—ªğ—µğ—®ğ˜'ğ˜€ ğ—»ğ—²ğ˜…ğ˜? â­ï¸\nIn the next lecture, we will cover the ğ—¶ğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—², so our system can serve predictions to end users. ğŸš€\nTHANK YOU\nPaul Iusztin\nand\nAlexandru RÄƒzvanÈ› ğŸ‘‹\nfor making this course possible. It is a pleasure working with you guys!\n----\nHi there! It's\nPau Labarta Bajo\nğŸ‘‹\nEvery week I share free, hands-on content, on production-grade ML, to help you build real-world ML products.\nğ—™ğ—¼ğ—¹ğ—¹ğ—¼ğ˜„ ğ—ºğ—² and ğ—°ğ—¹ğ—¶ğ—°ğ—¸ ğ—¼ğ—» ğ˜ğ—µğ—² ğŸ”” so you don't miss what's coming next\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nrealworldml\nhashtag\n#\nllmops\nhashtag\n#\nllms\nhashtag\n#\nbytewax\nhashtag\n#\nqdrant\nhashtag\n#\nvectordb\nhashtag\n#\nrealtimeml",
            "image": "https://media.licdn.com/dms/image/D4D10AQFT2dwwdcKnKw/image-shrink_800/0/1698910225666?e=1705082400&v=beta&t=I_tqazlPLy61eGxPuWRed7Ibat2si35BDaaV-xMA7eY"
        },
        "Post_17": {
            "text": "ğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—¶ğ—»ğ—´ & ğ—ºğ—®ğ—»ğ—®ğ—´ğ—¶ğ—»ğ—´ ML models is ğ—µğ—®ğ—¿ğ—±, especially when running your models on GPUs.\nBut ğ˜€ğ—²ğ—¿ğ˜ƒğ—²ğ—¿ğ—¹ğ—²ğ˜€ğ˜€ makes things ğ—²ğ—®ğ˜€ğ˜†.\nUsing\nBeam\nas your serverless provider, deploying & managing ML models can be as easy as â†“\nğ——ğ—²ğ—³ğ—¶ğ—»ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¶ğ—»ğ—³ğ—¿ğ—®ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—² & ğ—±ğ—²ğ—½ğ—²ğ—»ğ—±ğ—²ğ—»ğ—°ğ—¶ğ—²ğ˜€\nIn a few lines of code, you define the application that contains:\n- the requirements of your infrastructure, such as the CPU, RAM, and GPU\n- the dependencies of your application\n- the volumes from where you can load your data and store your artifacts\nğ——ğ—²ğ—½ğ—¹ğ—¼ğ˜† ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—·ğ—¼ğ—¯ğ˜€\nUsing the Beam application, you can quickly decore your Python functions to:\n- run them once on the given serverless application\n- put your task/job in a queue to be processed or even schedule it using a CRON-based syntax\n- even deploy it as a RESTful API endpoint\n.\nAs you can see in the image below, you can have one central function for training or inference, and with minimal effort, you can switch from all these deployment methods.\nAlso, you don't have to bother at all with managing the infrastructure on which your jobs run. You specify what you need, and Beam takes care of the rest.\nBy doing so, you can directly start to focus on your application and stop carrying about the infrastructure.\nThis is the power of serverless!\nâ†³ Check out\nBeam\nto learn more: ğŸ”—\nhttps://lnkd.in/d4-pkCxc\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQFTZKv4E86hGw/image-shrink_800/0/1698737423218?e=1705082400&v=beta&t=BKwJmd05FjemiJhCa4RC2YjkCgVxWxNLR_IM87Cj49o"
        },
        "Post_18": {
            "text": "This is how I ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—²ğ—± ğ—£ğ˜†ğ——ğ—¼ğ—°ğ˜€ for ğŸ­ğŸ¬ğŸ¬ ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€ in <ğŸ­ ğ—µğ—¼ğ˜‚ğ—¿ â†“\nThe most boring programming part is to write PyDocs, so I usually write clean code and let it speak for itself.\nBut, for open-source projects where you have to generate robust documentation, PyDocs are a must.\nThe good news is that now you can automate this process using Copilot.\nYou can see in the video below an example of how easy it is.\nI tested it on more complex functions/classes, and it works well. I chose this example because it fits nicely on one screen.\nOnce I tested Copilot's experience, I will never go back.\nIt is true that, in some cases, you have to make some minor adjustments. But that is still 10000% more efficient than writing it from scratch.\nIf you want more examples, check out our Hands-on LLMs course, where all the PyDocs are generated 99% using Copilot in <1 hour.\nâ†³ Check it out: ğŸ”—\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGkhXbIeRV1ag/image-shrink_800/0/1698651020181?e=1705082400&v=beta&t=FBlsJxnnrIT3xDkoQ1qdkMXg17GKRyCK1y3GixwrULE"
        },
        "Post_19": {
            "text": "ğ—ªğ—µğ—®ğ˜ do you need to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² an open-source ğ—Ÿğ—Ÿğ—  to create your own ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ—±ğ˜ƒğ—¶ğ˜€ğ—¼ğ—¿?\nThis is the ğ—Ÿğ—Ÿğ—  ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ—¸ğ—¶ğ˜ you must know â†“\nğ——ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜\nThe key component of any successful ML project is the data.\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\nThe best approach is to hire a bunch of experts to create it manually.\nBut, for a PoC, that might get expensive & slow.\nThe good news is that a method called \"ğ˜ğ˜ªğ˜¯ğ˜¦ğ˜µğ˜¶ğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜¸ğ˜ªğ˜µğ˜© ğ˜¥ğ˜ªğ˜´ğ˜µğ˜ªğ˜­ğ˜­ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯\" exists.\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to generate your fine-tuning data. After, use this data to fine-tune a smaller model (e.g., Falcon 7B).\"\nFor specializing smaller LLMs on specific use cases (e.g., financial advisors), this is an excellent method to kick off your project.\nğ—£ğ—¿ğ—²-ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ—Ÿğ—Ÿğ— \nYou never want to start training your LLM from scratch (or rarely).\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.\nYou want to fine-tune your LLM on your specific task.\nThe good news is that you can find a plethora of open-source LLMs on HuggingFace (e.g., Falcon, LLaMa, etc.)\nğ—£ğ—®ğ—¿ğ—®ğ—ºğ—²ğ˜ğ—²ğ—¿ ğ—²ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ˜ ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´\nAs LLMs are big... duh...\n... they don't fit on a single GPU.\nAs you want only to fine-tune the LLM, the community invented clever techniques that quantize the LLM (to fit on a single GPU) and fine-tune only a set of smaller adapters.\nOne popular approach is QLoRA, which can be implemented using HF's `ğ˜±ğ˜¦ğ˜§ğ˜µ` Python package.\nğ— ğ—Ÿğ—¢ğ—½ğ˜€\nAs you want your project to get to production, you have to integrate the following MLOps components:\n- experiment tracker to monitor & compare your experiments\n- model registry to version & share your models between the FTI pipelines\n- prompts monitoring to debug & track complex chains\nâ†³ All of them are available on ML platforms, such as Comet ML ğŸ”—\nhttps://lnkd.in/d7jNQz7m\nğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—½ğ—¹ğ—®ğ˜ğ—³ğ—¼ğ—¿ğ—º\nThe most common approach is to train your LLM on your on-prem Nivida GPUs cluster or rent them on cloud providers such as AWS, Paperspace, etc.\nBut what if I told you that there is an easier way?\nThere is! It is called serverless.\nFor example,\nBeam\nis a GPU serverless provider that makes deploying your training pipeline as easy as decorating your Python function with `@ğ˜¢ğ˜±ğ˜±.ğ˜³ğ˜¶ğ˜¯()`.\nAlong with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\nâ†³ Beam: ğŸ”—\nhttps://lnkd.in/dedCaMDh\n.\nâ†³ To see all these components in action, check out my FREE ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² & give it a â­:  ğŸ”—\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQHWQzZcToQQ1Q/image-shrink_800/0/1698388219549?e=1705082400&v=beta&t=9mrDC_NooJgD7u7Qk0PmrTGGaZtuwDIFKh3bEqeBsm0"
        },
        "Post_20": {
            "text": "ğ—§ğ—µğ—² ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ FREE ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² just ğ—½ğ—®ğ˜€ğ˜€ğ—²ğ—± ğŸ°ğŸ¬ğŸ¬+ ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ â­ğŸŒŸâ­\nA big ğ—§ğ—µğ—®ğ—»ğ—¸ ğ˜†ğ—¼ğ˜‚!, for everyone who supported the GitHub repo. This means a lot to me.\nAlso, I want to thank Pau Labarta and Alexandru Razvant for this fantastic collaboration and for making this course possible.\n.\nğ˜ğ˜°ğ˜³ ğ˜µğ˜©ğ˜¦ ğ˜±ğ˜¦ğ˜°ğ˜±ğ˜­ğ˜¦ ğ˜¸ğ˜©ğ˜° ğ˜¥ğ˜°ğ˜¯'ğ˜µ ğ˜¬ğ˜¯ğ˜°ğ˜¸, ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜ªğ˜´ ğ˜´ğ˜°ğ˜®ğ˜¦ ğ˜¤ğ˜°ğ˜¯ğ˜µğ˜¦ğ˜¹ğ˜µ ğ˜¢ğ˜£ğ˜°ğ˜¶ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜¤ğ˜°ğ˜¶ğ˜³ğ˜´ğ˜¦ â†“\nğ—§ğ—µğ—² ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¢ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a ğ—³ğ˜‚ğ—¹ğ—¹ğ˜† ğ—¼ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== ğ—ªğ—µğ—®ğ˜ ğ—¬ğ—¼ğ˜‚'ğ—¹ğ—¹ ğ—•ğ˜‚ğ—¶ğ—¹ğ—± ===\nWithin the course, you will leverage the ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—®ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—², as follows:\nğŸ­. ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll create a system to ingest real-time financial newsâ€”crucial for up-to-date advice.\nğŸ®. ğ—§ğ—¿ğ—®ğ—±ğ—¶ğ—»ğ—´ ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll fine-tune an LLM to specialize the model in making financial decisions.\nğŸ¯. ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nğ˜Šğ˜©ğ˜¦ğ˜¤ğ˜¬ ğ˜ªğ˜µ ğ˜°ğ˜¶ğ˜µ & ğ˜¨ğ˜ªğ˜·ğ˜¦ ğ˜ªğ˜µ ğ˜¢ â­ â†“\nâ†³ğŸ”— Hands-On LLMs course:\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQElWHm8-4V6Bg/image-shrink_800/0/1698301818646?e=1705082400&v=beta&t=NeFBCUd-3EbJffNnwHA1eWEslDt4a3DUpTrklfTfo2Y"
        },
        "Post_21": {
            "text": "You must know these ğŸ¯ ğ—ºğ—®ğ—¶ğ—» ğ˜€ğ˜ğ—®ğ—´ğ—²ğ˜€ of ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—®ğ—» ğ—Ÿğ—Ÿğ—  to train your own ğ—Ÿğ—Ÿğ—  on your ğ—½ğ—¿ğ—¼ğ—½ğ—¿ğ—¶ğ—²ğ˜ğ—®ğ—¿ğ˜† ğ—±ğ—®ğ˜ğ—®.\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ­: ğ—£ğ—¿ğ—²ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—³ğ—¼ğ—¿ ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜ğ—¶ğ—¼ğ—»\nYou start with a bear foot randomly initialized LLM.\nThis stage aims to teach the model to spit out tokens. More concretely, based on previous tokens, the model learns to predict the next token with the highest probability.\nFor example, your input to the model is \"The best programming language is ___\", and it will answer, \"The best programming language is Rust.\"\nIntuitively, at this stage, the LLM learns to speak.\nğ˜‹ğ˜¢ğ˜µğ˜¢:  >1 trillion token (~= 15 million books). The data quality doesn't have to be great. Hence, you can scrape data from the internet.\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ®: ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ˜ƒğ—¶ğ˜€ğ—²ğ—± ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ (ğ—¦ğ—™ğ—§) ğ—³ğ—¼ğ—¿ ğ—±ğ—¶ğ—®ğ—¹ğ—¼ğ—´ğ˜‚ğ—²\nYou start with the pretrained model from stage 1.\nThis stage aims to teach the model to respond to the user's questions.\nFor example, without this step, when prompting: \"What is the best programming language?\", it has a high probability of creating a series of questions such as: \"What is MLOps? What is MLE? etc.\"\nAs the model mimics the training data, you must fine-tune it on Q&A (questions & answers) data to align the model to respond to questions instead of predicting the following tokens.\nAfter the fine-tuning step, when prompted, \"What is the best programming language?\", it will respond, \"Rust\".\nğ˜‹ğ˜¢ğ˜µğ˜¢: 10K - 100K Q&A example\nğ˜•ğ˜°ğ˜µğ˜¦: After aligning the model to respond to questions, you can further single-task fine-tune the model, on Q&A data, on a specific use case to specialize the LLM.\n# ğ—¦ğ˜ğ—®ğ—´ğ—² ğŸ¯: ğ—¥ğ—²ğ—¶ğ—»ğ—³ğ—¼ğ—¿ğ—°ğ—²ğ—ºğ—²ğ—»ğ˜ ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ ğ—³ğ—¿ğ—¼ğ—º ğ—µğ˜‚ğ—ºğ—®ğ—» ğ—³ğ—²ğ—²ğ—±ğ—¯ğ—®ğ—°ğ—¸ (ğ—¥ğ—Ÿğ—›ğ—™)\nDemonstration data tells the model what kind of responses to give but doesn't tell the model how good or bad a response is.\nThe goal is to align your model with user feedback (what users liked or didn't like) to increase the probability of generating answers that users find helpful.\nğ˜™ğ˜“ğ˜ğ˜ ğ˜ªğ˜´ ğ˜´ğ˜±ğ˜­ğ˜ªğ˜µ ğ˜ªğ˜¯ 2:\n1. Using the LLM from stage 2, train a reward model to act as a scoring function using (prompt, winning_response, losing_response) samples (= comparison data). The model will learn to maximize the difference between these 2. After training, this model outputs rewards for (prompt, response) tuples.\nğ˜‹ğ˜¢ğ˜µğ˜¢: 100K - 1M comparisons\n2. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here, you will use the reward model trained above to give a score for every: (prompt, response). The RL algorithm will align the LLM to generate prompts with higher rewards, increasing the probability of generating responses that users liked.\nğ˜‹ğ˜¢ğ˜µğ˜¢: 10K - 100K prompts\n.\nNote: Post inspired by Chip Huyen's \"RLHF: Reinforcement Learning from Human Feedback\" article: ğŸ”—\nhttps://lnkd.in/dRTFHeFZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQEDEtt1xb7W3Q/image-shrink_800/0/1698129027577?e=1705082400&v=beta&t=O1MqhJko8sj4YYOtVbvxAOddmM8M7SzD4pGSBfEJg54"
        },
        "Post_22": {
            "text": "These are the ğŸ´ ğ˜ğ˜†ğ—½ğ—²ğ˜€ of ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ˜ğ—¼ğ—¼ğ—¹ğ˜€ that must be in your toolbelt to be a ğ˜€ğ˜‚ğ—°ğ—°ğ—²ğ˜€ğ˜€ğ—³ğ˜‚ğ—¹ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ â†“\nIf you are into MLOps, you are aware of the 1000+ tools in the space and think you have to know.\nThe reality is that all of these tools can be boiled down to 8 main categories.\nIf you learn the fundamentals and master one tool from each category, you will be fine.\n.\nBaÅŸak TuÄŸÃ§e Eskili\nand\nMaria Vechtomova\nfrom\nMarvelous MLOps\nwrote an excellent summary highlighting these 8 categories:\n1. ğ™‘ğ™šğ™§ğ™¨ğ™ğ™¤ğ™£ ğ™˜ğ™¤ğ™£ğ™©ğ™§ğ™¤ğ™¡: crucial for the traceability and reproducibility of an ML model deployment or run. Without a version control system, it is difficult to find out what exact code version was responsible for specific runs or errors you might have in production. (ğŸ”§ GitHub, GitLab, etc.)\n2. ğ˜¾ğ™„/ğ˜¾ğ˜¿: automated tests are triggered upon pull request creation & deployment to production should only occur through the CD pipeline (ğŸ”§ GitHub Actions, GitLab CI/CD, Jenkins, etc.)\n3. ğ™’ğ™¤ğ™§ğ™ ğ™›ğ™¡ğ™¤ğ™¬ ğ™¤ğ™§ğ™˜ğ™ğ™šğ™¨ğ™©ğ™§ğ™–ğ™©ğ™ğ™¤ğ™£: manage complex dependencies between different tasks, such as data preprocessing, feature engineering, ML model training (ğŸ”§  Airflow, ZenML, AWS Step Functions, etc.)\n4. ğ™ˆğ™¤ğ™™ğ™šğ™¡ ğ™§ğ™šğ™œğ™ğ™¨ğ™©ğ™§ğ™®: store, version, and share trained ML model artifacts, together with additional metadata (ğŸ”§  Comet ML, W&B, MLFlow, etc.)\n5. ğ˜¿ğ™¤ğ™˜ğ™ ğ™šğ™§ ğ™§ğ™šğ™œğ™ğ™¨ğ™©ğ™§ğ™®: store, version, and share Docker images. Basically, all your code will be wrapped up in Docker images and shared through this registry (ğŸ”§ Docker Hub, ECR, etc.)\n6 & 7. ğ™ˆğ™¤ğ™™ğ™šğ™¡ ğ™©ğ™§ğ™–ğ™ğ™£ğ™ğ™£ğ™œ & ğ™¨ğ™šğ™§ğ™«ğ™ğ™£ğ™œ ğ™ğ™£ğ™›ğ™§ğ™–ğ™¨ğ™©ğ™§ğ™ªğ™˜ğ™©ğ™ªğ™§ğ™š: if on-premise, you will likely have to go with Kubernetes. There are multiple choices if you are on a cloud provider: Azure ML on Azure, Sagemaker on AWS, and Vertex AI on GCP.\n8. ğ™ˆğ™¤ğ™£ğ™ğ™©ğ™¤ğ™§ğ™ğ™£ğ™œ: Monitoring in ML systems goes beyond what is needed for monitoring regular software applications. The distinction lies in that the model predictions can fail even if all typical health metrics appear in good condition. (ğŸ”§  SageMaker, NannyML, Arize, etc.)\nThe secret sauce in MLOps is knowing how to glue all these pieces together while keeping things simple.\n.\nâ†³ To read more about these components, check out the\nMarvelous MLOps\narticle: ğŸ”—\nhttps://lnkd.in/dUHRBzDt\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHFvX6aK5CX0Q/image-shrink_800/0/1697005817415?e=1705082400&v=beta&t=1xYpatJcu_GUTmJOQDLgsmkdTkuPe5ySrTiIADN62zQ"
        },
        "Post_23": {
            "text": "This is ğ—µğ—¼ğ˜„ you can ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² to populate a ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• to do ğ—¥ğ—”ğ—š for a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ powered by ğ—Ÿğ—Ÿğ— ğ˜€.\nIn a previous post, I covered ğ˜„ğ—µğ˜† you need a streaming pipeline over a batch pipeline when implementing RAG.\nNow, we will focus on the ğ—µğ—¼ğ˜„, aka implementation details â†“\nğŸ All the following steps are wrapped in\nBytewax\nfunctions and connected in a single streaming pipeline.\nğ—˜ğ˜…ğ˜ğ—¿ğ—®ğ—°ğ˜ ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—»ğ—²ğ˜„ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ—”ğ—¹ğ—½ğ—®ğ—°ğ—®\nYou need 2 types of inputs:\n1. A WebSocket API to listen to financial news in real time. This will be used to listen 24/7 for new data and ingest it as soon as it is available.\n2. A RESTful API to ingest historical data in batch mode. When you deploy a fresh vector DB, you must populate it with data between a given range [date_start; date_end].\nYou wrap the ingested HTML document and its metadata in a `pydantic` NewsArticle model to validate its schema.\nRegardless of the input type, the ingested data is the same. Thus, the following steps are the same for both data inputs â†“\nğ—£ğ—®ğ—¿ğ˜€ğ—² ğ˜ğ—µğ—² ğ—›ğ—§ğ— ğ—Ÿ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜\nAs the ingested financial news is in HTML, you must extract the text from particular HTML tags.\n`unstructured` makes it as easy as calling `partition_html(document)`, which will recursively return the text within all essential HTML tags.\nThe parsed NewsArticle model is mapped into another `pydantic` model to validate its new schema.\nThe elements of the news article are the headline, summary and full content.\nğ—–ğ—¹ğ—²ğ—®ğ—» ğ˜ğ—µğ—² ğ˜ğ—²ğ˜…ğ˜\nNow we have a bunch of text that has to be cleaned. Again, `unstructured` makes things easy. Calling a few functions we clean:\n- the dashes & bullets\n- extra whitespace & trailing punctuation\n- non ascii chars\n- invalid quotes\nFinally, we standardize everything to lowercase.\nğ—–ğ—µğ˜‚ğ—»ğ—¸ ğ˜ğ—µğ—² ğ˜ğ—²ğ˜…ğ˜\nAs the text can exceed the context window of the embedding model, we have to chunk it.\nYet again, `unstructured` provides a valuable function that splits the text based on the tokenized text and expected input length of the embedding model.\nThis strategy is naive, as it doesn't consider the text's structure, such as chapters, paragraphs, etc. As the news is short, this is not an issue, but LangChain provides a `RecursiveCharacterTextSplitter` class that does that if required.\nğ—˜ğ—ºğ—¯ğ—²ğ—± ğ˜ğ—µğ—² ğ—°ğ—µğ˜‚ğ—»ğ—¸ğ˜€\nYou pass all the chunks through an encoder-only model.\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model that can run on a CPU and outputs a 384 embedding.\nBut based on the size and complexity of your data, you might need more complex and bigger models.\nğ—Ÿğ—¼ğ—®ğ—± ğ˜ğ—µğ—² ğ—±ğ—®ğ˜ğ—® ğ—¶ğ—» ğ˜ğ—µğ—² ğ—¤ğ—±ğ—¿ğ—®ğ—»ğ˜ ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—•\nFinally, you insert the embedded chunks and their metadata into the\nQdrant\nvector DB.\nThe metadata contains the embedded text, the source_url and the publish date.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4D10AQFL1FYbWw2npQ/image-shrink_800/0/1696919418846?e=1705082400&v=beta&t=q9v0_nqPdV4EQminbksTOQ5h8xmUtr9HlAj_nC-aXfM"
        },
        "Post_24": {
            "text": "My ğ—»ğ—²ğ˜„ğ˜€ğ—¹ğ—²ğ˜ğ˜ğ—²ğ—¿ just ğ—½ğ—®ğ˜€ğ˜€ğ—²ğ—± 1400 subscribers ğŸ‰ğŸ‰ğŸ‰  If you are into ğ—Ÿğ—Ÿğ— ğ˜€, ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—•ğ˜€, and ğ— ğ—Ÿğ—¢ğ—½ğ˜€, you will like the ğ—³ğ˜‚ğ˜ğ˜‚ğ—¿ğ—² ğ˜€ğ—²ğ—¿ğ—¶ğ—²ğ˜€ I am about to ğ˜€ğ˜ğ—®ğ—¿ğ˜ â†“\n.\nâœŒï¸ ğ—™ğ—¶ğ—¿ğ˜€ğ˜, I want to thank everybody who reads my newsletter: \"Decoding ML.\" As it is completely free, your engagement is the only thing that motivates me.\n.\nğŸ”¥ ğ—¦ğ—²ğ—°ğ—¼ğ—»ğ—±ğ—¹ğ˜†, **ğ˜¦ğ˜¹ğ˜¤ğ˜ªğ˜µğ˜¦ğ˜¥ ğ˜·ğ˜°ğ˜ªğ˜¤ğ˜¦** here are my plans for the \"Decoding ML\" newsletter\nUntil now, the weekly articles were randomly picked from various ğ— ğ—Ÿğ—˜, ğ— ğ—Ÿğ—¢ğ—½ğ˜€, ğ——ğ—Ÿ, and ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ˜ƒğ—² ğ—”ğ—œ ğ˜ğ—¼ğ—½ğ—¶ğ—°ğ˜€.\nğ—¦ğ˜ğ—®ğ—¿ğ˜ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—¶ğ˜€ ğ˜„ğ—²ğ—²ğ—¸, based on my ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—², I will ğ—¹ğ—®ğ˜‚ğ—»ğ—°ğ—µ a ğ˜€ğ—²ğ—¿ğ—¶ğ—²ğ˜€ of ğ˜€ğ—µğ—¼ğ—¿ğ˜ ğ—®ğ—¿ğ˜ğ—¶ğ—°ğ—¹ğ—²ğ˜€ that will teach you how to ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—», ğ—¯ğ˜‚ğ—¶ğ—¹ğ—±, and ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜† an ğ—²ğ—»ğ—±-ğ˜ğ—¼-ğ—²ğ—»ğ—± ğ—Ÿğ—Ÿğ—  ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º for a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜.\nIt will cover topics such as:\n- the 3-pipeline / FTI architecture\n- building your own QA dataset\n- fine-tuning an LLM using QLoRA\n- building a streaming pipeline\n- using a vector DB for RAG\n- gluing everything together using LangChain\n- deploying the solution\nThis is not the course itself. It is just an overview of the most essential aspects.\nBut, if you are too busy to take the whole course, these weekly FREE 5-minute lessons are a great way to learn how to build an end-to-end LLM product seamlessly.\n.\nğŸ‘€ ğ—¦ğ˜‚ğ—¯ğ˜€ğ—°ğ—¿ğ—¶ğ—¯ğ—² to start receiving them in your mail â†“\nâ†³ğŸ”—\nhttps://lnkd.in/dsMR4ivA\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps",
            "image": "https://media.licdn.com/dms/image/D4D22AQE42Jts6xW5ZQ/feedshare-shrink_800/0/1696243073549?e=1707350400&v=beta&t=2DUcli8YM5s91a5Wi4Xwn4Vo6TWF6hemcUFLOPHoqO0"
        },
        "Post_25": {
            "text": "ğ—ªğ—µğ˜† do you need a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—»ğ˜€ğ˜ğ—²ğ—®ğ—± of a ğ—¯ğ—®ğ˜ğ—°ğ—µ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² when implementing ğ—¥ğ—”ğ—š in your ğ—Ÿğ—Ÿğ—  ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€? ğ—ªğ—µğ—®ğ˜ do you need to ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² for a financial assistant?\nâ†³ ğ—ªğ—µğ˜† ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—® ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—»ğ˜€ğ˜ğ—²ğ—®ğ—± ğ—¼ğ—³ ğ—® ğ—¯ğ—®ğ˜ğ—°ğ—µ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²?\nThe quality of your RAG implementation is as good as the quality & freshness of your data.\nThus, depending on your use case, you have to ask:\n\"How fresh does my data from the vector DB have to be to provide accurate answers?\"\nBut for the best user experience, the data has to be as fresh as possible, aka real-time data.\nFor example, when implementing a financial assistant, being aware of the latest financial news is critical. A new piece of information can completely change the course of your strategy.\nHence, when implementing RAG, one critical aspect is to have your vector DB synced with all your external data sources in real time.\nA batch pipeline will work if your use case accepts a particular delay (e.g., one hour, one day, etc.).\nBut with tools like Bytewax ğŸ, building streaming applications becomes much more accessible. So why not aim for the best?\nâ†³ ğ—ªğ—µğ—®ğ˜ ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—»ğ—²ğ—²ğ—± ğ˜ğ—¼ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—® ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—³ğ—¼ğ—¿ ğ—® ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜?\n- A financial news data source exposed through a web socket (e.g., Alpaca)\n- A Python streaming processing framework. For example,\nBytewax\nğŸ is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need the Java ecosystem to implement real-time pipelines anymore.  â†³ğŸ”—\nhttps://lnkd.in/dWJytkZ5\n- A Python package to process, clean, and chunk documents. `unstructured` offers a rich set of features that makes parsing HTML documents extremely convenient.\n- An encoder-only language model that maps your chunked documents into embeddings. `setence-transformers` is well integrated with HuggingFace and has a huge list of models of various sizes.\n- A vector DB, where to insert your embeddings and their metadata (e.g., the embedded text, the source_url, the creation date, etc.). For example,\nQdrant\nprovides a rich set of features and a seamless experience.  â†³ğŸ”—\nhttps://lnkd.in/d_FA9Bb3\n- A way to deploy your streaming pipeline. Docker + AWS will never disappoint you.\n- A CI/CD pipeline for continuous tests & deployments. GitHub Actions is a great serverless option with a rich ecosystem.\nThis is what you need to build & deploy a streaming pipeline solely in Python ğŸ”¥\n.\nWhat is your experience with batch vs. streaming pipelines? Let me know in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps",
            "image": "https://media.licdn.com/dms/image/D4D10AQG5nAhxYnjMLQ/image-shrink_800/0/1696573908831?e=1705082400&v=beta&t=sSLpB124q-OAdtUniLR4cycLRbi57QJzvSoXDmOJrxY"
        },
        "Post_26": {
            "text": "ğ—›ğ—¼ğ˜„ ğ˜ğ—¼ ğ—®ğ—±ğ—± ğ—¿ğ—²ğ—®ğ—¹-ğ˜ğ—¶ğ—ºğ—² ğ—ºğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ—ºğ—²ğ˜ğ—¿ğ—¶ğ—°ğ˜€ to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\nğ—œğ—» ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—», ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—®ğ˜‚ğ˜ğ—¼ğ—ºğ—®ğ˜ğ—¶ğ—°ğ—®ğ—¹ğ—¹ğ˜† ğ—®ğ—°ğ—°ğ—²ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ ğ—¶ğ—» ğŸ¯ ğ—ºğ—®ğ—¶ğ—» ğ˜€ğ—°ğ—²ğ—»ğ—®ğ—¿ğ—¶ğ—¼ğ˜€:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\nğ—™ğ—¼ğ—¿ ğ˜‚ğ˜€ğ—² ğ—°ğ—®ğ˜€ğ—²ğ˜€ ğŸ®. ğ—®ğ—»ğ—± ğŸ¯. ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—¾ğ˜‚ğ—¶ğ—°ğ—¸ğ—¹ğ˜† ğ—°ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—ºğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—» ğ˜ğ—µğ—² ğ—³ğ—¼ğ—¹ğ—¹ğ—¼ğ˜„ğ—¶ğ—»ğ—´ ğ˜„ğ—®ğ˜†:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a near real-time monitoring pipeline using Airflow and GCS, check out my article:\nâ†³ğŸ”— ğ˜Œğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜›ğ˜³ğ˜¶ğ˜´ğ˜µğ˜¸ğ˜°ğ˜³ğ˜µğ˜©ğ˜º ğ˜”ğ˜“ ğ˜šğ˜ºğ˜´ğ˜µğ˜¦ğ˜®ğ˜´ ğ˜ğ˜ªğ˜µğ˜© ğ˜‹ğ˜¢ğ˜µğ˜¢ ğ˜ğ˜¢ğ˜­ğ˜ªğ˜¥ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¯ğ˜¥ ğ˜™ğ˜¦ğ˜¢ğ˜­-ğ˜›ğ˜ªğ˜®ğ˜¦ ğ˜”ğ˜°ğ˜¯ğ˜ªğ˜µğ˜°ğ˜³ğ˜ªğ˜¯ğ˜¨:\nhttps://lnkd.in/dhqCrGkD\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEbYONqBbq7lg/image-shrink_800/0/1696487427417?e=1705082400&v=beta&t=MKvYXlUNHom2hiPWjK7Br4TpJvS8dwlnaSGSTPT0FrY"
        },
        "Post_27": {
            "text": "ğ—§ğ—¼ğ—½ ğŸ² ğ— ğ—Ÿ ğ—£ğ—¹ğ—®ğ˜ğ—³ğ—¼ğ—¿ğ—º ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ you must know and use in your ML system.\nHere they are â†“\n#ğŸ­. ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜ ğ—§ğ—¿ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#ğŸ®. ğ— ğ—²ğ˜ğ—®ğ—±ğ—®ğ˜ğ—® ğ—¦ğ˜ğ—¼ğ—¿ğ—²\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#ğŸ¯. ğ—©ğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ğ—¶ğ˜€ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#ğŸ°. ğ—¥ğ—²ğ—½ğ—¼ğ—¿ğ˜ğ˜€\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#ğŸ±. ğ—”ğ—¿ğ˜ğ—¶ğ—³ğ—®ğ—°ğ˜ğ˜€\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#ğŸ². ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—¥ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course. Link in the comments â†“\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQHdzHBAZbhnBg/feedshare-shrink_800/0/1696407245195?e=1707350400&v=beta&t=DFGXVyjJd2SvFcgV1Pe_ZXMfM-n4kJ6ckAKx_r6J7-M"
        },
        "Post_28": {
            "text": "This is what you need to know about ğ—°ğ—µğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ to ğ—¿ğ—²ğ—±ğ˜‚ğ—°ğ—² ğ—°ğ—¼ğ˜€ğ˜ğ˜€, ğ—¶ğ—»ğ—°ğ—¿ğ—²ğ—®ğ˜€ğ—² ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜†, ğ—®ğ—»ğ—± ğ—²ğ—®ğ˜€ğ—¶ğ—¹ğ˜† ğ—±ğ—²ğ—¯ğ˜‚ğ—´ your ğ—Ÿğ—Ÿğ—  ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€.\nHere it is â†“\nğ—–ğ—µğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ is an intuitive technique that states that you must split your prompts into multiple calls.\nğ—ªğ—µğ˜†? ğ—Ÿğ—²ğ˜'ğ˜€ ğ˜‚ğ—»ğ—±ğ—²ğ—¿ğ˜€ğ˜ğ—®ğ—»ğ—± ğ˜ğ—µğ—¶ğ˜€ ğ˜„ğ—¶ğ˜ğ—µ ğ˜€ğ—¼ğ—ºğ—² ğ—®ğ—»ğ—®ğ—¹ğ—¼ğ—´ğ—¶ğ—²ğ˜€.\nWhen cooking, you are following a recipe split into multiple steps. You want to move to the next step only when you know what you have done so far is correct.\nâ†³ You want every prompt to be simple & focused.\nAnother analogy is between reading all the code in one monolith/god class and using DRY to separate the logic between multiple modules.\nâ†³ You want to understand & debug every prompt easily.\n.\nChaining prompts is a ğ—½ğ—¼ğ˜„ğ—²ğ—¿ğ—³ğ˜‚ğ—¹ ğ˜ğ—¼ğ—¼ğ—¹ ğ—³ğ—¼ğ—¿ ğ—¯ğ˜‚ğ—¶ğ—¹ğ—±ğ—¶ğ—»ğ—´ ğ—® ğ˜€ğ˜ğ—®ğ˜ğ—²ğ—³ğ˜‚ğ—¹ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º where you must take different actions depending on the current state.\nIn other words, you control what happens between 2 chained prompts.\nğ˜‰ğ˜ºğ˜±ğ˜³ğ˜°ğ˜¥ğ˜¶ğ˜¤ğ˜µğ˜´ ğ˜°ğ˜§ ğ˜¤ğ˜©ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜±ğ˜³ğ˜°ğ˜®ğ˜±ğ˜µğ˜´:\n- increase in accuracy\n- reduce the number of tokens -> lower costs (skips steps of the workflow when not needed)\n- avoid context limitations\n- easier to include a human-in-the-loop -> easier to control, moderate, test & debug\n- use external tools/plugins (web search, API, databases, calculator, etc.)\n.\nğ—˜ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²\nYou want to build a virtual assistant to respond to customer service queries.\nInstead of adding in one single prompt the system message, all the available products, and the user inquiry, you can split it into the following:\n1. Use a prompt to extract the products and categories of interest.\n2. Enrich the context only with the products of interest.\n3. Call the LLM for the final answer.\nYou can evolve this example by adding another prompt that classifies the nature of the user inquiry. Based on that, redirect it to billing, technical support, account management, or a general LLM (similar to the complex system of GPT-4).\n.\nğ—§ğ—¼ ğ˜€ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ—¶ğ˜‡ğ—²:\nInstead of writing a giant prompt that includes multiple steps:\nSplit the god prompt into multiple modular prompts that let you keep track of the state externally and orchestrate the program.\nIn other words, you want modular prompts that you can combine easily (same as in writing standard functions/classes)\n.\nTo ğ—®ğ˜ƒğ—¼ğ—¶ğ—± ğ—¼ğ˜ƒğ—²ğ—¿ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´, use this technique when your prompt contains >= instruction.\nYou can leverage the DRY principle from software -> one prompt = one instruction.\nâ†³ Tools to chain prompts: LangChain\nâ†³ Tools to monitor and debug prompts: Comet LLMOps Tools\nLinks in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEwI0DWVEmMVQ/image-shrink_800/0/1696401020225?e=1705082400&v=beta&t=t1_Qh1_2pKp-A6xmbxUOzJgOTXLO7rFjPbweZ3OL-Pc"
        },
        "Post_29": {
            "text": "Want to ğ˜€ğ˜ğ—®ğ—¿ğ˜ ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´ to ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—»-ğ—¿ğ—²ğ—®ğ—±ğ˜† applications using ğ—Ÿğ—Ÿğ— ğ˜€? Then, I want to let you know that Pau and I ğ˜€ğ˜ğ—®ğ—¿ğ˜ğ—²ğ—± ğ—¿ğ—²ğ—¹ğ—²ğ—®ğ˜€ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—² ğ˜ƒğ—¶ğ—±ğ—²ğ—¼ ğ—¹ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ for the FREE ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¢ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—².\nThe Hands-On LLMs course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a ğ—³ğ˜‚ğ—¹ğ—¹ğ˜† ğ—¼ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== ğ—ªğ—µğ—®ğ˜ ğ—¬ğ—¼ğ˜‚'ğ—¹ğ—¹ ğ—•ğ˜‚ğ—¶ğ—¹ğ—± ===\nWithin the course, you will leverage the ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—®ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—², as follows:\nğŸ­. ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll create a system to ingest real-time financial newsâ€”crucial for up-to-date advice.\nğŸ®. ğ—§ğ—¿ğ—®ğ—±ğ—¶ğ—»ğ—´ ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll fine-tune an LLM to specialize the model in making financial decisions.\nğŸ¯. ğ—œğ—»ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nCheck it out â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGCjY8CTFExMw/image-shrink_800/0/1696314619063?e=1705082400&v=beta&t=f_Jc5oxVRZoju7Rq3qQbHlaIGV3JnIJ31FixFLoJ65M"
        },
        "Post_30": {
            "text": "ğ—¥ğ—”ğ—š: ğ˜„ğ—µğ—®ğ˜ problems does it solve, and ğ—µğ—¼ğ˜„ it's integrated into ğ—Ÿğ—Ÿğ— -ğ—½ğ—¼ğ˜„ğ—²ğ—¿ğ—²ğ—± ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€?\nLet's find out â†“\nRAG is a popular strategy when building LLMs to add external data to your prompt.\n=== ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ===\nWorking with LLMs has 3 main issues:\n1. The world moves fast\nAn LLM learns an internal knowledge base. However, the issue is that its knowledge is limited to its training dataset.\nThe world moves fast. New data flows on the internet every second. Thus, the model's knowledge base can quickly become obsolete.\nOne solution is to fine-tune the model every minute or day...\nIf you have some billions to spend around, go for it.\n2. Hallucinations\nAn LLM  is full of testosterone and likes to be blindly confident.\nEven if the answer looks 100% legit, you can never fully trust it.\n3. Lack of reference links\nIt is hard to trust the response of the LLM if we can't see the source of its decisions.\nEspecially for important decisions (e.g., health, financials)\n=== ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ===\nâ†’ Surprize! It is RAG.\n1. Avoid fine-tuning\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge base as the main memory (e.g., vector DB).\nThe memory is volatile, so you can quickly introduce or remove data.\n2. Avoid hallucinations\nBy forcing the LLM to answer solely based on the given context, the LLM will provide an answer as follows:\n-  use the external data to respond to the user's question if it contains the necessary insights\n- \"I don't know\" if not\n3. Add reference links\nUsing RAG, you can easily track the source of the data and highlight it to the user.\n=== ğ—›ğ—¼ğ˜„ ğ—±ğ—¼ğ—²ğ˜€ ğ—¥ğ—”ğ—š ğ˜„ğ—¼ğ—¿ğ—¸? ===\nLet's say we want to use RAG to build a financial assistant.\nğ˜ğ˜©ğ˜¢ğ˜µ ğ˜¥ğ˜° ğ˜¸ğ˜¦ ğ˜¯ğ˜¦ğ˜¦ğ˜¥?\n- a data source with historical and real-time financial news (e.g. Alpaca)\n- a stream processing engine (e.g., Bytewax - ğŸ”—\nhttps://lnkd.in/dWJytkZ5\n)\n- an encoder-only model for embedding the documents (e.g., pick one from `sentence-transformers`)\n- a vector DB (e.g., Qdrant - ğŸ”—\nhttps://lnkd.in/d_FA9Bb3\n)\nğ˜ğ˜°ğ˜¸ ğ˜¥ğ˜°ğ˜¦ğ˜´ ğ˜ªğ˜µ ğ˜¸ğ˜°ğ˜³ğ˜¬?\nâ†³ On the feature pipeline side:\n1. using Bytewax, you ingest the financial news and clean them\n2. you chunk the news documents and embed them\n3. you insert the embedding of the docs along with their metadata (e.g., the initial text, source_url, etc.) to Qdrant\nâ†³ On the inference pipeline side:\n4. the user question is embedded (using the same embedding model)\n5. using this embedding, you extract the top K most similar news documents from Qdrant\n6. along with the user question, you inject the necessary metadata from the extracted top K documents into the prompt template (e.g., the text of documents & its source_url)\n7. you pass the whole prompt to the LLM for the final answer\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQGlcGb3FkBzhQ/image-shrink_800/0/1695105033416?e=1705082400&v=beta&t=4RL40LiFU6Ri1telE0z71cQerxXsSF5v1EFh-5xs5sU"
        },
        "Post_31": {
            "text": "Want to learn ğ— ğ—Ÿğ—˜ & ğ— ğ—Ÿğ—¢ğ—½ğ˜€ in a ğ˜€ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ—± ğ˜„ğ—®ğ˜†, for ğ—³ğ—¿ğ—²ğ—², and with ğ—µğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—²ğ˜€?\nThen you should check out my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course.\nIn ğŸ®.ğŸ± ğ—µğ—¼ğ˜‚ğ—¿ğ˜€ ğ—¼ğ—³ ğ—¿ğ—²ğ—®ğ—±ğ—¶ğ—»ğ—´ & ğ˜ƒğ—¶ğ—±ğ—²ğ—¼ ğ—ºğ—®ğ˜ğ—²ğ—¿ğ—¶ğ—®ğ—¹ğ˜€, you will ğ—¹ğ—²ğ—®ğ—¿ğ—» ğ—µğ—¼ğ˜„ ğ˜ğ—¼:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\n- trade-offs & future improvements discussion\nâ€¦where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark.\nğ—¬ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—®ğ—°ğ—°ğ—²ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—² ğ—¼ğ—»:\nâ ğ˜”ğ˜¦ğ˜¥ğ˜ªğ˜¶ğ˜®'ğ˜´ ğ˜›ğ˜‹ğ˜š ğ˜±ğ˜¶ğ˜£ğ˜­ğ˜ªğ˜¤ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯: text tutorials + videos\nâ ğ˜ğ˜ªğ˜µğ˜ğ˜¶ğ˜£: open-source code + docs\nI published the course on Medium's TDS publication to make it accessible to as many people as people. Thus â†“\n... anyone can learn the fundamentals of MLE & MLOps.\nSo no more excuses. Just go and build your own project ğŸ”¥\nCheck it out â†“\nâ†³ğŸ”— ğ˜›ğ˜©ğ˜¦ ğ˜ğ˜¶ğ˜­ğ˜­ ğ˜šğ˜µğ˜¢ğ˜¤ğ˜¬ 7-ğ˜šğ˜µğ˜¦ğ˜±ğ˜´ ğ˜”ğ˜“ğ˜–ğ˜±ğ˜´ ğ˜ğ˜³ğ˜¢ğ˜®ğ˜¦ğ˜¸ğ˜°ğ˜³ğ˜¬:\nhttps://lnkd.in/daShNdjw\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHCppG0uxa0zA/image-shrink_800/0/1695018620482?e=1705082400&v=beta&t=ueqFxy38kIm_lDispp15acnQKGiCqhs8whC2gLyJq7U"
        },
        "Post_32": {
            "text": "We all know how ğ—ºğ—²ğ˜€ğ˜€ğ˜† ğ— ğ—Ÿ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—ºğ˜€ can get. That is where the ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—®ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² ğ—¸ğ—¶ğ—°ğ—¸ğ˜€ ğ—¶ğ—».\nThe 3-pipeline design is a way to bring structure & modularity to your ML system and improve your MLOps processes.\nThis is how â†“\n=== ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ===\nDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.\nIn 2022, only 54% of the models get into production. Auch.\nSo what happens?\nSometimes the model is not mature enough, sometimes there are some security risks, but most of the time...\n...the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.\nSo, good processes and a well-defined architecture are as crucial as good tools and models.\n=== ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—» ===\nğ˜›ğ˜©ğ˜¦ 3-ğ˜±ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ ğ˜¢ğ˜³ğ˜¤ğ˜©ğ˜ªğ˜µğ˜¦ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦.\nFirst, let's understand what the 3-pipeline design is.\nIt is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:\n1. the feature pipeline\n2. the training pipeline\n3. the inference pipeline\n...also known as the Feature/Training/Inference (FTI) architecture.\n.\n#ğŸ­. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store.\n#ğŸ®. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained models, which are stored and versioned inside a model registry.\n#ğŸ¯. The inference pipeline takes a given version of the features and trained models and outputs the predictions to a client.\n.\nThis is why the 3-pipeline design is so beautiful:\n- it is intuitive\n- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components\n- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate\n- the ML system has been built with modularity in mind since the beginning\n- the 3 components can easily be divided between multiple teams (if necessary)\n- every component can use the best stack of technologies available for the job\n- every component can be deployed, scaled, and monitored independently\n- the feature pipeline can easily be either batch, streaming or both\nBut the most important benefit is that...\n...by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.\n.\nWhat do you think about the 3-pipeline architecture? Have you used it?\nIf you want to know more about the 3-pipeline design, I recommend this awesome article from\nHopsworks\nâ†“\nâ†³ğŸ”— From MLOps to ML Systems with Feature/Training/Inference Pipelines:\nhttps://lnkd.in/dRnhHDdg\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about MLE and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGk0NIAvpKW4g/image-shrink_800/0/1694759426656?e=1705082400&v=beta&t=-gwb-vVolI4vIk5rtzpTQBloeMm8z7qT3JTqFTLR-_s"
        },
        "Post_33": {
            "text": "Want to learn how to ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—² ğ—®ğ—» ğ—Ÿğ—Ÿğ— , build a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—², use a ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—•, build a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—¯ğ—¼ğ˜ and ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜† ğ—²ğ˜ƒğ—²ğ—¿ğ˜†ğ˜ğ—µğ—¶ğ—»ğ—´ using serverless solutions?\nThen maybe you know that I,\nPau Labarta Bajo\nand\nAlexandru RÄƒzvanÈ› ğŸ‘‹\n(one of the best MLEs I know out there in the wild) are working on our ğ—›ğ—®ğ—»ğ—±ğ˜€-ğ—¼ğ—» ğ—Ÿğ—Ÿğ— ğ˜€ ğ—³ğ—¿ğ—²ğ—² ğ—°ğ—¼ğ˜‚ğ—¿ğ˜€ğ—². If not, now you know.\nâ†’ The course will teach you how to build a ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ powered by ğ—Ÿğ—Ÿğ— ğ˜€ leveraging the ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğŸ¯-ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—».\nThe ğ—¼ğ˜‚ğ˜ğ—°ğ—¼ğ—ºğ—² of the product is a ğ—±ğ—²ğ—½ğ—¹ğ—¼ğ˜†ğ—²ğ—± ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ that you can already show off.\n... and not another Notebook.\n.\nAs the course is still a ğ˜„ğ—¼ğ—¿ğ—¸ ğ—¶ğ—» ğ—½ğ—¿ğ—¼ğ—´ğ—¿ğ—²ğ˜€ğ˜€, we want to ğ—¸ğ—²ğ—²ğ—½ ğ˜†ğ—¼ğ˜‚ ğ˜‚ğ—½ğ—±ğ—®ğ˜ğ—²ğ—± on our progress â†“\nâ†³ Thus, we opened up the ğ—±ğ—¶ğ˜€ğ—°ğ˜‚ğ˜€ğ˜€ğ—¶ğ—¼ğ—» ğ˜ğ—®ğ—¯ under the course's GitHub Repository, where we will ğ—¸ğ—²ğ—²ğ—½ ğ˜†ğ—¼ğ˜‚ ğ˜‚ğ—½ğ—±ğ—®ğ˜ğ—²ğ—± with everything is happening.\n.\nAlso, if you have any ğ—¶ğ—±ğ—²ğ—®ğ˜€, ğ˜€ğ˜‚ğ—´ğ—´ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€ or want to ğ—°ğ—µğ—®ğ˜, we encourage you to ğ—°ğ—¿ğ—²ğ—®ğ˜ğ—² ğ—® \"ğ—»ğ—²ğ˜„ ğ—±ğ—¶ğ˜€ğ—°ğ˜‚ğ˜€ğ˜€ğ—¶ğ—¼ğ—»\".\nâ†“ We want the course to fill your real needs â†“\nâ†³ Hence, if your suggestion fits well with our hands-on course direction, we will consider implementing it.\n.\nCheck it out and leave a â­ if you like what you see:\nâ†³ğŸ”— Hands-on LLMs Course:\nhttps://lnkd.in/dKRmRgfZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEP-TMrmwBHrA/image-shrink_800/0/1694673016012?e=1705082400&v=beta&t=YZdMD5iLU2iQdzMt1vf2PwBBLuqVMYUD3OXPBCKO79s"
        },
        "Post_34": {
            "text": "If anyone told you that ğ— ğ—Ÿ or ğ— ğ—Ÿğ—¢ğ—½ğ˜€ is ğ—²ğ—®ğ˜€ğ˜†, they were ğ—¿ğ—¶ğ—´ğ—µğ˜.\nHere is a simple trick that I learned the hard way â†“\nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is â†’ \"ğ—™ğ—¼ğ—°ğ˜‚ğ˜€ ğ—¼ğ—» ğ˜ğ—µğ—² ğ—³ğ˜‚ğ—»ğ—±ğ—®ğ—ºğ—²ğ—»ğ˜ğ—®ğ—¹ğ˜€.\"\n.\nLet me explain â†“\nWhen you constantly chase the latest models (aka FOMO),  you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHiVva0h_Lf3w/image-shrink_800/0/1694586613948?e=1705082400&v=beta&t=2rOKnJVx_ly96k12ocHZaoOT342k0-NBj5QDM872Ofc"
        },
        "Post_35": {
            "text": "This is my ğ—³ğ—®ğ˜ƒğ—¼ğ—¿ğ—¶ğ˜ğ—² ğ—±ğ—²ğ˜€ğ—¶ğ—´ğ—» ğ—½ğ—®ğ˜ğ˜ğ—²ğ—¿ğ—» that you must know as an ML engineer.\nMost ML engineers completely ignore software design patterns, but let me explain why you should know this one for your machine learning projects â†“\nI am talking about Composite.\nThe Composite pattern is a structural design pattern that helps you compose objects in a tree-like structure.\nLet me explain by starting with the problem.\nğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º\nLet's say that you want to build an ML pipeline that performs object detection + tracking.\nYou can easily divide it into smaller pipelines, such as:\n1. preprocessing\n2. training | inference\n3. postprocessing\nAlso, these 3 pipelines, in their turn, are split into smaller components.\nLet's say that to speed up the ML pipeline. You want to run everything in parallel if possible.\nThus, depending on the use case, it would be best to have a module to compose components sequentially or in parallel.\nâŒ If you don't think this through, your code can quickly transform into spaghetti.\nğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»\nâœ… Now, the Composite design pattern kicks in.\n-> ğ˜›ğ˜©ğ˜ªğ˜´ ğ˜ªğ˜´ ğ˜©ğ˜°ğ˜¸ ğ˜ºğ˜°ğ˜¶ ğ˜¤ğ˜¢ğ˜¯ ğ˜ªğ˜®ğ˜±ğ˜­ğ˜¦ğ˜®ğ˜¦ğ˜¯ğ˜µ ğ˜µğ˜©ğ˜¦ ğ˜”ğ˜“ ğ˜±ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ ğ˜¢ğ˜£ğ˜°ğ˜·ğ˜¦ ğ˜¶ğ˜´ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ ğ˜Šğ˜°ğ˜®ğ˜±ğ˜°ğ˜´ğ˜ªğ˜µğ˜¦ ğ˜±ğ˜¢ğ˜µğ˜µğ˜¦ğ˜³ğ˜¯:\n1. Define a standard interface for all the transformations. Let's call it \"Transformation.\"\n2. We create an abstract class called \"AtomicTransformation\" that inherits the \"Transformation\" interface for an atomic transformation.\n3. We implement an abstract class called \"CompositeTransformation\" for running multiple transformations. This class inherits the \"Transformation\" interface but also inputs a list of \"Transformation\" objects as input.\n4. Depending on how you want to call a sequence of transformations, you can inherit the \"CompositeTransformation\" interface and implement classes for:\n- \"SequenceTransformations\"\n- \"ParallelTransformations,\"\n- \"DistributedTransformations,\" etc.\n5. Now, when you want to implement a granular transformation (e.g., normalize the image). You implement the \"AtomicTransformation\" interface.\n6. When you want to glue multiple transformations together, you leverage the \"CompositeTransformation\" classes.\n7. When you call a \"CompositeTransformation\" under the hood, it calls the list of \"Transformation\" objects until it hits an \"AtomicTransformation\" object which will do the actual transformation.\nNote that because both the \"AtomicTransformation\" and \"CompositeTransformation\" inherit the \"Transformation\" interface, you can use them interchangeably, like LEGOs.\nThat is powerful.\nThat is why we all love Sklearn and their \"Pipeline\" interface ğŸ”¥\n.\nIf you want to know how to apply other software design patterns in MLE, I left in the comments an interesting article â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ndesignpattern\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHWkQTWdDff3Q/image-shrink_800/0/1694500223093?e=1705082400&v=beta&t=SIucLwWC7eEP5m4obCwTaQG9GQ4LPveUofOqcK-5A4g"
        },
        "Post_36": {
            "text": "To successfully use ğ—¥ğ—”ğ—š in your ğ—Ÿğ—Ÿğ—  ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€, your ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• must constantly be updated with the latest data.\nHere is how you can implement a ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² to keep your vector DB in sync with your datasets â†“\n.\nğ—¥ğ—”ğ—š is a popular strategy when building LLMs to add context to your prompt about your private datasets.\nLeveraging your domain data using RAG provides 2 significant benefits:\n- you don't need to fine-tune your model as often (or at all)\n- avoid hallucinations\n.\nOn the ğ—¯ğ—¼ğ˜ ğ˜€ğ—¶ğ—±ğ—², to implement RAG, you have to:\n3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n4. Get the top N closest vectors and their metadata.\n5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n6. Pass the prompt to the LLM.\n7. Insert the user question + assistant answer to the chat history.\n.\nBut the question is, ğ—µğ—¼ğ˜„ do you ğ—¸ğ—²ğ—²ğ—½ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ˜ƒğ—²ğ—°ğ˜ğ—¼ğ—¿ ğ——ğ—• ğ˜‚ğ—½ ğ˜ğ—¼ ğ—±ğ—®ğ˜ğ—² ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—¹ğ—®ğ˜ğ—²ğ˜€ğ˜ ğ—±ğ—®ğ˜ğ—®?\nâ†³ You need a real-time streaming pipeline.\nHow do you implement it?\nYou need 2 components:\nâ†³ A streaming processing framework. For example,Â BytewaxÂ is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\nğŸ”— Bytewax:\nhttps://lnkd.in/dbJDDvKB\nâ†³ A vector DB. For example,Â QdrantÂ provides a rich set of features and a seamless experience.\nğŸ”— Qdrant:\nhttps://qdrant.tech/\n.\nHere is an example of how to implement a streaming pipeline for financial news â†“\nğŸ­. Financial news data source (e.g., Alpaca):\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\nâ†’ You run this once in the beginning.\nYou need the data exposed under a web socket to ingest news in real time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\nâ†’ Listens 24/7 for financial news.\nğŸ®. Build the streaming pipeline using Bytewax:\nImplement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\nThe rest of the steps can be shared between both connectors â†“\n- Clean financial news documents.\n- Chunk the documents.\n- Embed the documents (e.g., using Bert).\n- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\nğŸ¯-ğŸ³. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4D22AQGnoFVBiprmOg/feedshare-shrink_800/0/1694424095344?e=1707350400&v=beta&t=ulw2PNkbS7g2UkrXvw17jlQ2-6e8s5m2OPvFrMHQXSI"
        },
        "Post_37": {
            "text": "After 1 year, I finally decided to ğ˜€ğ˜ğ—®ğ—¿ğ˜ ğ—½ğ—¼ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ—¼ğ—» ğ—§ğ˜„ğ—¶ğ˜ğ˜ğ—²ğ—¿ or, as others like to call it, ğ—«.\nI took this decision because everybody has a different way of reading and interacting with their socials.\n...and I want everyone to enjoy my content on their favorite platform.\nIt took me a while to make this decision as I was not a Twitter user, but despite what people say, I started using it lately and enjoyed it.\nThus... It just made sense to start posting there, but I must warn you I don't have any followers ğŸ‘€\nI even bought that stu*** blue ticker to see that I am serious about this ğŸ˜‚\nSo...\nIf you like my content and you are a Twitter/X person â†“\nFollow me on Twitter/X:\nâ†³ ğŸ”—\nhttps://lnkd.in/d5ad8YSD\nâ†³ handler: @ğ—¶ğ˜‚ğ˜€ğ˜‡ğ˜ğ—¶ğ—»ğ—½ğ—®ğ˜‚ğ—¹\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQEO3O4dH2H-zQ/image-shrink_800/0/1692685814516?e=1705082400&v=beta&t=7_pYP7T3fDfKQgTWmUM7TybCiPuB-38XMs_2YVOLv2o"
        },
        "Post_38": {
            "text": "This is how you can build a CI/CD pipeline using GitHub Actions and Docker in just a few lines of code.\nAs an ML/MLOps engineer, you should master serving models by building CI/CD pipelines.\nThe good news is that GitHub Actions + Docker simplifies building a CI/CD pipeline.\n.\nğ—ªğ—µğ˜†?\n- you can easily trigger jobs when merging various branches\n- the CI/CD jobs run on GitHub's VMs (free)\n- easy to implement: copy & paste pre-made templates + adding credentials\n.\nğ—™ğ—¼ğ—¿ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—², ğ˜ğ—µğ—¶ğ˜€ ğ—¶ğ˜€ ğ—µğ—¼ğ˜„ ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—¯ğ˜‚ğ—¶ğ—¹ğ—± ğ—® ğ—–ğ—œ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—» ğŸ¯ ğ˜€ğ—¶ğ—ºğ—½ğ—¹ğ—² ğ˜€ğ˜ğ—²ğ—½ğ˜€:\n#1. The CI pipeline is triggered when you merge your new feature branch into the main branch.\n#2. You log into the Docker Registry (or any other compatible registry such as ECR).\n#3. You build the image. Run your tests (if you have any), and if the tests pass, you push the image into the registry.\n.\nğ—§ğ—¼ ğ—¶ğ—ºğ—½ğ—¹ğ—²ğ—ºğ—²ğ—»ğ˜ ğ˜ğ—µğ—²ğ—º ğ˜‚ğ˜€ğ—¶ğ—»ğ—´ ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ ğ—”ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ˜†ğ—¼ğ˜‚ ğ—µğ—®ğ˜ƒğ—² ğ˜ğ—¼:\n- Dockerize your code\n- search \"CI Template GitHub Actions\" on Google\n- copy-paste the template\n- add your Docker Registry credentials\n...and bam... you are done.\nEasy right? The steps are similar when building your CD pipeline (deploying the new image to production).\nIf you want to see how I used GitHub Actions to build & deploy an ML system to GCP, check out my article from the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ninfrastructure\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFhSY09duAKvQ/image-shrink_800/0/1692599416201?e=1705082400&v=beta&t=C3ewhhj7MLwbqWRj6veAOcUmaBsj4Vi9XGefKx_8Tp8"
        },
        "Post_39": {
            "text": "Here are 3 techniques you must know to evaluate your LLMs quickly.\nManually testing the output of your LLMs is a tedious and painful process â†’ you need to automate it.\nIn generative AI, most of the time, you cannot leverage standard metrics.\nThus, the real question is, how do you evaluate the outputs of an LLM?\nDepending on your problem, here is what you can do â†“\n#ğŸ­. ğ—¦ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ—± ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ğ˜€ - ğ˜†ğ—¼ğ˜‚ ğ—¸ğ—»ğ—¼ğ˜„ ğ—²ğ˜…ğ—®ğ—°ğ˜ğ—¹ğ˜† ğ˜„ğ—µğ—®ğ˜ ğ˜†ğ—¼ğ˜‚ ğ˜„ğ—®ğ—»ğ˜ ğ˜ğ—¼ ğ—´ğ—²ğ˜\nEven if you use an LLM to generate text, you can ask it to generate a response in a structured format (e.g., JSON) that can be parsed.\nYou know exactly what you want (e.g., a list of products extracted from the user's question).\nThus, you can easily compare the generated and ideal answers using classic approaches.\nFor example, when extracting the list of products from the user's input, you can do the following:\n- check if the LLM outputs a valid JSON structure\n- use a classic method to compare the generated and real answers\n#ğŸ®. ğ—¡ğ—¼ \"ğ—¿ğ—¶ğ—´ğ—µğ˜\" ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ (ğ—².ğ—´., ğ—´ğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—»ğ—´ ğ—±ğ—²ğ˜€ğ—°ğ—¿ğ—¶ğ—½ğ˜ğ—¶ğ—¼ğ—»ğ˜€, ğ˜€ğ˜‚ğ—ºğ—ºğ—®ğ—¿ğ—¶ğ—²ğ˜€, ğ—²ğ˜ğ—°.)\nWhen generating sentences, the LLM can use different styles, words, etc. Thus, traditional metrics (e.g., BLUE score) are too rigid to be useful.\nYou can leverage another LLM to test the output of our initial LLM. The trick is in what questions to ask.\nWhen testing LLMs, you won't have a big testing split size as you are used to. A set of 10-100 tricky examples usually do the job (it won't be costly).\nHere, we have another 2 sub scenarios:\nâ†³ ğŸ®.ğŸ­ ğ—ªğ—µğ—²ğ—» ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ğ—»'ğ˜ ğ—µğ—®ğ˜ƒğ—² ğ—®ğ—» ğ—¶ğ—±ğ—²ğ—®ğ—¹ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ (ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ğ—»'ğ˜ ğ—µğ—®ğ˜ƒğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ)\nYou don't have access to an expert to write an ideal answer for a given question to compare it to.\nBased on the initial prompt and generated answer, you can compile a set of questions and pass them to an LLM. Usually, these are Y/N questions that you can easily quantify and check the validity of the generated answer.\nThis is known as \"Rubric Evaluation\"\nFor example:\n\"\"\"\n- Is there any disagreement between the response and the context? (Y or N)\n- Count how many questions the user asked. (output a number)\n...\n\"\"\"\nThis strategy is intuitive, as you can ask the LLM any question you are interested in as long it can output a quantifiable answer (Y/N or a number).\nâ†³ ğŸ®.ğŸ®. ğ—ªğ—µğ—²ğ—» ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼ ğ—µğ—®ğ˜ƒğ—² ğ—®ğ—» ğ—¶ğ—±ğ—²ğ—®ğ—¹ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ˜ğ—¼ ğ—°ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—² ğ˜ğ—µğ—² ğ—¿ğ—²ğ˜€ğ—½ğ—¼ğ—»ğ˜€ğ—² ğ˜ğ—¼ (ğ˜†ğ—¼ğ˜‚ ğ—µğ—®ğ˜ƒğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ)\nWhen you have access to an answer manually created by a group of experts, things are easier.\nYou will use an LLM to compare the generated and ideal answers based on semantics, not structure.\nFor example:\n\"\"\"\n(A) The submitted answer is a subset of the expert answer and entirely consistent.\n...\n(E) The answers differ, but these differences don't matter.\n\"\"\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4E10AQFVvUzUSkKEdw/image-shrink_800/0/1692253818126?e=1705082400&v=beta&t=s9-vu0ns7Xl4RNCiUhX6rg76J8K1I3GNoiiFrQRYdK0"
        },
        "Post_40": {
            "text": "Writing your own ML models is history.\nThe true value is in your data, how you prepare it, and your computer power.\nTo demonstrate my statement. Here is how you can write a Python script to train your LLM at scale in under 5 minutes â†“\n#ğŸ­. Load your data in JSON format and convert it into a Hugging Dataset\n#ğŸ®. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with the tokenizer and training & evaluation datasets.\n#ğŸ¯. Wrap your training script with a serverless solution, such as Beam, which quickly lets you access a cluster of GPUs to train large models.\nğŸš¨ As you can see, the secret ingredients are not the LLM but:\n- the amount of data\n- the quality of data\n- how you process the data\n- $$$ for compute power\n- the ability to scale the system\n.\nğŸ’¡ My advice\nâ†³ If you don't plan to become an ML researcher, shift your focus from the latest models to your data and infrastructure.\n.\nğ—¡ğ—¼ğ˜ğ—²: Integrating serverless services, such as Beam, makes the deployment of your training pipeline fast & seamless, leaving you to focus only on the last piece of the puzzle: your data.\nâ†³ğŸ”— Check out Beam's docs to find out more:\nhttps://lnkd.in/dtu2MWSp\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGIoYT_PRwkIQ/image-shrink_800/0/1692167420327?e=1705082400&v=beta&t=uCNKgRs0cnILi5Jgui4Fp6zf6ylxsRn9TpoYvMuziLg"
        },
        "Post_41": {
            "text": "ğ—ğ—¼ğ—¯ ğ—¿ğ—¼ğ—¹ğ—²ğ˜€ tell you there is just ğ—¼ğ—»ğ—² ğ˜ğ˜†ğ—½ğ—² ğ—¼ğ—³ ğ— ğ—Ÿ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´, but there are ğ—®ğ—°ğ˜ğ˜‚ğ—®ğ—¹ğ—¹ğ˜† ğŸ¯\nHere they are â†“\nThese are the 3 ML engineering personas I found while working with different teams in the industry:\n#ğŸ­. ğ—¥ğ—²ğ˜€ğ—²ğ—®ğ—¿ğ—°ğ—µğ—²ğ—¿ğ˜€ ğ˜‚ğ—»ğ—±ğ—²ğ—¿ğ—°ğ—¼ğ˜ƒğ—²ğ—¿\nThey like to stay in touch with the latest papers, understand the architecture of models, optimize them, run experiments, etc.\nThey are great at picking the best models but not that great at writing clean code and scaling the solution.\n#ğŸ®. ğ—¦ğ—ªğ—˜ ğ˜‚ğ—»ğ—±ğ—²ğ—¿ğ—°ğ—¼ğ˜ƒğ—²ğ—¿\nThey pretend they read papers but don't (maybe only when they have to). They are more concerned with writing modular code and data quality than the latest hot models. Usually, these are the \"data-centric\" people.\nThey are great at writing clean code & processing data at scale but lack deep mathematical skills to develop complex DL solutions.\n#ğŸ¯. ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—³ğ—¿ğ—²ğ—®ğ—¸ğ˜€\nThey ultimately don't care about the latest research & hot models. They are more into the latest MLOps tools and building ML systems. They love to automate everything and use as many tools as possible.\nGreat at scaling the solution and building ML pipelines, but not great at running experiments & tweaking ML models. They love to treat the ML model as a black box.\n.\nI started as #1. , until I realized I hated it - now I am a mix of:\nâ†’ #ğŸ­. 20%\nâ†’ #ğŸ®. 40%\nâ†’ #ğŸ¯. 40%\nBut that doesn't mean one is better - these types are complementary.\nA great ML team should have at least one of each persona.\nWhat do you think? Did I get it right?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQG4Bc3gA-HP5A/image-shrink_800/0/1692081017812?e=1705082400&v=beta&t=nu3M1J9RgpkB0AziriXGZZdeiCcWr0imJrLnLTZj9So"
        },
        "Post_42": {
            "text": "What is the ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² between your ğ— ğ—Ÿ ğ—±ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—ºğ—²ğ—»ğ˜ and ğ—°ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—²ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜ğ˜€?\nThey might do the same thing, but their design is entirely different â†“\nğ— ğ—Ÿ ğ——ğ—²ğ˜ƒğ—²ğ—¹ğ—¼ğ—½ğ—ºğ—²ğ—»ğ˜ ğ—˜ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜\nAt this point, your main goal is to ingest the raw and preprocessed data through versioned artifacts (or a feature store), analyze it & generate as many experiments as possible to find the best:\n- model\n- hyperparameters\n- augmentations\nBased on your business requirements, you must maximize some specific metrics, find the best latency-accuracy trade-offs, etc.\nYou will use an experiment tracker to compare all these experiments.\nAfter you settle on the best one, the output of your ML development environment will be:\n- a new version of the code\n- a new version of the configuration artifact\nHere is where the research happens. Thus, you need flexibility.\nThat is why we decouple it from the rest of the ML systems through artifacts (data, config, & code artifacts).\nğ—–ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—˜ğ—»ğ˜ƒğ—¶ğ—¿ğ—¼ğ—»ğ—ºğ—²ğ—»ğ˜\nHere is where you want to take the data, code, and config artifacts and:\n- train the model on all the required data\n- output a staging versioned model artifact\n- test the staging model artifact\n- if the test passes, label it as the new production model artifact\n- deploy it to the inference services\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub Actions):\n- builds a docker image from the code artifact (e.g., triggered manually or when a new artifact version is created)\n- start the training pipeline inside the docker container that pulls the feature and config artifacts and outputs the staging model artifact\n- manually look over the training report -> If everything went fine, manually trigger the testing pipeline\n- manually look over the testing report -> if everything worked fine (e.g., the model is better than the previous one), manually trigger the CD pipeline that deploys the new model to your inference services\nNote how the model registry quickly helps you to decouple all the components.\nAlso, because training and testing metrics are not always black & white, it is tough to 100% automate the CI/CD pipeline.\nThus, you need a human in the loop when deploying ML models.\nTo conclude...\nThe ML development environment is where you do your research to find better models:\n- ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ: data artifact\n- ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µ: code & config artifacts\nThe continuous training environment is used to train & test the production model at scale:\n- ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ: data, code, config artifacts\n- ğ˜°ğ˜¶ğ˜µğ˜±ğ˜¶ğ˜µ: model artifact\nThis is not a fixed solution, as ML systems are still an open question. I would love to see your opinion in the comments. â†“\n.\nBut if you want to see this strategy in action,\nCheck out my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE Course.\nLink in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4D10AQHkGDkmdTfbxA/image-shrink_800/0/1691994731716?e=1705082400&v=beta&t=5Ob3Dy8O4KvKsezzvFWlqpen5xjsOYcgvVKbPoEzTvE"
        },
        "Post_43": {
            "text": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways you can transform your data:\nğŸ. ğğğŸğ¨ğ«ğ ğ¬ğ­ğ¨ğ«ğ¢ğ§ğ  ğ­ğ¡ğ ğğšğ­ğš ğ¢ğ§ ğ­ğ¡ğ ğŸğğšğ­ğ®ğ«ğ ğ¬ğ­ğ¨ğ«ğ\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\nğŸŸ¢ ultra-low latency\nğŸ”´ hard to do EDA on transformed data\nğŸ”´ store duplicated/redundant data\nğŸ. ğ’ğ­ğ¨ğ«ğ ğ­ğ¡ğ ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§ ğ¢ğ§ ğ²ğ¨ğ®ğ« ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ ğ¨ğ« ğ¦ğ¨ğğğ¥ ğ©ğ«ğ-ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  ğ¥ğšğ²ğğ«ğ¬\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reduction steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\nğŸŸ¢ store only cleaned data\nğŸŸ¢ easily explore your data\nğŸ”´ the transformations are done on the client\nğŸ‘. ğ˜ğ¨ğ® ğšğ­ğ­ğšğœğ¡ ğ­ğ¨ ğğ¯ğğ«ğ² ğœğ¥ğğšğ§ğğ ğğšğ­ğš ğ¬ğ¨ğ®ğ«ğœğ ğš ğ”ğƒğ… ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\nğŸŸ¢ store only cleaned data\nğŸŸ¢ easily explore your data\nğŸŸ¢ the transformations are done on the server\nğŸŸ¢ scalable (using Spark)\nğŸ”´ hard to implement\nAs a recap,\nThere are 3 ways you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\n.\nâ†³ To see method #ğŸ®. in action\nCheck out my ğ˜ˆ ğ˜ğ˜¶ğ˜ªğ˜¥ğ˜¦ ğ˜µğ˜° ğ˜‰ğ˜¶ğ˜ªğ˜­ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜Œğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜—ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜”ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® ğ˜™ğ˜¦ğ˜´ğ˜¶ğ˜­ğ˜µğ˜´ article\nLink in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHyraJCZsv7-w/image-shrink_800/0/1691735430767?e=1705082400&v=beta&t=43FY2Ra1R3VmJqga4T13tgDdk-IJ6VdwFpHJYB82vv8"
        },
        "Post_44": {
            "text": "ğŸ³ ğ˜€ğ˜ğ—²ğ—½ğ˜€ on how to ğ—°ğ—µğ—®ğ—¶ğ—» your ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€ to build a production-ready ğ—³ğ—¶ğ—»ğ—®ğ—»ğ—°ğ—¶ğ—®ğ—¹ ğ—®ğ˜€ğ˜€ğ—¶ğ˜€ğ˜ğ—®ğ—»ğ˜ using ğ—Ÿğ—Ÿğ— ğ˜€ â†“\nWhen building LLM applications, you frequently have to divide your application into multiple steps & prompts, which are known as \"chaining prompts\".\nHere are 7 standard steps when building a financial assistant using LLMs (or any other assistant) â†“\nğ—¦ğ˜ğ—²ğ—½ ğŸ­: Check if the user's question is safe using OpenAI's Moderation API\nIf the user's query is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ® â†“\nğ—¦ğ˜ğ—²ğ—½ ğŸ®: Query your proprietary data (e.g., financial news) to enrich the prompt with fresh data & additional context.\nTo do so, you have to:\n- use an LM to embed the user's input\n- use the embedding to query your proprietary data stored in a vector DB\nğ˜•ğ˜°ğ˜µğ˜¦: You must use the same LM model to embed:\n- the data that will be stored in the vector DB\n- the user's question used to query the vector DB\nğ—¦ğ˜ğ—²ğ—½ ğŸ¯: Build the prompt using:\n- a predefined template\n- the user's question\n- extracted financial news as context\n- your conversation history as context\nğ—¦ğ˜ğ—²ğ—½ ğŸ°: Call the LLM\nğ—¦ğ˜ğ—²ğ—½ ğŸ±: Check if the assistant's answer is safe using the OpenAI's Moderation API.\nIf the assistant's answer is safe, move to ğ—¦ğ˜ğ—²ğ—½ ğŸ± â†“\nğ—¦ğ˜ğ—²ğ—½ ğŸ²: Use an LLM to check if the final answer is satisfactory.\nTo do so, you build a prompt using the following:\n- a validation predefined template\n- the user's initial question\n- the assistants answer\nThe LLM has to give a \"yes\" or \"no\" answer.\nThus, if it answers \"yes,\" we show the final answer to the user. Otherwise, we will return a predefined response, such as:\n\"Sorry, we couldn't answer your question because we don't have enough information.\"\nğ—¦ğ˜ğ—²ğ—½ ğŸ³: Add the user's question and assistant's answer to a history cache. Which will be used to enrich the following prompts with the current conversation.\nJust to remind you, the assistant should support a conversation. Thus, it needs to know what happened in the previous questions.\nâ†’ In practice, you usually keep only the latest N (question, answer) tuples or a conversation summary to keep your context length under control.\n.\nâ†³ If you want to see this strategy in action, check out our new FREE Hands-on LLMs course (work in progress) & give it a â­ to stay updated with its latest progress.\nLink in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHQ6ha-ey95XA/image-shrink_800/0/1691649020280?e=1705082400&v=beta&t=RGO3hWvTXhtqallV3uw2ows6kMD9XQUT6xnSpD-QyPk"
        },
        "Post_45": {
            "text": "Here are 4 ways to monitor and check the output prompts of any LLM to increase the reliability and accuracy of your system.\n#ğŸ­. ğ—¢ğ—½ğ—²ğ—»ğ—”ğ—£ğ—œ ğ— ğ—¼ğ—±ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—”ğ—£ğ—œ\nYou can check whether the LLM's answer is harmful with a simple API call. It classifies the prompt as hate, harassment, self-harm, sexual, and violence.\nYou don't want your LLM to become a bully without knowing it.\n#ğŸ®. ğ—Ÿğ—Ÿğ— ğ—¢ğ—½ğ˜€: ğ— ğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ ğ˜ğ—µğ—² ğ—½ğ—¿ğ—¼ğ—ºğ—½ğ˜ğ˜€\nOne part of LLMOps is to monitor, track, and see the lineage of all the prompts that come into & out of your system.\nYou can easily do that with Comet ML's LLMOps features. Link in the comments â†“\n#ğŸ¯. ğ—¨ğ˜€ğ—² ğ˜ğ—µğ—² ğ˜€ğ—®ğ—ºğ—² ğ—Ÿğ—Ÿğ—  ğ˜ğ—¼ ğ—°ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ˜† ğ˜ğ—µğ—² ğ—¼ğ˜‚ğ˜ğ—½ğ˜‚ğ˜ ğ—®ğ˜€ ğ˜€ğ—®ğ˜ğ—¶ğ˜€ğ—³ğ˜†ğ—¶ğ—»ğ—´ ğ—¼ğ—¿ ğ—»ğ—¼ğ˜\nAlong with generating text, an LLM can also be used as a classifier (without additional training).\nAfter all, outputting a class can still be considered text generation, right?\nTo do so, you have to:\n- write a system prompt: \"You are an assistant that evaluates ... respond with \"Y\" if the output is sufficient and \"N\" otherwise.\n- add the user question\n- add the LLM answer\n- add the additional context used by the LLM to generate the answers (e.g., a set of product information)\nâ†³ concatenate everything and pass it to the same LLM...\n... and vualÃ¡, you've built a monitoring system that constantly classifies the LLM's answers between satisfying or not.\n#ğŸ°. ğ—šğ—²ğ—»ğ—²ğ—¿ğ—®ğ˜ğ—² ğ—ºğ—¼ğ—¿ğ—² ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿ğ˜€ ğ—®ğ—»ğ—± ğ˜‚ğ˜€ğ—² ğ˜ğ—µğ—² ğ˜€ğ—®ğ—ºğ—² ğ—Ÿğ—Ÿğ—  ğ˜ğ—¼ ğ—½ğ—¶ğ—°ğ—¸ ğ˜ğ—µğ—² ğ—¯ğ—²ğ˜€ğ˜ ğ—®ğ—»ğ˜€ğ˜„ğ—²ğ—¿\nQuite self-explanatory.\nAnother option is letting the user pick the best option - a popular strategy for generating stuff.\nA big downside to this strategy is that it adds extra costs.\n.\nSo remember...\nThere are 4 ways to parse your LLM's outputs:\n1. use the OpenAI Moderation API\n2. log them to Comet ML\n3. build a Y/N satisfying classifier\n4. generate more options and pick the best\nHave you used any of these options? Let me know â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\nğŸ’¡  Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQHVuWwBp11U0Q/image-shrink_800/0/1690266018546?e=1705082400&v=beta&t=9DgPQZiZNycSLA21fx3WoTdtYJMB1fj4jpml4bSUaZ4"
        },
        "Post_46": {
            "text": "In the last month, I read 100+ ML monitoring articles.\nI trimmed them for you to 3 key resources:\n1. A series of excellent articles made by\nArize AI\nthat will make you understand what ML monitoring is all about.\nâ†³ğŸ”—\nhttps://lnkd.in/dDVWRujh\n2. The\nEvidently AI\nBlog, where you can find answers to all your questions regarding ML monitoring.\nâ†³ğŸ”—\nhttps://lnkd.in/du35hWp2\n3. The monitoring hands-on examples hosted by\nDataTalksClub\nwill teach you how to implement an ML monitoring system.\nâ†³ğŸ”—\nhttps://lnkd.in/d4ziHhxH\nAfter wasting a lot of time reading other resources...\nUsing these 3 resources is a solid start for learning about monitoring ML systems.\nHave you tried them?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEJh8LVPrW0KQ/image-shrink_800/0/1689920419727?e=1705082400&v=beta&t=Kn2_tZ5f_fApWKtXEWt17CP9DLkumJ6ejobTApK_dZI"
        },
        "Post_47": {
            "text": "I was uselessly spending 1000$ dollars every month on cloud machines until I started using this tool ğŸ‘‡\nTerraform!\n.\nğ…ğ¢ğ«ğ¬ğ­, ğ¥ğğ­'ğ¬ ğ®ğ§ğğğ«ğ¬ğ­ğšğ§ğ ğ°ğ¡ğ² ğ°ğ ğ§ğğğ ğ“ğğ«ğ«ğšğŸğ¨ğ«ğ¦.\nWhen you want to deploy a software application, there are two main steps:\n1. Provisioning infrastructure\n2. Deploying applications\nA regular workflow would be that before deploying your applications or building your CI/CD pipelines, you manually go and spin up your, let's say, AWS machines.\nInitially, this workflow should be just fine, but there are two scenarios when it could get problematic.\n#1. Your infrastructure gets too big and complicated. Thus, it is cumbersome and might yield bugs in manually replicating it.\n#2. In the world of AI, there are many cases when you want to spin up a GPU machine to train your models, and afterward, you don't need it anymore. Thus, if you forget to close it, you will end up uselessly paying a lot of $$$.\nWith Terraform, you can solve both of these issues.\n.\nSo...\nğ–ğ¡ğšğ­ ğ¢ğ¬ ğ“ğğ«ğ«ğšğŸğ¨ğ«ğ¦?\nIt sits on the provisioning infrastructure layer as a: \"infrastructure as code\" tool that:\n- is declarative (you focus on the WHAT, not on the HOW)\n- automates and manages your infrastructure\n- is open source\nYeah... yeah... that sounds fancy. But ğ°ğ¡ğšğ­ ğœğšğ§ ğˆ ğğ¨ ğ°ğ¢ğ­ğ¡ ğ¢ğ­?\nLet's take AWS as an example, where you have to:\n- create a VPC\n- create AWS users and permissions\n- spin up EC2 machines\n- install programs (e.g., Docker)\n- create a K8s cluster\nUsing Terraform...\nYou can do all that just by providing a configuration file that reflects the state of your infrastructure.\nBasically, it helps you create all the infrastructure you need programmatically. Isn't that awesome?\n.\nIf you want to quickly understand Terraform enough to start using it in your own projects,\nâ†³ check out my 7-minute read article: ğ˜šğ˜µğ˜°ğ˜± ğ˜”ğ˜¢ğ˜¯ğ˜¶ğ˜¢ğ˜­ğ˜­ğ˜º ğ˜Šğ˜³ğ˜¦ğ˜¢ğ˜µğ˜ªğ˜¯ğ˜¨ ğ˜ ğ˜°ğ˜¶ğ˜³ ğ˜ˆğ˜ğ˜š ğ˜ğ˜¯ğ˜§ğ˜³ğ˜¢ğ˜´ğ˜µğ˜³ğ˜¶ğ˜¤ğ˜µğ˜¶ğ˜³ğ˜¦. ğ˜œğ˜´ğ˜¦ ğ˜›ğ˜¦ğ˜³ğ˜³ğ˜¢ğ˜§ğ˜°ğ˜³ğ˜®!\nLink from the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nsoftwareengineering\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFwiTy6CMuN7A/image-shrink_800/0/1689834015158?e=1705082400&v=beta&t=qWEce49W9R-8Qr265fOPs3DR6fkcppH0iFQy_Ovk8Cg"
        },
        "Post_48": {
            "text": "One strategy that makes the ğ—±ğ—¶ğ—³ğ—³ğ—²ğ—¿ğ—²ğ—»ğ—°ğ—² ğ—¯ğ—²ğ˜ğ˜„ğ—²ğ—²ğ—» ğ—´ğ—¼ğ—¼ğ—± ğ—°ğ—¼ğ—±ğ—² ğ—®ğ—»ğ—± ğ—´ğ—¿ğ—²ğ—®ğ˜ ğ—°ğ—¼ğ—±ğ—² is adding ğ—¿ğ—²ğ˜ğ—¿ğ˜† ğ—½ğ—¼ğ—¹ğ—¶ğ—°ğ—¶ğ—²ğ˜€.\nTo manually implement them can get tedious and complicated.\nRetry policies are a must when you:\n- make calls to an external API\n- read from a queue, etc.\n.\nğ—¨ğ˜€ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—² ğ—§ğ—²ğ—»ğ—®ğ—°ğ—¶ğ˜ğ˜† ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» ğ—½ğ—®ğ—°ğ—¸ğ—®ğ—´ğ—²...\nğ˜ ğ˜°ğ˜¶ ğ˜¤ğ˜¢ğ˜¯ ğ˜²ğ˜¶ğ˜ªğ˜¤ğ˜¬ğ˜­ğ˜º ğ˜¥ğ˜¦ğ˜¤ğ˜°ğ˜³ğ˜¢ğ˜µğ˜¦ ğ˜ºğ˜°ğ˜¶ğ˜³ ğ˜§ğ˜¶ğ˜¯ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ğ˜´ ğ˜¢ğ˜¯ğ˜¥ ğ˜¢ğ˜¥ğ˜¥ ğ˜¤ğ˜¶ğ˜´ğ˜µğ˜°ğ˜®ğ˜ªğ˜»ğ˜¢ğ˜£ğ˜­ğ˜¦ ğ˜³ğ˜¦ğ˜µğ˜³ğ˜º ğ˜±ğ˜°ğ˜­ğ˜ªğ˜¤ğ˜ªğ˜¦ğ˜´, ğ˜´ğ˜¶ğ˜¤ğ˜© ğ˜¢ğ˜´:\n1. Add fixed and random wait times between multiple retries.\n2. Add a maximum number of attempts or computation time.\n3. Retry only when specific errors are thrown (or not thrown).\n... as you can see, you easily compose these policies between them.\nThe cherry on top is that you can access the statistics of the retries of a specific function:\n\"\nprint(raise_my_exception.retry.statistics)\n\"\n.\nWhat is your current strategy for adding retry policies to your Python code?\nhashtag\n#\nmachinelearning\nhashtag\n#\npython\nhashtag\n#\nsoftwareengineering\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHECGHK_U3a1g/image-shrink_800/0/1689661214323?e=1705082400&v=beta&t=Lw8bYVKOpi6qiO6hTOZJItL62FyRJuTKmuo0fHEggQY"
        },
        "Post_49": {
            "text": "ğ—›ğ—¼ğ˜„ ğ˜ğ—¼ ğ—®ğ—±ğ—± ğ—¿ğ—²ğ—®ğ—¹-ğ˜ğ—¶ğ—ºğ—² ğ—ºğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ—ºğ—²ğ˜ğ—¿ğ—¶ğ—°ğ˜€ to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\nğ—œğ—» ğ—½ğ—¿ğ—¼ğ—±ğ˜‚ğ—°ğ˜ğ—¶ğ—¼ğ—», ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—®ğ˜‚ğ˜ğ—¼ğ—ºğ—®ğ˜ğ—¶ğ—°ğ—®ğ—¹ğ—¹ğ˜† ğ—®ğ—°ğ—°ğ—²ğ˜€ğ˜€ ğ˜ğ—µğ—² ğ—´ğ—¿ğ—¼ğ˜‚ğ—»ğ—± ğ˜ğ—¿ğ˜‚ğ˜ğ—µ ğ—¶ğ—» ğŸ¯ ğ—ºğ—®ğ—¶ğ—» ğ˜€ğ—°ğ—²ğ—»ğ—®ğ—¿ğ—¶ğ—¼ğ˜€:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\nğ—™ğ—¼ğ—¿ ğ˜‚ğ˜€ğ—² ğ—°ğ—®ğ˜€ğ—²ğ˜€ ğŸ®. ğ—®ğ—»ğ—± ğŸ¯. ğ˜†ğ—¼ğ˜‚ ğ—°ğ—®ğ—» ğ—¾ğ˜‚ğ—¶ğ—°ğ—¸ğ—¹ğ˜† ğ—°ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—ºğ—¼ğ—»ğ—¶ğ˜ğ—¼ğ—¿ğ—¶ğ—»ğ—´ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—² ğ—¶ğ—» ğ˜ğ—µğ—² ğ—³ğ—¼ğ—¹ğ—¹ğ—¼ğ˜„ğ—¶ğ—»ğ—´ ğ˜„ğ—®ğ˜†:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a near real-time monitoring pipeline using Airflow and GCS, check out my article: ğ˜Œğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜›ğ˜³ğ˜¶ğ˜´ğ˜µğ˜¸ğ˜°ğ˜³ğ˜µğ˜©ğ˜º ğ˜”ğ˜“ ğ˜šğ˜ºğ˜´ğ˜µğ˜¦ğ˜®ğ˜´ ğ˜ğ˜ªğ˜µğ˜© ğ˜‹ğ˜¢ğ˜µğ˜¢ ğ˜ğ˜¢ğ˜­ğ˜ªğ˜¥ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¯ğ˜¥ ğ˜™ğ˜¦ğ˜¢ğ˜­-ğ˜›ğ˜ªğ˜®ğ˜¦ ğ˜”ğ˜°ğ˜¯ğ˜ªğ˜µğ˜°ğ˜³ğ˜ªğ˜¯ğ˜¨. Link in the comments â†“\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQE6nfAE4SVpAw/feedshare-shrink_800/0/1689424500226?e=1707350400&v=beta&t=z77jCyq0jh-myateTVvL2Fv5dGej7wMJVftWnq0y81Y"
        },
        "Post_50": {
            "text": "I wasn't expecting this, but somehow it happened.\nI recently hit 10k+ followers on LinkedIn.\nI will be honest with you guys. That was my goal for the end of 2023.\nBut I don't mind ğŸ˜‚ This was a great surprise for me.\nI have to thank you guys for following me  ğŸ™\nThis motivates me to create more and better content to help you decode ML & MLOps concepts.\nHave a fantastic day, and see you tomorrow ğŸ”¥\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata",
            "image": "https://media.licdn.com/dms/image/D4D10AQGvZkacf6SLPA/image-shrink_800/0/1689315014338?e=1705082400&v=beta&t=yOioehrE_wH3ve12D32e2UPD24c47lpOLbJGN6CqQhU"
        },
        "Post_51": {
            "text": "ğ—§ğ—¼ğ—½ ğŸ² ğ— ğ—Ÿ ğ—£ğ—¹ğ—®ğ˜ğ—³ğ—¼ğ—¿ğ—º ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ you must know and use in your ML system.\nHere they are â†“\n#ğŸ­. ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜ ğ—§ğ—¿ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#ğŸ®. ğ— ğ—²ğ˜ğ—®ğ—±ğ—®ğ˜ğ—® ğ—¦ğ˜ğ—¼ğ—¿ğ—²\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#ğŸ¯. ğ—©ğ—¶ğ˜€ğ˜‚ğ—®ğ—¹ğ—¶ğ˜€ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#ğŸ°. ğ—¥ğ—²ğ—½ğ—¼ğ—¿ğ˜ğ˜€\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#ğŸ±. ğ—”ğ—¿ğ˜ğ—¶ğ—³ğ—®ğ—°ğ˜ğ˜€\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#ğŸ². ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—¥ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ FREE course. Link in the comments â†“\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQE85Agi1BGMAA/image-shrink_800/0/1689228615215?e=1705082400&v=beta&t=HzT3hh9rPEb7qMUdX6Wzjh9w_lxadcDjs5n1f_Vo2ko"
        },
        "Post_52": {
            "text": "Here is how you can ğ—²ğ—ºğ—¯ğ—²ğ—± ğ—® ğ˜€ğ—½ğ—¿ğ—²ğ—®ğ—±ğ˜€ğ—µğ—²ğ—²ğ˜ ğ—±ğ—¶ğ—¿ğ—²ğ—°ğ˜ğ—¹ğ˜† ğ—¶ğ—»ğ˜ğ—¼ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—¦ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¹ğ—¶ğ˜ ğ—®ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» with just ğ—® ğ—³ğ—²ğ˜„ ğ—¹ğ—¶ğ—»ğ—²ğ˜€ ğ—¼ğ—³ ğ—°ğ—¼ğ—±ğ—².\nFor sure, you heard about\nTry Mito\n.\nğ˜ğ˜§ ğ˜¯ğ˜°ğ˜µ, ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜ªğ˜´ ğ˜¢ ğ˜²ğ˜¶ğ˜ªğ˜¤ğ˜¬ ğ˜³ğ˜¦ğ˜¤ğ˜¢ğ˜±:\n\"Mito is a way to incorporate an Excel-like component into your beloved notebooks to explore data without writing any Python code (it writes it for you based on your interactions).\"\n.\nRecently, they introduced the same experience, but this time for Streamlit.\nNow you can add a spreadsheet in your dashboard with just a few lines of code:\n\"\"\"\nfrom mitosheet.streamlit.v1 import spreadsheet\n# ... rest of your streamlit app\nspreadsheet()\n\"\"\"\n.\nğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜„ğ—µğ—®ğ˜ ğ—¶ğ˜ ğ—°ğ—®ğ—» ğ—±ğ—¼:\n- Import, clean, and transform datasets into a format required by the rest of the Streamlit app.\n- Do flexible data exploration and analysis.\n- Create Python scripts using Mito's code-gen capabilities.\n.\nAccess the docs in the comments â†“\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQH8mAym6SvqHg/image-shrink_800/0/1687242012148?e=1705082400&v=beta&t=8EFx7xxarg74R-sQJMimfPhMH1hcZL2ykQkEtTEaQhU"
        },
        "Post_53": {
            "text": "Looking for a hub where to ğ—¹ğ—²ğ—®ğ—¿ğ—» ğ—®ğ—¯ğ—¼ğ˜‚ğ˜ ğ— ğ—Ÿ ğ—²ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—³ğ—¿ğ—¼ğ—º ğ—¿ğ—²ğ—®ğ—¹-ğ˜„ğ—¼ğ—¿ğ—¹ğ—± ğ—²ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—²ğ—»ğ—°ğ—²?\nThen, I want to let you know that I just launched my personal site, where I will constantly aggregate my:\n- courses\n- articles\n- talks\n...and more\nâ†’ Sweet part: Everything will revolve around MLE & MLOps\nIt is still a work in progress...\nBut please check it out and let me know what you think â†“\nYour opinion is deeply appreciated ğŸ™\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQGz5m27QZuHGg/feedshare-shrink_800/0/1687236920136?e=1707350400&v=beta&t=Pb65fX_86R5VsAsVtWshtNRzCzT0onebA8oQz-VwsyU"
        },
        "Post_54": {
            "text": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\n.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\nğ“ğ¡ğğ¬ğ ğšğ«ğ ğ­ğ¡ğ ğ¦ğšğ¢ğ§ ğ¬ğ­ğğ©ğ¬ ğ¨ğŸ ğš ğ›ğšğ­ğœğ¡ ğšğ«ğœğ¡ğ¢ğ­ğğœğ­ğ®ğ«ğ:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\n.\nğ˜›ğ˜©ğ˜¦ ğ˜®ğ˜¢ğ˜ªğ˜¯ ğ˜¥ğ˜°ğ˜¸ğ˜¯ğ˜´ğ˜ªğ˜¥ğ˜¦ of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real-time, and it will update the predictions only at the end of the day.\nMoving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\n.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your usual strategy to serve models is â†“\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFsY8se-q7v6w/image-shrink_800/0/1686896419062?e=1705082400&v=beta&t=YGNi-lOIAne-g3mFn2DkKSjAo6rHpe1etjhLx4XD7bk"
        },
        "Post_55": {
            "text": "Your quick guide on how diffusion models learn to predict your favorite images.\n.\nğ—¤ğ˜‚ğ—¶ğ—°ğ—¸ ğ—¿ğ—²ğ—ºğ—¶ğ—»ğ—±ğ—²ğ—¿!\nA diffusion model takes a noisy image as input and outputs the noise level from the image.\nAt inference time, you take the input image and subtract the predicted noise from it.\nAlso, A diffusion model is parameterized by a timestamp T that reflects the diffusion process from T to 0.\nThus, for different timestamps, it predicts different levels of noise.\nWhen the timestamp is near T, the model expects noisier images.\nAs it approaches 0, the expected noise level in the image is reduced.\n.\nğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜ğ—µğ—² ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ ğ—¹ğ—¼ğ—¼ğ—½ ğŸ‘‡\n1. Sample a training image from the dataset.\n1. Sample timestamp t from the interval [0, T], which determines the noise level.\n2. Sample the noise.\n3. Add the noise to the image based on the sample timestamp t.\n4. Pass it through the diffusion model, which predicts the noise from the image.\n5. Use an MSE loss to compare the predicted noise with the true one.\n6. Use backpropagation to update the model.\n6. Repeat!\nFollowing this training strategy, the model learns to differentiate between the actual information from an image (e.g., the features of a cat) and the noise.\n.\nTo summarize...\nTo train a diffusion model you:\n- add noise to an image based on timestamp t\n- the models learn to predict the noise from timestamp t\n- you use MSE as a loss to compare the real noise with the predicted noise\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\nstablediffusion\n-----\nğŸ’¡ Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQEF8BtilstppQ/image-shrink_800/0/1686810013162?e=1705082400&v=beta&t=wRCmrZtbAKf7po6Maa4Wpc1rxOtFeWXArqTSYsN3NRE"
        },
        "Post_56": {
            "text": "Don't know what ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—¦ğ˜ğ—¼ğ—¿ğ—² to use?\nI recommend you check out\nHopsworks\n.\nI had an excellent time using it while implementing ğ—§ğ—µğ—² ğ—™ğ˜‚ğ—¹ğ—¹ ğ—¦ğ˜ğ—®ğ—°ğ—¸ ğŸ³-ğ—¦ğ˜ğ—²ğ—½ğ˜€ ğ— ğ—Ÿğ—¢ğ—½ğ˜€ ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜„ğ—¼ğ—¿ğ—¸ free course.\nThey provide:\n- an intuitive Python package to interface with their platform\n- a robust documentation\n- all the features you need from a feature store\nThey are still a fast-growing company, so the tool is evolving, making it more robust & feature-rich.\nWhile developing the course, I didn't know the team or have any connections with them, but they were kind enough to approach me and send me a gift.\nI love the logo & branding on their items. ğŸ”¥\nI guess I have no other option but to drink this weekend ğŸ˜‚\nThank you,\nHopsworks\n!\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nfeaturestore\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFPhgv-9PyAeg/image-shrink_800/0/1686637213150?e=1705082400&v=beta&t=YVhnHy5TUvtxDTvqyIJVFtnwYHK0IYgrQEpAh0cYoT0"
        },
        "Post_57": {
            "text": "ğ—¨ğ—»ğ—¶ğ—³ğ˜† ğ—¯ğ—®ğ˜ğ—°ğ—µ ğ—®ğ—»ğ—± ğ˜€ğ˜ğ—¿ğ—²ğ—®ğ—ºğ—¶ğ—»ğ—´ ğ— ğ—Ÿ ğ—½ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²ğ˜€\nWhat happens if you want to introduce a real-time/streaming data source into your system?\nYou cry. Just kidding. It is a lot easier than it sounds.\nLet's get some context.\nUntil now, you used only a static data source to train your model & compute your features.\nBut you find out that your business wants to use real-time news feeds as features for your model.\nğ—ªğ—µğ—®ğ˜ ğ—±ğ—¼ ğ˜†ğ—¼ğ˜‚ ğ—±ğ—¼?\nYou have to implement 2 ğ˜®ğ˜¢ğ˜ªğ˜¯ ğ˜±ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜ºğ˜°ğ˜¶ğ˜³ ğ˜¯ğ˜¦ğ˜¸ ğ˜´ğ˜µğ˜³ğ˜¦ğ˜¢ğ˜®ğ˜ªğ˜¯ğ˜¨ ğ˜ªğ˜¯ğ˜±ğ˜¶ğ˜µ ğ˜´ğ˜°ğ˜¶ğ˜³ğ˜¤ğ˜¦:\n#ğŸ­. One that will quickly transform the raw data into features and make them accessible into the feature store to be used by the production services.\n#ğŸ®. One that will store the raw data in the static raw data source (e.g., a warehouse) so it will be used later for experimentation and research.\nBefore ingesting into your system, the real-time data source might need an extra processing step to standardize and adapt the data format to your interface.\nA standard strategy for:\n#ğŸ­. Kafka as your streaming platform\n#ğŸ®. Flink/Kafka Streams as your streaming processing units\nFor step #2. most of the time, you will have access to out-of-the-box data connectors that quickly load the real-time data into your static data storage (e.g., from Kafka to an S3 bucket or Big Query data warehouse).\nTo conclude...\nTo add a streaming data source to your current infrastructure, you need the following:\n- Kafka\n- Flink/Kafka Streams\n- to move your streaming data source into your static one\n- to quickly compute features and load them into the feature store\nThus, it isn't hardâ€”just a lot of infrastructure to set up.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFm1MPAqvvB8Q/image-shrink_800/0/1684306818157?e=1705082400&v=beta&t=glcu9B71RfiBr9fjV8wSi2li6vhEdmB60BGki0H1Tqg"
        },
        "Post_58": {
            "text": "Are you into MLOps and ML Engineering?\nI was honored to pour my MLE & MLOps wisdom into the podcast ğ˜“ğ˜¦ğ˜µ'ğ˜´ ğ˜›ğ˜¢ğ˜­ğ˜¬ ğ˜ˆğ˜ hosted.\nI had a great time talking with\nThomas Bustos\n, where he had some fantastic questions about:\n- building and engineering AI systems\n- finding your niche in AI\n- different ML job positions\n- Airflow for automating ML\n- deploying multiple versions and communicating effectively\n- explaining technical complexity to customers\n... and more\nIf this sounds like something you are interested in, check it out ğŸ‘‡\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQH6UaaJhK3--Q/image-shrink_800/0/1684221352292?e=1705082400&v=beta&t=L19Jwyd3RGuI5I-yDXgwz-5VntzN659z88kdPDzmAGU"
        },
        "Post_59": {
            "text": "ğ—¦ğ˜‚ğ—½ğ—²ğ—¿ğ—°ğ—µğ—®ğ—¿ğ—´ğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ— ğ—Ÿ ğ˜€ğ˜†ğ˜€ğ˜ğ—²ğ—º: ğ˜‚ğ˜€ğ—² ğ—® ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¿ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜†\nA model registry is the holy grail of any production-ready ML system.\nThe model registry is the critical component that decouples your offline pipeline (experimental/research phase) from your production pipeline.\nğ—–ğ—¼ğ—ºğ—½ğ˜‚ğ˜ğ—² ğ—¢ğ—³ğ—³ğ—¹ğ—¶ğ—»ğ—² ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€\nUsually, when training your model, you use a static data source.\nUsing a feature engineering pipeline, you compute the necessary features used to train the model.\nThese features will be stored inside a features store.\nAfter processing your data, your training pipeline creates the training & testing splits and starts training the model.\nThe output of your training pipeline is the trained weights, also known as the model artifact.\nğ—›ğ—²ğ—¿ğ—² ğ—¶ğ˜€ ğ˜„ğ—µğ—²ğ—¿ğ—² ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¿ğ—²ğ—´ğ—¶ğ˜€ğ˜ğ—¿ğ˜† ğ—¸ğ—¶ğ—°ğ—¸ğ˜€ ğ—¶ğ—» ğŸ‘‡\nThis artifact will be pushed into the model registry under a new version that can easily be tracked.\nSince this point, the new model artifact version can be pulled by any serving strategy:\n#1. batch\n#2. request-response\n#3. streaming\nYour inference pipeline doesnâ€™t care how the model artifact was generated. It just has to know what model to use and how to transform the data into features.\nNote that this strategy is independent of the type of model & hardware you use:\n- classic model (Sklearn, XGboost),\n- distributed system (Spark),\n- deep learning model (PyTorch)\nTo summarize...\nUsing a model registry is a simple and effective method to:\n-> detach your experimentation from your production environment,\nregardless of what framework or hardware you use.\nTo learn more, check out my practical & detailed example of how to use a model registry in my article: ğ˜ˆ ğ˜ğ˜¶ğ˜ªğ˜¥ğ˜¦ ğ˜µğ˜° ğ˜‰ğ˜¶ğ˜ªğ˜­ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜Œğ˜§ğ˜§ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜›ğ˜³ğ˜¢ğ˜ªğ˜¯ğ˜ªğ˜¯ğ˜¨ ğ˜—ğ˜ªğ˜±ğ˜¦ğ˜­ğ˜ªğ˜¯ğ˜¦ğ˜´ ğ˜§ğ˜°ğ˜³ ğ˜”ğ˜¢ğ˜¹ğ˜ªğ˜®ğ˜¶ğ˜® ğ˜™ğ˜¦ğ˜´ğ˜¶ğ˜­ğ˜µğ˜´\nLink in the comments ğŸ‘‡\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\nğŸ’¡ Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHL0Zcqj1dXEA/image-shrink_800/0/1683788419175?e=1705082400&v=beta&t=wfbz-WEVU78rH95Vy3FR7yiv4sXv8-6KbwqVf6kkwNk"
        },
        "Post_60": {
            "text": "I usually donâ€™t care about someone's nationality, but damn, I am proud of my country Romania ğŸ‰\nhashtag\n#\nmathematics\nhashtag\n#\nromania",
            "image": "https://media.licdn.com/dms/image/D4D10AQGLL5T6K2SNDA/image-shrink_800/0/1683704670833?e=1705082400&v=beta&t=njHqMcTrrzmGxwBOB7fG_ZCoP46xFeilb9y0MKF2MRw"
        },
        "Post_61": {
            "text": "I never forget anything. Said no one but only your second brain.\nAfter 6+ months of refinement, this is my second brain strategy ğŸ‘‡\nTiago's Forte book inspired me, but I adapted his system to my needs.\n.\n#ğŸ¬. ğ—–ğ—¼ğ—¹ğ—¹ğ—²ğ—°ğ˜\nThis is where you are bombarded with information from all over the place.\n#ğŸ­. ğ—§ğ—µğ—² ğ—šğ—¿ğ—®ğ˜ƒğ—²ğ˜†ğ—®ğ—¿ğ—±\nThis is where I save everything that looks interesting.\nI won't use 90% of what is here, but it satisfied my urge to save that \"cool article\" I saw on LinkedIn.\nTools: Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists, etc.\n#ğŸ®. ğ—§ğ—µğ—² ğ—•ğ—¼ğ—®ğ—¿ğ—±\nHere, I start converging the information and planning what to do next.\nTools: Notion\n#ğŸ¯. ğ—§ğ—µğ—² ğ—™ğ—¶ğ—²ğ—¹ğ—±\nHere is where I express myself through learning, coding, writing, etc.\nTools: whatever you need to express yourself.\n2 & 3 are iterative processes. Thus I often bounce between them until the information is distilled.\n#ğŸ°. ğ—§ğ—µğ—² ğ—ªğ—®ğ—¿ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²\nHere is where I take the distilled information and write it down for cold storage.\nTools: Notion, Google Drive\n.\nWhen I want to search for a piece of information, I start from the Warehouse and go backward until I find what I need.\nAs a minimalist, I  kept my tools to a minimum. I primarily use only: Brave, Notion, and Google Drive.\nYou don't need 100+ tools to be productive. They just want to take your money from you.\nSo remember...\nYou have to:\n- collect\n- link\n- plan\n- distill\n- store\nWhat is your second brain strategy? Leave your thoughts in the comments.\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain\nhashtag\n#\nmachinelearning\n-----\nğŸ’¡ Follow me for weekly insights about designing ML systems.",
            "image": "https://media.licdn.com/dms/image/D4D22AQFhN9q0xEvHqA/feedshare-shrink_800/0/1683286221390?e=1707350400&v=beta&t=9ftIcuPqc_gnlCd9tkH4rzyItg7oukmcYSapa9FP7ic"
        },
        "Post_62": {
            "text": "Have you ever searched for yourself on ChatGPT?\nDo you think this is the next generation of hiring and dating?\nThis is what it returned after prompting: \"You are a private detective. Tell me everything you know about Paul Iusztin.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nchatgpt\n-----\nğŸ’¡ Follow me for weekly insights about designing ML systems.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFD5wyVmvXyQg/image-shrink_800/0/1683183618617?e=1705082400&v=beta&t=ZlKX0EQtkG1YcTfXVqv2hX-ZeLfsouFyTgZanQfElCc"
        },
        "Post_63": {
            "text": "The Perfect DUO: FastAPI + Streamlit\n2 tools you should know as an ML Engineer\nHere are 2 reasons why FastAPI & Streamlit should be in your MLE stack ğŸ‘‡\n#ğŸ­. ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—», ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—», ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—»!\nAs an MLE, Python is your magic wand.\nUsing FastAPI & Streamlit, you can build full-stack web apps using solely Python.\n#ğŸ®. ğ—˜ğ˜…ğ˜ğ—¿ğ—²ğ—ºğ—²ğ—¹ğ˜† ğ—³ğ—¹ğ—²ğ˜…ğ—¶ğ—¯ğ—¹ğ—²\nUsing FastAPI & Streamlit, you can deploy an ML model in almost any scenario.\n<< ğ˜‰ğ˜¢ğ˜µğ˜¤ğ˜© >>\nExpose the predictions from any storage, such as S3 or Redis, using FastAPI as REST endpoints.\nVisualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< ğ˜™ğ˜¦ğ˜²ğ˜¶ğ˜¦ğ˜´ğ˜µ-ğ˜™ğ˜¦ğ˜´ğ˜±ğ˜°ğ˜¯ğ˜´ğ˜¦ >>\nWrap your model using FastAPI and expose its functionalities as REST endpoints.\nYet again... visualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< ğ˜šğ˜µğ˜³ğ˜¦ğ˜¢ğ˜® >>\nWrap your model using FastAPI and expose it as REST endpoints.\nBut this time, the REST endpoints will be called from a Flink or Kafka Streams microservice.\n.\nUsing this tech stack won't be the most optimal solution in 100% use cases,\n... but in most cases:\n- it will get the job done\n- you can quickly prototype almost any ML application.\n.\nSo remember...\nYou should learn FastAPI & Streamlit because:\n- Python all the way!\n- you can quickly deploy a model in almost any architecture scenario\nDo you use FastAPI & Streamlit?\nTo learn more, check out my Medium article ğ˜ğ˜¢ğ˜´ğ˜µğ˜ˆğ˜—ğ˜ ğ˜¢ğ˜¯ğ˜¥ ğ˜šğ˜µğ˜³ğ˜¦ğ˜¢ğ˜®ğ˜­ğ˜ªğ˜µ: ğ˜›ğ˜©ğ˜¦ ğ˜—ğ˜ºğ˜µğ˜©ğ˜°ğ˜¯ ğ˜‹ğ˜¶ğ˜° ğ˜ ğ˜°ğ˜¶ ğ˜”ğ˜¶ğ˜´ğ˜µ ğ˜’ğ˜¯ğ˜°ğ˜¸ ğ˜ˆğ˜£ğ˜°ğ˜¶ğ˜µ. Link in the comments ğŸ‘‡\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npython\n-----\nğŸ’¡ Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/D4D10AQG0vAyQB1gZeA/image-shrink_800/0/1683097221571?e=1705082400&v=beta&t=UexIMwfQHpZzg-c8R_pdMTUn2F5RE4RpIa6OyCLhKlg"
        },
        "Post_64": {
            "text": "The saddest truth about programming.\nEven amplified if you work in AI.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelliegence\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/D4D10AQFknHQZaDMvoQ/image-shrink_800/0/1680764421146?e=1705082400&v=beta&t=5-MPY9CFT_NwQb_1L5d-D5_HpIjfjtsQKcmyKJUon5A"
        },
        "Post_65": {
            "text": "Close to releasing all the lessons from The Full Stack 7-Steps MLOps Framework.\nhashtag\n#\nlearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/D4D10AQF8ncTNw73s7A/image-shrink_800/0/1680159618403?e=1705082400&v=beta&t=-hfCmObDdAFltKxlsxjnSl0-rTGnf2iz3GZUz8A0x74"
        },
        "Post_66": {
            "text": "As\nPaul Iusztin\n's series continues to explore a 7-step MLOps framework, a new installment explains how to consume and visualize your model's predictions.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFybK57dgdnyQ/image-shrink_1280/0/1680073215153?e=1705082400&v=beta&t=Ne8rem0KQTZ4pXTdcOq0_DvDURsqNR3XsQrDrm0kjCE"
        },
        "Post_67": {
            "text": "I am always anxious that I forget what I read.\nThat is why I built my second brain.\nIf you are working in any data/AI-related field, you are constantly bombarded with information.\nMuch of it is noise,  but some will be of great value.\n.\nHere are the main ideas I extracted from the ğ˜‰ğ˜¶ğ˜ªğ˜­ğ˜¥ğ˜ªğ˜¯ğ˜¨ ğ˜¢ ğ˜šğ˜¦ğ˜¤ğ˜°ğ˜¯ğ˜¥ ğ˜‰ğ˜³ğ˜¢ğ˜ªğ˜¯ ğ˜£ğ˜º ğ˜›ğ˜ªğ˜¢ğ˜¨ğ˜° ğ˜ğ˜°ğ˜³ğ˜µğ˜¦ famous book ğŸ‘‡\n#ğŸ­. Your brain's job is to plan, create and enjoy life, not to remember things.\n#ğŸ®. Four steps to follow:\n1. Capture the relevant ideas\n2. Organize by actionability\n3. Distill your notes to prevent from hoarding\n4. Use & express your work\n#ğŸ¯. Follow the PARA system to organize your information:\nP - Projects: bounded responsibilities\nA - Areas: timeless responsibilities\nR - Resources: useful data\nA - Archive: your graveyard\n#ğŸ°. Main productivity tools:\n- calendar\n- TODOist\n- note-taking app\n- Notion: the central hub\nThe tools are not revolutionary.\nYou need the right mindset to apply them consistently in the right way.\n.\nTo conclude...\nYou must create a robust system to capture & use all the essential information from your environment.\nOtherwise, you will overload your brain, forgetting important information and wasting opportunities.\nDo you have a second brain?\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain",
            "image": "https://media.licdn.com/dms/image/C4D10AQEYvoupSYzw1g/image-shrink_800/0/1678348835498?e=1705082400&v=beta&t=YGcB3s38hVroIz2VxWgq3LjR4S5OFRTiRFM8n3QBf3s"
        },
        "Post_68": {
            "text": "A wise man said: ğ˜ƒğ—®ğ—¹ğ—¶ğ—±ğ—®ğ˜ğ—² ğ—²ğ˜ƒğ—²ğ—¿ğ˜†ğ˜ğ—µğ—¶ğ—»ğ—´!\n100% you heard that data validation is good...\nbut where should we validate the data? Everywhere!\nThat might be an overstatement, but let me explain.\nWhen the outputs of an ML model are poor, there are 1000+ reasons why that happened.\nBut even if you know that the issue is data related...\nNarrowing down to the actual function that messed up everything is extremely hard.\nThus, by adding data validation before & after:\n- the ingestion ETL;\n- the data engineering pipeline;\n- the feature engineering pipeline;\nyou might add some redundancy, but this will make scanning for errors extremely easy.\n.\nImagine that you would have a data validation check only after the FE pipeline. If that fails, you know it failed ğ˜£ğ˜¶ğ˜µ ğ˜¥ğ˜°ğ˜¯'ğ˜µ ğ˜¬ğ˜¯ğ˜°ğ˜¸ ğ˜¸ğ˜©ğ˜¦ğ˜³ğ˜¦ ğ˜ªğ˜µ ğ˜§ğ˜¢ğ˜ªğ˜­ğ˜¦ğ˜¥.\nIf the system is small, that is not an issue, but imagine you have 100+ transformations spread across multiple teams...\nğŸ¥² Finding the right error might take you hours or even days.\nğŸ’› By adding multiple data validation points in your system, you can quickly answer to: \"where the system failed\".\nThus, by adding data validation in multiple, you automatically slice the pipeline making it easy to diagnose.\n.\nNote that this is just an example. Your data infrastructure might look different.\nBut the fundamental idea remains the same. Add data validation in all the essential points of your data pipelines to quickly slice and dice the upcoming errors.\nIf you want a hands-on example about using GE to validate your data, check out my article ğ˜Œğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜›ğ˜³ğ˜¶ğ˜´ğ˜µğ˜¸ğ˜°ğ˜³ğ˜µğ˜©ğ˜º ğ˜”ğ˜“ ğ˜šğ˜ºğ˜´ğ˜µğ˜¦ğ˜®ğ˜´ ğ˜ğ˜ªğ˜µğ˜© ğ˜‹ğ˜¢ğ˜µğ˜¢ ğ˜ğ˜¢ğ˜­ğ˜ªğ˜¥ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¯ğ˜¥ ğ˜™ğ˜¦ğ˜¢ğ˜­-ğ˜›ğ˜ªğ˜®ğ˜¦ ğ˜”ğ˜°ğ˜¯ğ˜ªğ˜µğ˜°ğ˜³ğ˜ªğ˜¯ğ˜¨. Link in the comments ğŸ‘‡\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C4D10AQFnA5xq6M00Jg/image-shrink_800/0/1678089625961?e=1705082400&v=beta&t=kh1pU6ZkncYYICMWcTCDgX46K2CeIwox-yC2Fr4b3dU"
        },
        "Post_69": {
            "text": "Do you want to learn ML & MLOps from real-world experience?\nThen I suggest you join\nPau Labarta Bajo\nReal-World Machine Learning\nweekly newsletter, along with another 7.5 ML developer.\nPau Labarta Bajo\nis a great teacher that makes learning seamless âœŒ\nâ†’\nhttps://lnkd.in/dXx-GxN8\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/C4D22AQHTedzl38JhtQ/feedshare-shrink_800/0/1677919949980?e=1707350400&v=beta&t=LIuW6QKyYyZIPI5rylNx8ODkYO3_puu5wi4MgU07G2w"
        },
        "Post_70": {
            "text": "ğ——ğ—®ğ˜ğ—® ğ˜ƒğ—®ğ—¹ğ—¶ğ—±ğ—®ğ˜ğ—¶ğ—¼ğ—» shouldn't be hard.\nHere is your data validation guide in under 2 minutes ğŸ‘‡\nData validation ensures the integrity and quality of your data ingested automatically into your ML system.\nThus, implementing your data validation layer is crucial in any successful ML system.\n.\nğŸ§˜ğŸ¼â€â™‚ï¸ ğ˜ğ˜³ğ˜¦ğ˜¢ğ˜µ ğ˜Œğ˜¹ğ˜±ğ˜¦ğ˜¤ğ˜µğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ğ˜´ makes everything straightforward.\nUsing GE, you must stack multiple ğ˜Œğ˜¹ğ˜±ğ˜¦ğ˜¤ğ˜µğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ğ˜Šğ˜°ğ˜¯ğ˜§ğ˜ªğ˜¨ğ˜¶ğ˜³ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ objects, where each object checks a single rule/feature.\nFor example:\n\"\"\"\nExpectationConfiguration(\nexpectation_type=\"expect_column_distinct_values_to_be_in_set\",\nkwargs={\"column\": \"area\", \"value_set\": (0, 1, 2)},\n)\n\"\"\"\n, checks if the \"area\" feature contains only values equal to 0, 1 or 2.\nThe most common checks you have to do are for the following:\n- the schema of the table;\n- the type of each column;\n- the values of each column: an interval for continuous variables or an expected set for discrete variables;\n- null values.\n.\nAfter you run your GE validation suit, you will get a success %.\nBased on the success % you can make various decisions, such as:\nğŸŸ¢  == 100% - ingest the data without an alert\nğŸŸ¡  >=90% - ingest the data with an alert\nğŸ”´   <90% - drop the data with an error\nP.S. Using GE +\nHopsworks\nas your Feature Store makes everything even simpler ğŸ”¥\n.\nSo remember...\nGE makes implementing your data validation layer straightforward.\nYou have to check every feature for a given set of rules.\nBased on the success % you have to take various actions.\nIf you want a hands-on example about using GE to validate your data, check out my article ğ˜Œğ˜¯ğ˜´ğ˜¶ğ˜³ğ˜ªğ˜¯ğ˜¨ ğ˜›ğ˜³ğ˜¶ğ˜´ğ˜µğ˜¸ğ˜°ğ˜³ğ˜µğ˜©ğ˜º ğ˜”ğ˜“ ğ˜šğ˜ºğ˜´ğ˜µğ˜¦ğ˜®ğ˜´ ğ˜ğ˜ªğ˜µğ˜© ğ˜‹ğ˜¢ğ˜µğ˜¢ ğ˜ğ˜¢ğ˜­ğ˜ªğ˜¥ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜¢ğ˜¯ğ˜¥ ğ˜™ğ˜¦ğ˜¢ğ˜­-ğ˜›ğ˜ªğ˜®ğ˜¦ ğ˜”ğ˜°ğ˜¯ğ˜ªğ˜µğ˜°ğ˜³ğ˜ªğ˜¯ğ˜¨. Link in the comments ğŸ‘‡\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C5622AQE8-0GY_fbvdg/feedshare-shrink_800/0/1677826092295?e=1707350400&v=beta&t=sz6CMOI_4dj1uAheZYW98aErSI7OzPSy_4rO3cVINVY"
        },
        "Post_71": {
            "text": "Master writing clean & modular Terraform files.\nUsing this one simple technique:\nVariables\nLet's take a look at how to:\n- define a variable\n- reference a variable\n- assign a value to a variable\n- use files to assign values to variables\n... in Terraform.\nNote that assigning values in Terraform is quite strange, as in your Terraform file, you define the structure and type of the variable. The value is assigned only on runtime.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelliegence\nhashtag\n#\nmlops\n-----\nğŸ’¡ Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHKt-pNZ90O9g/image-shrink_800/0/1676884514444?e=1705082400&v=beta&t=JFgFodA6CsV5ORPHTqrRYCc-CCLFcGPne7m0Z3oxZQY"
        },
        "Post_72": {
            "text": "90% of the ML models start being served in batch mode.\nThus, as an ML engineer, learning to train and serve a model in batch mode successfully is your first step to success.\nYou need to know how to implement the following:\n- a feature engineering pipeline\n- a training pipeline\n- a batch prediction pipeline\n- a feature store\n- an ML Platform\n- a storage system to store your predictions\nMany ML systems start in batch mode and naturally move toward other architectures.\nThis happens because serving an ML model in batch mode is the fastest without spending too much time on constraints such as low latency and high throughput.\nRemember that while in production, if your model isn't running fast enough, it will probably be useless.\nIf you want to learn how to implement the steps above step-by-step practically,\nI want to let you know that...\nI finally released the first 3 lessons for  \"The Full Stack 7-Steps MLOps Framework\" FREE course that will teach you just that.\nThe lessons are accessible on Towards Data Science Medium's publication.\nCheck the comments for the links...\nAnd start learning about training and serving your ML model in batch mode.\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/C4D22AQEB4t8RjoaRsw/feedshare-shrink_800/0/1676127628660?e=1707350400&v=beta&t=KBk3hv2ojdxmDW80fvGwUSjt2Y41E92FdwuzvPclloQ"
        },
        "Post_73": {
            "text": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\nğ“ğ¡ğğ¬ğ ğšğ«ğ ğ­ğ¡ğ ğ¦ğšğ¢ğ§ ğ¬ğ­ğğ©ğ¬ ğ¨ğŸ ğš ğ›ğšğ­ğœğ¡ ğšğ«ğœğ¡ğ¢ğ­ğğœğ­ğ®ğ«ğ:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\nThe main downside of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real time, and it will update the predictions only at the end of the day.\nThat is why moving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your strategy is.\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nğŸ’¡ My goal is to help machine learning engineers level up in designing and productionizing ML systems.\nğŸ‘‰ Follow me for weekly insights.",
            "image": "https://media.licdn.com/dms/image/C4D10AQF1jBJF8PzmRQ/image-shrink_800/0/1676625314479?e=1705082400&v=beta&t=mE2kLlpXPQOKYmkSYCMrlhxqOsvZdutIWiSVT70Xxzc"
        },
        "Post_74": {
            "text": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways how you can transform your data:\nğŸ. ğğğŸğ¨ğ«ğ ğ¬ğ­ğ¨ğ«ğ¢ğ§ğ  ğ­ğ¡ğ ğğšğ­ğš ğ¢ğ§ ğ­ğ¡ğ ğŸğğšğ­ğ®ğ«ğ ğ¬ğ­ğ¨ğ«ğ\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\nğŸŸ¢ ultra-low latency\nğŸ”´ hard to do EDA on transformed data\nğŸ”´ store duplicated/redundant data\nğŸ. ğ’ğ­ğ¨ğ«ğ ğ­ğ¡ğ ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§ ğ¢ğ§ ğ²ğ¨ğ®ğ« ğ©ğ¢ğ©ğğ¥ğ¢ğ§ğ ğ¨ğ« ğ¦ğ¨ğğğ¥ ğ©ğ«ğ-ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  ğ¥ğšğ²ğğ«ğ¬\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reductions steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\nğŸŸ¢ store only cleaned data\nğŸŸ¢ easily explore your data\nğŸ”´ the transformations are done on the client\nğŸ‘. ğ˜ğ¨ğ® ğšğ­ğ­ğšğœğ¡ ğ­ğ¨ ğğ¯ğğ«ğ² ğœğ¥ğğšğ§ğğ ğğšğ­ğš ğ¬ğ¨ğ®ğ«ğœğ ğš ğ”ğƒğ… ğ­ğ«ğšğ§ğ¬ğŸğ¨ğ«ğ¦ğšğ­ğ¢ğ¨ğ§\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\nğŸŸ¢ store only cleaned data\nğŸŸ¢ easily explore your data\nğŸŸ¢ the transformations are done on the server\nğŸŸ¢ scalable (using Spark)\nğŸ”´ hard to implement\nAs a recap,\nThere are 3 ways when you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nI just started my ML engineering weekly newsletter.\nSubscribe to level up in building ML systems:\nhttps://lnkd.in/dsMR4ivA",
            "image": "https://media.licdn.com/dms/image/C4D22AQHm2wmWIwnJ7Q/feedshare-shrink_800/0/1676121104276?e=1707350400&v=beta&t=h6L-7qXJOca3jNcnQQxKYHJspKUnNfRbnncruyNiFQo"
        },
        "Post_75": {
            "text": "After 3 months of overthinking...\nI finally did it.\nAs I value people's time, and I understand that everybody has their way of reading...\nI want to announce,\nThat I will start my free weekly ML engineering newsletter.\n.\nThe mission of my newsletter will be the same as my LinkedIn content:\n\"To help machine learning engineers level up in designing and productionizing ML systems.\"\nI will do my best to provide the best value for your time.\n.\nIf you never want to miss my weekly insights about ML engineering,\nSubscribe to my free newsletter here:\nhttps://lnkd.in/dsMR4ivA\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/C4D10AQHuTXQlOvxAUw/image-shrink_800/0/1676282268515?e=1705082400&v=beta&t=4pyghbq-GoBA615bvVg_Nze1OskhDuc_W3bg_DAg8Kc"
        },
        "Post_76": {
            "text": "In the last month, I read 100+ ML monitoring articles.\nI trimmed them for you to 3 key resources:\n1. A series of excellent articles made by\nArize AI\nthat will make you understand what ML monitoring is all about.\nLink:\nhttps://lnkd.in/dDVWRujh\n2. The\nEvidently AI\nBlog, where you can find answers to all your questions regarding ML monitoring.\nLink:\nhttps://lnkd.in/du35hWp2\n3. The monitoring hands-on examples hosted by\nDataTalksClub\nwill teach you how to implement an ML monitoring system.\nLink:\nhttps://lnkd.in/d4ziHhxH\nAfter wasting a lot of time reading other resources...\nUsing these 3 resources is a solid start for learning about monitoring ML systems.\nHave you tried them?\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmonitoring\n-----\nğŸ’¡ My goal is to help machine learning engineers level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHq3HFws2E6SA/image-shrink_800/0/1674730816012?e=1705082400&v=beta&t=6jjEb928g4_a4MASWyIPGmKIWA1bOcvxTqhc6FLZvww"
        },
        "Post_77": {
            "text": "My dreams came true âœŒï¸\nWhile on vacation, I had the wonderful surprise of passing 5k followers on LinkedIn.\nAs a side note, Amsterdam is a beautiful place, and it 100% deserves to be on most of the go-to tourist destinations.\nFirst of all, I want to thank everybody who is reading my content. I am doing my best to provide you with value about ML engineering, building ML systems and MLOps.\nSecondly, I want to keep the tradition of sharing the key steps that got me here:\n1. Consistency when you have <10 likes\nWhen you are just starting, you probably havenâ€™t built trust around your online persona.\nThus most likely, you will be writing into the void.\nJust donâ€™t get discouraged. At some point, your content will gain traction.\n2. Just start and adapt along the way\nIt is hard to find your best niche, plan, etc., without any real feedback.\nStop overthinking and start posting.\nBut always listen and adapt to see what suits you and your audience better.\nAlways learn âœŒï¸\nSo...\nConsistency, regardless of your engagement and taking action when you think you donâ€™t know what you are doing are my two best pieces of advice for building your brand on LinkedIn.\nWhat do you think about my advice? Would you add something else?\nhashtag\n#\nmachinelearning\nhashtag\n#\nlinkedin\nhashtag\n#\nbrand\n---\nIf you want to learn more about ML engineering, ML systems, and MLOps...\nFollow me on LinkedIn and Medium ğŸ‘‡",
            "image": "https://media.licdn.com/dms/image/C4D10AQH0My-ZT05H0w/image-shrink_800/0/1674725410167?e=1705082400&v=beta&t=sxe1SfdxO8aUJDe3SnnQZQFU7tzBIgPTxXh8NAa4c3E"
        },
        "Post_78": {
            "text": "This is one MLOps practice you 100% have to know.\nMany engineers ignore it,\nbut let me explain why it is so important.\nI showed you one post before about how to extract an embedding from your model.\nI showed you a few examples of extraction methods for various models.\nBut what happens if you want to change the extraction method or model to compare the performance?\nMost probably, it will soon become a mess.\nWe all encountered situations such as: \"final_model,\" \"best_final_model,\" \"best_final_final_model,\" etc. You get the idea... It is tough to keep track of our changes.\n.\n3 types of changes can occur when extracting embeddings:\n#ğŸ. ğ‚ğ¡ğšğ§ğ ğ ğ²ğ¨ğ®ğ« ğ¦ğ¨ğğğ¥ ğšğ«ğœğ¡ğ¢ğ­ğğœğ­ğ®ğ«ğ\nThis is considered a major change: O.x.x\nChanging your model architecture might change the semantics of the embeddings and their dimensionality. Also, as a by-product, it changes your extraction method, and you must retrain your model.\n#ğŸ. ğ‚ğ¡ğšğ§ğ ğ ğ²ğ¨ğ®ğ« ğğ±ğ­ğ«ğšğœğ­ğ¢ğ¨ğ§ ğ­ğ²ğ©ğ\nThis is considered a minor change: x.O.x\nAgain this might result in changes in your semantics or dimensionality, but you don't have to retrain your model.\n#ğŸ‘. ğ‘ğğ­ğ«ğšğ¢ğ§ ğ²ğ¨ğ®ğ« ğ¦ğ¨ğğğ¥\nThis is considered a patch version change: x.x.O\nThis won't change your embedding structure, but by retraining, they won't be compatible with your old set of embeddings as the vector space might change.\n.\nAs you see, your embeddings will change quite often, that is why you need to...\nVersion your data!\nData versioning is one key aspect of a clean ML system.\nEvery change will result in a new data version. Then, when you use a specific set of embeddings, you will know exactly how they were computed.\nYou can easily version your data directly in the feature store for structured data. You can quickly add data versioning for unstructured data using tools such as S3 + DVC/your custom software.\nTo conclude...\nYou should fire up a new version of your data whenever you change your data process.\nHow are you versioning your data?\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nğŸ’¡ My goal is to help machine learning engineers level up in designing and productionizing ML systems.\nğŸ‘‰ Follow me for weekly insights.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHsdra6RmOgHA/image-shrink_1280/0/1674558018513?e=1705082400&v=beta&t=yRYMDXlH4BaV-A7I__HL72qJcGqCPbp7gEDSHYyrHIk"
        },
        "Post_79": {
            "text": "After 3 months of hard work and no sleep...\nI can finally say that I know everything about MLOps.\nHehe, not really, but I have put in a lot of work and...\nI finally finished my hands-on \"Full Stack 7-Steps MLOps Framework\" series.\nThis a step-by-step course that will explain to you how to design, implement, and deploy an ML system using MLOps good practices.\nAt the end of the 7 lessons course, you will know how to:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\nIf this sounds interesting to you.\nI want you to know that...\nThe course is free and published on Medium under the\nTowards Data Science\npublication so that anybody can level up their ML engineering journey.\nWant to start building your project?\nJust check out the GitHub repository and the first lesson of the series in the comments.\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nlearning\n-----\nI want to thank @Kurtis Pykes for helping me build this series and being an excellent copilot. It was a great collaboration.",
            "image": "https://media.licdn.com/dms/image/C4D10AQE5QnWPZfsOkQ/image-shrink_1280/0/1673515816117?e=1705082400&v=beta&t=4wfPxD2ZsU9ZBhpeXZOY4N4eqSr4T2voGYWH4EbYYNI"
        },
        "Post_80": {
            "text": "ML is more than training, evaluating and beating metrics.\nTo keep your model healthy while in production, you must carefully validate and monitor your data.\nThese are the 5 main criteria you have to be careful about.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nmonitoring",
            "image": "https://media.licdn.com/dms/image/C4D10AQFCnb9VmoV23A/image-shrink_800/0/1673257519840?e=1705082400&v=beta&t=q46DldIbZmoUrdiysGV0gZ6hRHgJrgH3AyvA6uKNstE"
        },
        "Post_81": {
            "text": "My recommender infrastructure was a mess until I started using these 2 key components.\n#1 Feature Store\nYou can efficiently query features anywhere in the online pipeline using a feature store. Thus your online application shouldn't bother anymore with cleaning, validation, and feature engineering. All of that is delegated to the feature store.\nTherefore the feature store solves the train-serve skew and makes your features available by exposing them in a low latency key-value database such as Redis or RocksDB.\n#2 Swapped NN with ANN\nAs the users x items space might get to the magnitude of millions, querying items using a naive NN approach will probably break your system. That is why you must use an ANN (Approximate Neireght Neighbor) approach that gives up perfect results in favor of speed. This step should reduce your space from millions to thousands of candidates. Hence a coarse but fast search is excellent.\nPopular ANN tools: Faiss, Scann\nhashtag\n#\nrecommendersystem\nhashtag\n#\nml\nhashtag\n#\nmlops\n-----\nğŸ’¡ My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4D10AQGid1pq6qdwHg/image-shrink_800/0/1670056776109?e=1705082400&v=beta&t=UswSMCZglI6E1OHrsVFGsVglT9wUJf4nlqL8gu08wbg"
        },
        "Post_82": {
            "text": "My last recommender system was so slow that instead of improving the user experience, people stopped using the app.\nIn the real world, user experience is the most critical aspect you must take care of. When it narrows down to ML, balancing performance and speed is the most essential trade-off you must consider.\n.\nIn a recommender system, when we want to query for similar users/items to speed up your system, you should:\nğ¬ğ°ğ¢ğ­ğœğ¡\nâ— nearest neighbors (NN)\nğ°ğ¢ğ­ğ¡\nâœ… approximate nearest neighbors (ANN)\nRunning NN on millions of records will burn down your system. Read below to understand why.\n.\nğ–ğ¡ğ²?\nWhen building a recommender system, we transform the users and items into a lower-dimensional embedding space. Afterward, we use different distances in the embedding space, e.g., cosine similarity, to find similar relationships: user-item, user-user, and item-item.\nFor example, to find the most similar items for a user, using the nearest neighbors technique, you have to compute the distances between a user and all the items. Imagine how slow this could get when you have millions of items.\nğˆğ§ğ¬ğ­ğğšğ, ğğ¨ ğ­ğ¡ğ¢ğ¬...\nUsing an approximate nearest neighbors approach, you can find \"approximate\" similar items but in a much faster way. In a recsys setup, that is plenty because the results shouldn't be perfect.\nUsing such a technique, you can quickly reduce your space from millions to thousands/hundreds of items which you can later order and refine.\nCheckout the comments for various packages of ANN implementations ğŸ‘‡\nhashtag\n#\nrecommendersystem\nhashtag\n#\nml\nhashtag\n#\nmlops\n-----\nğŸ’¡ My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHvGSYfleTqBQ/image-shrink_800/0/1669793938941?e=1705082400&v=beta&t=w72OkEylqhijKOGnDnW8AORPbP2kFn6gdvSDGce-ALA"
        },
        "Post_83": {
            "text": "Successfully packaging and deploying a Python module was so tedious and painful until I started using this simple technique\nhashtag\n#\npython\nhashtag\n#\nmlops\nhashtag\n#\nsoftwareengineering",
            "image": "https://media.licdn.com/dms/image/C4D10AQGeWy2QzFKfEw/image-shrink_800/0/1669707787813?e=1705082400&v=beta&t=NZ13E4ejgtF5RiHi_GYfo7SdXPEhR239VtnLrjms4eo"
        },
        "Post_84": {
            "text": "If you are learning all the nitty little details of all the new shiny algorithms that our there, you won't have time to learn how to deploy them to production professionally.\nPick your battles wisely.\nIf you are inclined more towards researching, do 80% research and 20% engineering ML systems.\nIf you are inclined more towards engineering, do 20% research and 80% engineering ML systems.\nIndeed you need to understand both to grasp the whole perspective, but you don't have to master both.\nPick one and be the best at it ğŸ”¥\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nlearning\n-----\nğŸ’¡ My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4D10AQEiWclKmd0tMQ/image-shrink_800/0/1668757490230?e=1705082400&v=beta&t=RUMqj5opBdbsxPpLv6pWSKuH2OfXE8xrunjm65kFq88"
        },
        "Post_85": {
            "text": "How exciting is this? ğŸ”¥",
            "image": "https://media.licdn.com/dms/image/C4D10AQHuRhRcs0_oJg/image-shrink_800/0/1668671242174?e=1705082400&v=beta&t=da1NzIRIETJcr7S4Z80-T0DUciL_-QOc6RHAmriSxlY"
        },
        "Post_86": {
            "text": "As you type, a 3D world emerges.\nThis is super cool.\nFrom:\nOpusAI\nPS. Get today's top AI stories in a quick 3-minute digest. Join 33K+ other professionals staying smart on AI:\nhttp://bit.ly/3iKiI10",
            "image": "https://media.licdn.com/dms/image/C4D10AQEE2GmsXlQ2Fg/image-shrink_800/0/1668584550200?e=1705082400&v=beta&t=tnX0Lwr7cw2zHOrjuzN3yqUeAkEWjXJIlwG-CjjpqBw"
        },
        "Post_87": {
            "text": "With all the hype around ChatGPT, let me tell you how, in reality, ğˆ ğ›ğ¨ğ¨ğ¬ğ­ğğ ğğğ¯ğğ¥ğ¨ğ©ğ¦ğğ§ğ­ ğ©ğ«ğ¨ğğ®ğœğ­ğ¢ğ¯ğ¢ğ­ğ² ğ›ğ² ğŸ“ğŸ% - Spoiler alert: not by using ChatGPT.\n.\nDo you know ChatGPT has a distant, less famous cousin called GitHub Copilot?\nWell... That is what I am really using in my day-to-day programming experience.\n.\nğ–ğ¡ğ²?\n- it is specialized only in writing code\n- it generates code based on the context of your current codebase\n- it is already integrated into your IDE - you hit tab & bam... you have your code\n- it saves a lot of time in writing tedious boiler code\n- it saves a lot of time in writing tedious boiler documentation ğŸ˜\n- it saves a lot of time in searching syntax for various packages\n.\nI read stories about people writing an entire MVP without Google or ChatGPT but using solely GitHub Copilot. How cool is that?\nThat probably won't be the case for you, as you still need Google to look up more complex or subtle topics. But hey, for sure, it will lift the burden of looking up syntax every 5 minutes ğŸ˜‚\n.\nğğ¨ğ§ğ®ğ¬ ğ­ğ¨ğ¨ğ¥ - Besides GitHub Copilot, I use 'black' as a formatter that automatically formats my code following PEP8 standards.\nUsing these 2 tools, all the boring part of coding is automated, and I can focus on building awesome stuff ğŸ”¥\nhashtag\n#\nproductivity\nhashtag\n#\ndevelopment\nhashtag\n#\nprogramming\n-----\nğŸ’¡ My goal is to document the ups and downs of my learning journey about ML engineering. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQF0Q3ChUrjdyQ/feedshare-shrink_800/0/1668508686857?e=1707350400&v=beta&t=9uA7pmxxYiCOvVBKc8D98cRkVdzlbtR69QJlsnyh1RA"
        },
        "Post_88": {
            "text": "As a self-taught ML engineer, I used to learn daily and still felt like I didn't know enough.\nAs I love learning, I didn't want to give up, but seeing that information did not stick with me was unfulfilling. Everything changed when I realized and applied this one simple trick ğŸ‘‡\nğ˜ğ¨ğ® ğ¬ğ¡ğ¨ğ®ğ¥ğ ğšğ¥ğ°ğšğ²ğ¬ ğ¡ğšğ¯ğ ğš ğœğ¥ğğšğ« ğ ğ¨ğšğ¥ ğšğ§ğ ğ©ğ¥ğšğ§ ğŸğ¨ğ« ğ°ğ¡ğšğ­ ğ²ğ¨ğ® ğ°ğšğ§ğ­ ğ­ğ¨ ğ¥ğğšğ«ğ§.\n.\n# ğ–ğ¡ğ² ğ¢ğ¬ ğ­ğ¡ğ¢ğ¬ ğš ğ ğšğ¦ğ-ğœğ¡ğšğ§ğ ğğ«?\nWell, I love systems ğŸ‘€ But doing this...\n- you can focus more easily\n- you eliminate distractions more easily (e.g., social media)\n- you eliminate anxiety/impostor syndrome by reducing the number of new concepts that you have to learn\n- you have time to understand the fundamentals that will stick with you in the long run\n- you have time to really understand the topic and not just memorize it\n.\n# ğ‡ğ¨ğ° ğ¬ğ¡ğ¨ğ®ğ¥ğ ğ²ğ¨ğ® ğ©ğ¢ğœğ¤ ğ²ğ¨ğ®ğ« ğ ğ¨ğšğ¥?\nWell, this depends on your context, like level of experience, dedication, time, etc. Still, usually, it should be something concrete, something that excites you, and something incremental (go from T to T+1).\n# ğ‡ğ¨ğ° ğ¬ğ¡ğ¨ğ®ğ¥ğ ğ²ğ¨ğ® ğ©ğ¥ğšğ§ ğ¢ğ­?\nFor example, instead of saying I want to learn about MLOps, make a plan and divide it into smaller sections like:\n- Week 1: Experiment tracking\n- Week 2: Data versioning\n- Week 3: Orchestration\netc...\nThe essential idea is that in the timespan you planned to learn ğ¨ğ§ğ ğ­ğ¡ğ¢ğ§ğ , you should read articles, watch videos, and code examples only on that one thing. Through this, your brain will work for you and quickly internalize the new concept.\n.\nIf you believe it or not, I was that guy that wanted to learn everything as fast as possible. But that left me only with confusion, anxiety, and a superficial understanding.\nTake your time and enjoy your learning process âœŒï¸\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nlearning\n-----\nğŸ’¡ My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4E10AQGd4jl0O4vlaw/image-shrink_800/0/1668497957230?e=1705082400&v=beta&t=dmuJJQthGad1V0DsNOwqEUg9cqxrKtTvctJjg5dZVsI"
        },
        "Post_89": {
            "text": "These are the two most common recommendation system algorithms you have to know ğŸ”¥.\n#ğŸ.  ğ‚ğ¨ğ¥ğ¥ğšğ›ğ¨ğ«ğšğ­ğ¢ğ¯ğ ğ…ğ¢ğ¥ğ­ğğ«ğ¢ğ§ğ \nIt is based on the interaction between users and items (likes, ratings, comments, etc.).\nFor example, if Paul likes Python and SQL and Andrew likes Python, SQL, and ML. Then, we can recommend to Paul to look into ML.\nMost common algorithms learn user and items embeddings through:\n- Matrix Factorization  (e.g., SVD)\n- Deep Learning\n#ğŸ. ğ‚ğ¨ğ§ğ­ğğ§ğ­ ğ…ğ¢ğ¥ğ­ğğ«ğ¢ğ§ğ \nBased on the items a user likes, it learns with the help of the user's and the item's features to predict if a user might like an unseen item.\nFor example, suppose Paul likes Python and SQL. In that case, we can look at the characteristics of Paul (engineer, young, excited about tech, etc.) and the features of Python & SQL (programming languages, used in data fields, etc.) and find a confident match between Paul and ML.\nA common approach is to use various models (even a simple logistic regression can work) that take as input the features of a user and item and predict the probability that the user might interact with that item.\n-----\nğŸ’¡ In the coming weeks (maybe months), I will focus my posts on:\n-\nhashtag\n#\nrecommendersystems\n- hands-on examples of\nhashtag\n#\nML\nin production using\nhashtag\n#\nMLOps\ngood practices.\nFollow me if you want to learn more!",
            "image": "https://media.licdn.com/dms/image/C4D10AQGsUBcmioEt0g/image-shrink_800/0/1668066551583?e=1705082400&v=beta&t=ZKsgS33zuo4WoHAIikNZqJeHi5HGHQw1Z9O67SOcgXY"
        },
        "Post_90": {
            "text": "This is one critical teaching I have learned from my last year working as an ML engineer freelancer/contractor ğŸ‘‡\n.\nAs a curious person, I tend to stick my nose in all I can get. But after five years of experience, this left me with a broad knowledge of various domains but without knowing to do a particular thing exceptionally well. I mean extremely well. Like, to be the best at doing that thing.\nUnfortunately, in the world of freelancing/contracting, you are thriving in the spectrum of being the best in doing one thing. This sounds quite niched, but hear out my plan.\n.\nğŸ‘‰ In the coming months, I plan to get very good at building recommendation systems following good MLOps practices. Therefore, I can quickly build, train, deploy, and monitor recommendation systems for future clients, which will let me build trust and, ultimately, a brand around this.\nThus, I can expand my client base around building recommendation systems. Slowly, I will get more real-life experience in building production-ready ML systems. Therefore, at that point, it will be straightforward to learn to solve other problems and expand my skill set.\nMy point is that in the beginning, it is better to focus all your energy on becoming extremely good at one thing. After, you can quickly build on top of that.\n.\nWhy recommendation systems?\nAfter some research, I saw recommendation systems are estimated to have an annual CAGR of >50% (which is huge). Also, intuitively makes a lot of sense, as almost all web applications will need personalization in the future.\nAfter all, we all love when our tech adapts to our needs, right?\nhashtag\n#\ncareer\nhashtag\n#\ncontracting\nhashtag\n#\nfreelancing\nhashtag\n#\nonemanbusiness\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/D4D10AQE0N8YLWnLe1A/image-shrink_800/0/1666853483144?e=1705082400&v=beta&t=VggF0SG6D_fOp_OVMOnlKaOFtVY0SeRg6RG_7QJgXhM"
        },
        "Post_91": {
            "text": "Often a good dashboard is all you need ğŸ”¥\nI compiled for you a list of 7 open-source dashboards that you have to know to level up your data game.\nhashtag\n#\ndata\nhashtag\n#\nml\nhashtag\n#\nai",
            "image": "https://media.licdn.com/dms/image/C4D10AQED1LQtWOK6vw/image-shrink_800/0/1666767048482?e=1705082400&v=beta&t=c21rmEBMahyp9FoDMKjjYY7ChfKesUB_liAcEV2Apkc"
        },
        "Post_92": {
            "text": "I recently tested WakaTime, a tool that monitors all my development activity. I must say I love it ğŸ”¥.\n.\nWakaTime automatically generates weekly charts for visualizing distributions for:\n- coding time/day\n- projects you have been working on\n- programming languages\n- IDEs\n- OS\n- workstations\n- trends in programming time (don't mind my 100% decrease, I just had a research week where I mostly read and designed stuff ğŸ˜‚)\nYou just have to set up an account on WakaTime and install their plugin in your favorite IDE, and you are done!\n.\nAs a data person, I love to look at charts. Thus, looking at my own developing activity makes it even better.\nCheck the link in the comments ğŸ‘‡\nhashtag\n#\nmonitoring\nhashtag\n#\ndeveloping\nhashtag\n#\nsoftware\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D10AQEUW2tsE4TmmQ/image-shrink_800/0/1666767050051?e=1705082400&v=beta&t=DhSCGX34NX21r21hLcQ6B2f9C3ICFC2Z9QiNvwg9BEQ"
        },
        "Post_93": {
            "text": "I don't see any problem with this solution. Do you? ğŸ˜‚\nhashtag\n#\nai\nhashtag\n#\nml\nhashtag\n#\nclustering",
            "image": "https://media.licdn.com/dms/image/C4D10AQHhzeVU55WNjQ/image-shrink_800/0/1666767049826?e=1705082400&v=beta&t=ApZU_9DBipG6rBaqCTpdv8xyikQqFKUrgsOrCnzZXQM"
        },
        "Post_94": {
            "text": "Black Mirror, is that you?\nhashtag\n#\nai\nhashtag\n#\nml",
            "image": "https://media.licdn.com/dms/image/C4D10AQEpuyTPZX7sVQ/image-shrink_800/0/1666680071415?e=1705082400&v=beta&t=MzDwR2gHAb7Ame0RWwEen5bcwQG6JTjAnjmwweDW4gQ"
        },
        "Post_95": {
            "text": "This AI paper honestly scares me a bit.\nHere's what researchers did:\n- Send people into an MRI\n- Show them an image\n- Measure their brain waves\n- Put their brain waves into an AI image model (Stable Diffusion)\nGuess what happened?\nThe image model was able to reconstruct an image from those brain waves. It's not perfect - almost like a memory of the image.\nIt's as if Stable Diffusion can read minds.\nWhat else can today's language and image models do that we're not expecting?\nbiorxiv: 2022.11.18.517004v2\nPS. Get today's top AI stories in a quick 3-minute digest. Join 18K+ other professionals staying smart on AI:\nhttp://bit.ly/3iKiI10\nhashtag\n#\nartificialintelligence\nhashtag\n#\nmachinelearning\nhashtag\n#\ntechnology",
            "image": "https://media.licdn.com/dms/image/C4D10AQELrotau8bZgA/image-shrink_1280/0/1666248335172?e=1705082400&v=beta&t=j0EWEz4ySJpHTPznWyGyi8syTebZ1wEZ43yjOjmekX0"
        },
        "Post_96": {
            "text": "If you are a perfectionist, you know there is always another function to change, another class to refactor, and another document to be prettified...\nYou know the saying: \"The perfectionist's mind is always in pain.\"\nBut how can we bypass this in the pragmatic world we live in?\nWell... There is another saying that states the following:\n80% of the work is done in 20% of the time.\nThe other 20% of the work is finished in the remaining 80% of the time.\nAs perfectionists, we are often stuck refining that 20% of the work when most of the job is done.\nThat is why I found it extremely helpful to define what 80% of the work would look like and stop there no matter what. Only after I validate my project, idea, prototype, etc., and see a need for further refinement will I continue the other 20% of the work.\nI am not saying to ignore the details. Ultimately, the details differentiate a decent product from a great one. This is a method to prioritize your work and ensure that what you do has real impact and value.\n.\nWhat do you think about this strategy? Would you use it?\nhashtag\n#\nproductivity\nhashtag\n#\nsystem\nhashtag\n#\nperfectionist",
            "image": "https://media.licdn.com/dms/image/C4D10AQGjHH_a-BGrnQ/image-shrink_800/0/1666248334949?e=1705082400&v=beta&t=7FFhm9jzQFAEUW23RUUVYFrBHrKHE534yh-zEHZ7KhM"
        },
        "Post_97": {
            "text": "Have you looked at job descriptions for any Data Scientist/Machine Learning/MLOps role lately?\nAfter reading multiple JD and discussing various positions, I realized that the requirements for most data positions are a mess...\nFor many companies, the boundaries between various roles are practically inexistent, which results in one size fits all people and unrealistic expectations.\nFor example, a company wants an ML engineer that has the following qualifications:\n- 5 years of experience in writing data pipelines\n- 7 years of experience in training models\n- 10 years of experience in deploying models\nBut, If you look closely, all three skills are part of different roles. I am not saying this isn't good. I love working in interdisciplinary roles. But sometimes, navigating the data world and understanding what to learn and improve is difficult.\nI believe that this confusion might be because of the ML/MLOps engineer's emerging roles or because some European companies realized that they are behind in the data-centric movement and are just starting to integrate data people into their teams. In the end, this field is moving so quickly that sometimes it is hard to keep track of what is happening.\nDisclaimer: I am not saying this happens 100% of the time. I just saw a general trend in this confusion and wanted to hear your opinion about it.\n.\nIt is incredibly satisfying to work on a novel field and contribute to its way to maturity, but sometimes things might get confusing. That is why I believe hearing other people's opinions are helpful.\nThus I would love to hear your thoughts on this topic.\nWhat do you think? How was your experience in finding a job that fits you well? Do you believe that companies have realistic expectations and know what they want for their new data people hires?\nhashtag\n#\njobs\nhashtag\n#\ncareer\nhashtag\n#\nlearning",
            "image": "https://media.licdn.com/dms/image/C4D10AQGu0LHIBXJ-vg/image-shrink_800/0/1666161018881?e=1705082400&v=beta&t=7AHHWs7llTFN93_OhoB9HwXChnpVIuj8uDK8Y876pCg"
        },
        "Post_98": {
            "text": "Have you ever wondered where you can quickly learn more about various software design patterns?\n.\nYou can read about dozens of software design patterns on Refactoring Guru:\nhttps://lnkd.in/dVUKymMT\nThey provide intuitive examples and code snippets to understand various concepts practically.\nAlso, they have support for all your beloved programming languages: Python, Java, C, TypeScript, etc.\nhashtag\n#\ndesignpatterns\nhashtag\n#\nsoftwareengineering\nhashtag\n#\narchitecture\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D10AQFkE0gWn0mbVg/image-shrink_800/0/1666077124300?e=1705082400&v=beta&t=InVMvu9XaP5whV2uLq4sPvKCNz_K40RHHpQvIww7hMY"
        },
        "Post_99": {
            "text": "This visualization could replace your OOP class ğŸ˜‚\nhashtag\n#\nsoftwareengineer\nhashtag\n#\noop",
            "image": "https://media.licdn.com/dms/image/C5610AQEQ52CL8R7cqw/image-shrink_800/0/1664435735000?e=1705082400&v=beta&t=nEZITBV3LL7ejok4b6o8bldWcAEk5-QjkCTw7T_wlRg"
        },
        "Post_100": {
            "text": "Visualization is an art form: pass by reference vs pass by value.\nâ†“\nCheck out\nhttps://AlphaSignal.ai\nto get a weekly summary of the top breakthroughs in Machine Learning.\n-\ncredit penjee",
            "image": "https://media.licdn.com/dms/image/C5610AQEXz-YTeDxwvQ/image-shrink_800/0/1664346623847?e=1705082400&v=beta&t=fHwqAAonLjCl8UxlZ06S8quApvvU9G5h4l8Lkjj5vDU"
        },
        "Post_101": {
            "text": "Have you ever wondered where to start learning MLOps?\n.\nYou can find lots of valuable information by checking out the MLOps organization site:\nhttps://ml-ops.org/\nAlso, you can find hundreds of essential references by following this GitHub repository:\nhttps://lnkd.in/dxknAmTK\nNow you have no excuse to start learning MLOps!\nhashtag\n#\nmlops\nhashtag\n#\ncontent\nhashtag\n#\nlearning\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E10AQGXJoIqehDqSw/image-shrink_800/0/1664263660845?e=1705082400&v=beta&t=pRmC3SJcHQGHyslduqUadLkX136YQ27zgqZgArMLcPk"
        },
        "Post_102": {
            "text": "Who fell for it raise your hand ğŸ˜‚ğŸ˜‚ğŸ˜‚",
            "image": "https://media.licdn.com/dms/image/C4E22AQF6te4xqmKLqw/feedshare-shrink_800/0/1664009611443?e=1707350400&v=beta&t=DK1uCGSJwya9t7en3C8eSLm7qC2eun_xsJ2im1N-p4M"
        },
        "Post_103": {
            "text": "This is so hilarious ğŸ¤­ğŸ˜‚ğŸ˜‚ğŸ˜‚\nCredit\nCorey Quinn\nhashtag\n#\nAWS\nhashtag\n#\nawscloud\nhashtag\n#\nawscloudpractitioner\nhashtag\n#\nmachinelearning\nhashtag\n#\nnew\nhashtag\n#\nbrad",
            "image": "https://media.licdn.com/dms/image/C4E22AQEsH5BoNQUwmw/feedshare-shrink_800/0/1664009611520?e=1707350400&v=beta&t=4IF19D8c7j96tmq0fRBv7Am_UzXfm5eDacxmOBlfF8A"
        },
        "Post_104": {
            "text": "These are 3 characteristics showing that a feature store is a crucial component within any end-to-end ML production system.\n.\n#ğŸ ğ…ğğšğ­ğ®ğ«ğ ğ¦ğšğ§ğšğ ğğ¦ğğ§ğ­\nFeature management is the ability to discover and share features across teams. This characteristic may save time and computation as multiple features can be shared across models. Also, it helps share knowledge across teams or projects.\n#ğŸ ğ…ğğšğ­ğ®ğ«ğ ğœğ¨ğ¦ğ©ğ®ğ­ğšğ­ğ¢ğ¨ğ§\nFeature computation means you compute your features only once and store them for future uses. In this case, it acts like a data warehouse, and it may save a lot of time and computation, especially when computing specific features consume a lot of resources (usually the case).\n#ğŸ‘ ğ…ğğšğ­ğ®ğ«ğ ğœğ¨ğ§ğ¬ğ¢ğ¬ğ­ğğ§ğœğ²\nFeature consistency refers to the problem of having two separate pipelines for the model between training and production (known as the training-serving skew issue). Using a feature store, you unify the logic of feature creation, using the feature store both in training and production.\n.\nI am curious to know what tools you use for your feature store solutions. Please Let me know if the comments below ğŸ‘‡\nhashtag\n#\nfeaturestore\nhashtag\n#\ndatawarehouse\nhashtag\n#\nmlops\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFKE41Cb2RxTA/feedshare-shrink_800/0/1664009611481?e=1707350400&v=beta&t=_q8ocLIGlMT32vm3NjjqmFEqg3Sota7sQXHVfWit2eA"
        },
        "Post_105": {
            "text": "ğ–ğ¨ğ«ğ¤ğŸğ¥ğ¨ğ° ğ¦ğšğ§ğšğ ğğ¦ğğ§ğ­/ğ¬ğœğ¡ğğğ®ğ¥ğğ«ğ¬ and ğ¨ğ«ğœğ¡ğğ¬ğ­ğ«ğšğ­ğ¢ğ¨ğ§ ğ­ğ¨ğ¨ğ¥ğ¬ are two critical elements of any successful MLOps infrastructure. But what are they used for, and what are some vital differences between them?\n.\n#ğŸ ğ–ğ¨ğ«ğ¤ğŸğ¥ğ¨ğ° ğ¦ğšğ§ğšğ ğğ¦ğğ§ğ­ ğ­ğ¨ğ¨ğ¥ğ¬ (ğ¨ğ« ğ¬ğœğ¡ğğğ®ğ¥ğğ«ğ¬)\nWorkflow management tools are concerned with when to run jobs and what resources are needed to run those jobs.\nSchedulers are based on job-type abstractions: DAGs or priority queues. The dependencies between multiple jobs are defined within a YAML or Python file. For complex applications, this is essential because, for example, if job A fails, you don't want to execute job B. Another example would be that if job A fails, retry to run it 5 times until you give up.\nSchedulers are used for periodic jobs.\nPopular tools: Airflow, Argo, Prefect, Kubeflow, Metaflow\n#ğŸ ğğ«ğœğ¡ğğ¬ğ­ğ«ğšğ­ğ¢ğ¨ğ§ ğ­ğ¨ğ¨ğ¥ğ¬\nOrchestration tools concern where to get resources to run the jobs and how to scale the resources to run the jobs efficiently.\nOrchestration tools deal with lower-level attractions: machines, instances, or clusters. They manage the hardware resources on top of which you run your system.\nOrchestration tools are responsible for scaling the computational power size to efficiently support the given jobs (e.g., if the number of jobs is more significant than the number of instances, it will spin up more instances).\nOrchestration tools are used for long-running servers that serve clients' requests.\nPopular tools: Kubernetes (K8s)\n.\nThese tools are usually used interchangeably. Schedulers typically run on top of orchestration tools.\nhashtag\n#\ntools\nhashtag\n#\nschedulers\nhashtag\n#\norchestration\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQE0VhXfdWO0vQ/feedshare-shrink_800/0/1664009611111?e=1707350400&v=beta&t=CSibfshNVK-YCVmKcm2pVwVwEVxYxz0deWl3Gq_spuU"
        },
        "Post_106": {
            "text": "You don't need a feature store...\nSaid no one after he understood why using one is so essential for any ML system.\nHere is why ğŸ‘‡\nhashtag\n#\nmlops\nhashtag\n#\nml\nhashtag\n#\nfeaturestore",
            "image": "https://media.licdn.com/dms/image/C4E22AQFwQNEtNZfEoQ/feedshare-shrink_800/0/1664009611450?e=1707350400&v=beta&t=x-aNerQvGJAIqf8we19cSAgKZnrLIUeFQJUb_JD8Q28"
        },
        "Post_107": {
            "text": "Top 3 tools to keep your Python code professional without any additional effort ğŸ‘‡\n.\n#ğŸ. ğğ¥ğšğœğ¤\nA tool for automatically formatting your code.\nCheck it out:\nhttps://lnkd.in/dR_RdjT9\n#ğŸ. ğ…ğ¥ğšğ¤ğğŸ–\nA tool that checks and validates your coding style.\nCheck it out:\nhttps://lnkd.in/dBmyuECV\n#ğŸ‘. ğ‘ğğŸğ®ğ«ğ›\nA tool for automatically refurbishing and modernizing your Python codebases.\nCheck it out:\nhttps://lnkd.in/d67J9hbt\n.\nAn important note is that you can automate the trigger of these tools using pre-commit. Thus, these tools will be triggered every time you run a commit.\nhashtag\n#\npython\nhashtag\n#\ncleancode\nhashtag\n#\ncodingstyle\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQEtU9tJZCedcQ/feedshare-shrink_2048_1536/0/1663822911250?e=1707350400&v=beta&t=G-LJmPj4io6STcp7ek--sMOzIzDyZnBh70OYgsyWtrs"
        },
        "Post_108": {
            "text": "I compiled a list of the top 10 metrics your ML monitoring system should track.\n.\n#ğŸ ğğ©ğğ«ğšğ­ğ¢ğ¨ğ§ğšğ¥ ğ¦ğğ­ğ«ğ¢ğœğ¬\nUsed to monitor the health of your overall software system.\nâ latency\nâ throughput\nâ CPU/GPU utilization\nâ memory utilization\nâ the number of requests your model receives in the last X minutes/hours/days\nâ the number of successful requests\n#ğŸ ğŒğ‹-ğ¬ğ©ğğœğ¢ğŸğ¢ğœ ğ¦ğğ­ğ«ğ¢ğœğ¬\nUsed to monitor the performance of your ML system.\nâ accuracy (extremely useful when you have feedback or natural labels from the user: click rate, upvote, downvote, purchases, bookmarks, views, etc.)\nâ predictions (as predictions are low dimensional various statistics are easy to compute + the distribution of the prediction represents a proxy for the input distribution)\nâ features (feature validation + two-sample tests for drift detection)\nâ raw inputs (these are harder to monitor due to their scattered nature within the infrastructure)\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D10AQFF13WkaXvq9A/image-shrink_800/0/1663828711420?e=1705082400&v=beta&t=H45_IFogt0_nFieeIKL0x_wrMzUC2eQgWl3wxSFrSLM"
        },
        "Post_109": {
            "text": "I compiled a table with the top 8 most demanding domains in AI/ML. I want to understand in which field to specialize further as a freelancer. Thus I wanted to take a data-centric approach in making this decision. I used the market size as the 'demand' metric because where the money flows represents a strong signal of demand in that field.\nNote that the absolute values in $$$ might not be 100% accurate compared as they might be extracted from different years, such as 2020-2021-2022. The most interesting column is ğ‚ğ€ğ†ğ‘ % which shows the potential growth in that field. If there is growth, there is new demand for engineers in that field.\nThe market size within the table consists only of the AI/ML part of that domain (i.e., the entire energy domain is way bigger than 3.82$ Billion). Also, some domains might cross each other, so the Global Machine Learning values won't add to the rest of the fields.\n.\nWhat do you think? Do you believe that this table is relevant? Did I miss any domains? What field in AI/ML do you think has the most potential?\nI am trying to figure out the answers to these questions myself, so feel free to start an open discussion in the comments below ğŸ‘€.\nhashtag\n#\ncarrer\nhashtag\n#\nai\nhashtag\n#\ndemand\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E10AQGFK8tYf0y5KQ/image-shrink_800/0/1663655696508?e=1705082400&v=beta&t=SCdYV79zEKcki-8azv4OxjQtJNiR2s2fIEqKjjc8NtM"
        },
        "Post_110": {
            "text": "Everybody says you can improve your model by training it on more data. Is it true? How can we test this theory before spending money and effort collecting that new data?\n.\nGood news. The solution is quite simple.\nUsing the ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ‚ğ®ğ«ğ¯ğ graph, you can quickly see the performance evolution of the model with various amounts of data.\nIt is important to validate this because more data is not the answer to all your problems. ğŸ˜‚\n.\nThe main idea of the ğ‹ğğšğ«ğ§ğ¢ğ§ğ  ğ‚ğ®ğ«ğ¯ğ method is simple:\n1. You create N subsets from your training dataset. Where the first subset is the smallest. You keep increasing the subset size at a given rate until you reach the full dataset size: 1 < 2 < 3 < ... < N.\n2. You train your model on subsample 1.\n3. You test your model on the test split (which is not divided; it is not good practice to touch your test split).\n4. Save the results.\n5. Repeat steps 2-4 on all your subsets.\n6. Plot the results.\n.\nUsing this mechanism, you can simulate if adding more data will boost your model's performance.\nAlso, you can quickly compare different models and evaluate which one needs more/fewer data to achieve your desired accuracy.\n.\nCheck out how easy it is to do this with Sklearn:\nhttps://lnkd.in/d358Dhhd\n.\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQGnqUTdbrlucQ/feedshare-shrink_800/0/1660716949072?e=1707350400&v=beta&t=VhCxp0eknc3WcP43-UQ9R8CYU_1PA6h7UO1SgRsH-5k"
        },
        "Post_111": {
            "text": "How you frame a\nhashtag\n#\nmachinelearning\nproblem is essential for your future self.\nIt can make your life easy or a living hell.\nğŸ’¡ ğ„ğ±ğšğ¦ğ©ğ¥ğ\nA user just clicked on our e-commerce site that sells tech. We want to welcome them and give them a personalized discount on a hand-picked item.\nHow do we solve this problem using\nhashtag\n#\nML\n?\nAt first sight, we might start building a multiclass classifier that takes input features about the user and predicts the probability that the user might like a particular item.\nBut this decision will make your future life a living hell if we consider it.\n.\nğ–ğ¡ğ²?\nFor every new item in the store, you need to retrain the model to adapt its output to the new inventory.\n.\nğ’ğ¨ğ¥ğ®ğ­ğ¢ğ¨ğ§\nBuild a binary classifier that takes as input the user's features and information about a specific item. The model's output will reflect only the probability that the user might like the item you used as input.\nIn this scenario, we will have no issue when a new item is added to the store.\n.\nğ–ğ¡ğ²?\nBecause the output is not dependent on the number of items within the store.\nWe will predict the probability for every item and take the most significant out of them.\nIn conclusion, it might be worth taking a deep breath and thinking about the problem before starting to code your solution.\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQGGqouhjN4ANQ/feedshare-shrink_800/0/1659623203087?e=1707350400&v=beta&t=0GpCiltAwx5IgR6ybqn4Argz9ih3dpFwmsqbP5cBg8M"
        },
        "Post_112": {
            "text": "I just started watching an excellent  ML course on building real-world, end-to-end ML applications.\nIf you ever want to level up your ML knowledge and make it production-ready, I recommend this course by\nPau Labarta Bajo\n.\nHe is a DS and ML professional with 8+ years of experience building production-ready ML systems. Thus, you can get real-world guidance on how to build end-to-end ML applications.\nWithin the course, you will get clear explanations of how to build and design an end-to-end ML application.\nI think it's worth it to check it out.\nP.S. Also, you can follow him on LinkedIn and Twitter, where he often posts content about ML in production. I learned a lot from him.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nrealworld\nhashtag\n#\ntutorial\nhashtag\n#\ncourse",
            "image": "https://media.licdn.com/dms/image/C5622AQGYvQmmtnTGbg/feedshare-shrink_2048_1536/0/1654833749888?e=1707350400&v=beta&t=hfIcvbeNw9TNhdt3uCbI4SI9KIzyWCcLS70vSfG3Ces"
        },
        "Post_113": {
            "text": "I found the top 10 most in-demand skills in DS and ML after I analyzed 111 jobs. Using a data-centric approach, here is how I have done it ğŸ¦¾\n.\nUsing LinkedIn Premium, I randomly sampled 111 positions across Europe and US. I saved them to\nTeal\n, which automatically extracted essential keywords from the job descriptions and generated the graph below.\nI believe this graph is beneficial in optimizing your resume/LinkedIn profile with the proper keywords. Because I sampled positions from various domains (so I won't be biased), these are more general terms. But along these keywords, you can also repeat the same process in your domain of interest (e.g., CV, NLP, recommendation systems, etc.) to build a killer profile.\nNote that you have to pay for LinkedIn Premium, but to use this feature from Teal is free of charge.\nLink to Teal in the comments ğŸ‘‡\nhashtag\n#\njobs\nhashtag\n#\nresume\nhashtag\n#\nskills\n-----\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQE6GrRqLwZyaw/feedshare-shrink_800/0/1654272075833?e=1707350400&v=beta&t=0Wd_ub71J9bj1JV7ahHpkxCdCDCHWUEuPb3B5cxFdYk"
        },
        "Post_114": {
            "text": "We'll send five of these to lucky people anywhere in the world ğŸŒ\nWe want to celebrate with you - our book just turned 1 - they grow up so fast.\nğŸ‘‰ Simply comment below by adding (@) friends who should get the book. We randomly select your friends and contact them for their address to ship the books.\nğŸ“– About the book:\n\"Why does my neural network not learn?\" by @Frank Hafner and I, will kickstart your friends' deep learning journey!\nWith best practices and extensive experience from applying deep learning in research and development, they will get their own neural networks to train.\nğŸš€ Check it out in more detail, and get your own copy here (make sure to search on your amazon marketplace, we are basically available everywhere) >>\nhttps://lnkd.in/exfkjY9t\nâ˜• And as always, feel free to follow here on LinkedIn.\nhashtag\n#\ndatascience\nhashtag\n#\npython\nhashtag\n#\ndeeplearning\nhashtag\n#\nGiveaway\nhashtag\n#\nbooks\nhashtag\n#\nkdp\nhashtag\n#\nmachinelearning",
            "image": "https://media.licdn.com/dms/image/C5622AQGxmGsXLqJorw/feedshare-shrink_800/0/1653666061964?e=1707350400&v=beta&t=asToqika68XG9Qp1jCil0OAFGCIkbtKDLzo7q1joJAw"
        },
        "Post_115": {
            "text": "These are 3 data distribution shifts you have to know when building real-world ML applications ğŸ‘‡\nhashtag\n#\nml\nhashtag\n#\ndata\nhashtag\n#\ndistributionshift\nhashtag\n#\nmonitoring",
            "image": "https://media.licdn.com/dms/image/C5622AQEpbH-nQXuj0A/feedshare-shrink_2048_1536/0/1650294001082?e=1707350400&v=beta&t=C_hNLMpiBA8mpDMeiXRbHEBIEDRdVh6Xf_3oc5B1sTU"
        },
        "Post_116": {
            "text": "Let's clarify the difference between online and batch prediction for deploying your ML modelsğŸ”¥\n.\nFirst, we must understand ğ­ğ°ğ¨ ğŸğğšğ­ğ®ğ«ğ ğ­ğ²ğ©ğğ¬ that your ML model can learn from.\n#ğŸ ğğšğ­ğœğ¡ ğŸğğšğ­ğ®ğ«ğğ¬\nBatch features are represented by the good old static datasets you are accustomed to. It represents historical data that can be stored in databases, data warehouses, etc.\nDon't confuse batch features with batch prediction (we will get to it immediately).\n#ğŸ ğ’ğ­ğ«ğğšğ¦ğ¢ğ§ğ  ğŸğğšğ­ğ®ğ«ğğ¬\nStreaming features are computed from real-time data sources. These are used when you have access to streaming infrastructure.\n.\n#ğŸ ğğšğ­ğœğ¡ ğ©ğ«ğğğ¢ğœğ­ğ¢ğ¨ğ§\nBatch prediction uses batch features to periodically generate predictions that are stored in a database. When a client makes a prediction request to the system, based on some metadata, the system will directly look for the precomputed predictions in the database.  It is also known as an ğšğ¬ğ²ğ§ğœğ¡ğ«ğ¨ğ§ğ¨ğ®ğ¬ ğ©ğ«ğğğ¢ğœğ­ğ¢ğ¨ğ§.\nBatch prediction is used when the model computes the predictions with high latency, and your application has to be in real-time. It is like you are caching all possible predictions in a database so you can instantly serve the result.\nThe most significant advantage of this method is also its biggest weakness. You need a database and computing power to generate predictions for all possible combinations (e.g., in a song recommendation system, you must compute all the recommendations for your users hourly), which might be costly and redundant. Imagine that you have 100k users and only 1k use your app daily. If you compute your predictions daily, 99k of predictions are redundantly computed.\n#ğŸ ğğ§ğ¥ğ¢ğ§ğ ğ©ğ«ğğğ¢ğœğ­ğ¢ğ¨ğ§\nOnline prediction can use both batch features and streaming features to make predictions. It is a more intuitive method as it is the usual way to do things. When a client makes a prediction request to the system, the prediction is computed by the model on demand. It is also known as a ğ¬ğ²ğ§ğœğ¡ğ«ğ¨ğ§ğ¨ğ®ğ¬ ğ©ğ«ğğğ¢ğœğ­ğ¢ğ¨ğ§.\nOnline prediction is used when the system has to be real-time, and the model predictions can be calculated with low latency. Also, in many cases, you are forced to apply the online prediction pattern because you can't possibly compute all the combinations required for your users (e.g., an NLP translation app can't compute all the combinations between all the sentence options and languages).\n.\nBoth methods can be used for real-time predictions. Only the way the predictions are served is different.\nAlso, these two methods are complementary. For example, you can use batch prediction for your top most common requests and make an online prediction for the rest.\n.\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQHeDO1Xk9nlmg/feedshare-shrink_800/0/1650018577071?e=1707350400&v=beta&t=BOjHwhAueXJBGQNzso_7FJQ-4ckeAGve-TUqlbiog-A"
        },
        "Post_117": {
            "text": "As an ML engineer, you are still a software engineer specializing in building AI/ML systems. Therefore, you still have to write clean, maintainable, and scalable software.\nThus, you need to be aware of good software practices and patterns.\nHaving that in mind, I researched and compiled an article about 10 software patterns that fit well in any ML application.\nhashtag\n#\nml\nhashtag\n#\nsoftwarepatterns\nhashtag\n#\nai",
            "image": "https://media.licdn.com/dms/image/C4E22AQE0S9Y0dHuBAg/feedshare-shrink_800/0/1650039433740?e=1707350400&v=beta&t=TVev7A-xddnxc3RvTNQXxMgsKnNS7PMfktUeVLqpNyA"
        },
        "Post_118": {
            "text": "This is a method that usually solves the class imbalance issue and is generally omitted ğŸ‘‡ğŸ»\n.\nStop complicating with oversampling or undersampling your data distribution.\nThe easiest way is to use loss weights to raise the error on your minority class and lower the error on other classes.\n.\nBut the most elegant solution is by using: Focal Loss.\nA loss function initially introduced in Computer Vision by Facebook Research that dynamically moves its learning attention to samples from which it needs to learn more.\nFrom my experience, this method is often omitted when training models with tabular data.\n.\nAdditional to the difference between the model prediction and the ground truth, the \"Focal Loss\" focuses on the confidence of the output:\nWhen the confidence in a prediction is high, the loss will be very low.\nWhen the confidence in a prediction is low, the loss will be very high.\nTo find out more, check out the paper:\nhttps://lnkd.in/dWyPbAD5\n.\nğŸ’¡ Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFKb__mL4LRpg/feedshare-shrink_800/0/1643387997184?e=1707350400&v=beta&t=o6k9wWmj_nwS8vqF_bnipEmrzwNbvVbonaWso7Z9xCU"
        },
        "Post_119": {
            "text": "Code quality is what sets apart good software systems from bad ones.\nData quality distinguishes great AI systems from those that will never make it to production.\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQGCzw1nHfIPqw/feedshare-shrink_800/0/1627203653569?e=1707350400&v=beta&t=dDO4iLPcoMZN2JEabKZI-Oq2jCpUOyJEHisn9hPwzU8"
        },
        "Post_120": {
            "text": "Those are the 2 most common issues of data quality:\nğŸ. ğ„ğğ ğ ğœğšğ¬ğğ¬ ğšğ«ğ ğ«ğšğ«ğ.\nOften due to random sampling. You can get significant performance by picking more difficult training data.\nWhat doesn't kill you makes you stronger.\nEdge cases are often environmental conditions (not only class imbalance).\nğŸ. ğ’ğšğ¦ğ©ğ¥ğğ¬ ğšğ«ğ ğ¦ğ¢ğ¬ğ¥ğšğ›ğğ¥ğğ ğ¨ğ« ğ¦ğ¢ğ¬ğ¬ğ¢ğ§ğ .\nThis makes sense because images seen 1,000 times are easier to label than seen only once.\nOne innovative method to find mislabeled samples is picking the ones in your trained model with the highest error.\nThis is known as sort-by-loss or sort-by-confidence, where your model and ground truth disagree.\n.\nA good strategy is to pick high-quality data, starting from difficult ones and finding more of the same.\nThis strategy is also known as a derivative of Active Learning.\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQHAxYVtTgvrqA/feedshare-shrink_800/0/1627811822843?e=1707350400&v=beta&t=daV-rIyzFXXLWwQihj1FxtKpHIRfDk047WJyU91vwio"
        },
        "Post_121": {
            "text": "Here is an excellent site to read and research the most pressing world problems. They also provide suggestions on how you can contribute to adding value to these fields.\nI stumbled upon this site as I was trying to find a field where to contribute and find real meaning. It helped me grasp the big picture of the world's pressing problems.\n.\nTheir list of most pressing world problems (sorted by importance):\n1. Risks from artificial intelligence\n2. Catastriohic pandemics\n3. Building effective altruism\n4. Global priorities research\n5. Nuclear war\n6. Epistemics and institutional decision-making\n7. Climate change\n.\nAs expected, as\nhashtag\n#\ndata\npeople, we can contribute to all of these. This is awesome because we can directly have a tangible impact on the future of the earth.\nIt is interesting to see that climate change is at the bottom of their list and risks from artificial intelligence at the very top. Is AI more dangerous than climate change? What do you think?\n-----\nYou can find the link to the site in the comments ğŸ‘‡\nhashtag\n#\nvalue\nhashtag\n#\nworldproblems\nhashtag\n#\nai\nhashtag\n#\nml",
            "image": "https://media.licdn.com/dms/image/C4D22AQG2Av4Wf2vX1Q/feedshare-shrink_800/0/1627591805899?e=1707350400&v=beta&t=QfYerJQEZBSdpoYJe00SIE-0X0ewSEhJegbTA4vl0jo"
        },
        "Post_122": {
            "text": "These are the most common ways to handle missing data before feeding it to your ML model.\nhashtag\n#\ndata\nhashtag\n#\nml\nhashtag\n#\nmissingdata",
            "image": "https://media.licdn.com/dms/image/C4D22AQF18UYOBfIqVw/feedshare-shrink_800/0/1624867441048?e=1707350400&v=beta&t=zJMtR4HRqs73TriBRhU19Hcgu-cJu9bBNQVREtg-Qx8"
        },
        "Post_123": {
            "text": "I found another use-case for the \"*\" symbol in Python ğŸ‘‡\n.\nAs you know, the \"*\" is used in Python for deconstructing iterable structures (and not for signaling pointers as in other languages, like C & C++ğŸ˜).\nI recently learned that you could use the \"*\" deconstruction logic to take the first/last element from a list.\n.\nYou can achieve the same functionality with other methods like \"pop,\" but I found this syntax a lot cleaner as it is more verbose.\nIt is yelling to the reader: \"Hey, I just popped out the last element from the list into variable 'tail'.\"\nAs you can see in the image below ğŸ‘‡\n.\nAs a person who writes code, it is valuable for you and others who will read the code to write it as verbosely as possible. In this manner, the code is readable and documents itself, removing the need for useless comments (which are hard to maintain in the long run).\n.\nğŸ’¡ My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQFsplyh0YtRbg/feedshare-shrink_800/0/1624867463197?e=1707350400&v=beta&t=9mRiAec5SeCtNupL367J43Id9MVjQlrTRfVpKAYENZk"
        },
        "Post_124": {
            "text": "ML models are limited, dumb, and wrong.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\ndata",
            "image": "https://media.licdn.com/dms/image/C4D22AQFrKm0kTozXEA/feedshare-shrink_800/0/1624867463951?e=1707350400&v=beta&t=MLPDFt4I0_9ApTwIlrDtaq95vyUcy5q-rIlTLq2xBW4"
        },
        "Post_125": {
            "text": "New Year's Eve is another trip around the sun, nothing special if you put it that way. But personally, I find it a great reminder to reflect on your life. I firmly believe that you must understand yourself to add value to the world.\n.\nFrom my perspective, it is essential to periodically ask yourself questions such as:\nâ What matters right now in my life?\nâ Are my current systems and habits aligned with my long-term goals?\nâ Do I have the same goals, or should I reevaluate them?\nâ How can I make life simpler and more meaningful?\nâ Is my life balanced for a happy and meaningful life?\n.\nAs human beings, we are constantly changing relative to our environment. Thus it is essential to reevaluate our desires continually. What we thought would make us happy at 20 years old might completely change at 25 years old, which is perfectly fine. We don't have to create our prison of desires.\nMy goal with this post is not to brag (I hate bragging) but to cast what I have in mind to the world.\n2022 was an intense year:\nâ I finished my ML master's degree and DS nanodegree\nâ I started my career as a freelancer\nâ I moved in with my girlfriend\nâ I started creating content on LinkedIn and Medium (as an introvert, this took me completely outside my comfort zone)\nâ I learned to accept who I am and to follow my path.\nIn 2023 I plan to:\nâ master the craft of bringing ML systems into production\nâ to continue to improve, learn, and add value by making content on LinkedIn and Medium\nâ to improve my freelancing skills: selling, communication, and negotiation\nâ to learn to work with people better\nâ to move to a tech hub in Europe and use ML to contribute to real-world such as the global warming\nâ to take better care of my mind: learn to meditate more profoundly, be curious, grateful, and present, to learn to be more kind to myself and more open to people\nâ to take better care of my body: I want to be more active by learning to play tennis, swim better, go to the sauna, and do cold baths\n.\nAt the end of the day, what else is there than living a life full of meaning and fulfillment?\n.\nThis was more of a personal and philosophical post, but I hope you liked it.\nHappy holidays and a new year full of prosperity and meaning  ğŸ‰ğŸ‰ğŸ‰.\nhashtag\n#\nlife\nhashtag\n#\nresolutions\nhashtag\n#\nwellbeing",
            "image": "https://media.licdn.com/dms/image/C4D22AQFbl5mv5FKfjA/feedshare-shrink_800/0/1624867444054?e=1707350400&v=beta&t=SeWExhpPicnFRn0fEF-4JXrmbIch1I9z2OhJWwZ3TH0"
        },
        "Post_126": {
            "text": "Have you ever felt overwhelmed by the monstrous number of tools you have to know to build an\nhashtag\n#\nMLOps\nsystem?\n.\nPersonally, I sure did. There are just too many...\nAs the field is only at the beginning, it is only natural for tools to pop all over the place.\nThough it is hard to keep track of them...\n.\nFortunately, here is a tool that summaries all of them and groups them relative to their functionality:\nâ Experiment tracking\nâ Data versioning\nâ Pipeline orchestration\nâ Runtime engine\nâ Artifact tracking\nâ Model registry\nâ Model serving\nâ Model monitoring\nCheck it out:\nhttps://mymlops.com/\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQFgD1PZn4NB6g/feedshare-shrink_800/0/1624867456703?e=1707350400&v=beta&t=y6BcO7W_poM0ir6sz9HzTMq9ge7A-Qw5tB8tjrKPlmA"
        },
        "Post_127": {
            "text": "ChatGPT's ability to \"solve\" leetcode/DSA exercises with pseudocode doesn't prove how awesome and advanced it is.\nIt proves how useless those kinds of problems are for hiring.",
            "image": "https://media.licdn.com/dms/image/C4E22AQHhJXdg9zbqfQ/feedshare-shrink_800/0/1624528753593?e=1707350400&v=beta&t=yG7uzE0FVIo3FJTkec2xLMCmo4WYAnfbkbrjYkvPZ4U"
        },
        "Post_128": {
            "text": "If users have the slightest possibility of inputting incorrect data, be sure they will.\n.\nThey could input text instead of numbers.\nThe images might be corrupted.\nThe numbers might exceed the supported range/interval.\nAnd the possibilities are endless.\n.\nThat is why data validation in a production system is essential.\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQEg3IKY0KqoVA/feedshare-shrink_2048_1536/0/1623660125369?e=1707350400&v=beta&t=X9G3K5KeLHz8rkuV72i3BqMnRSrBVxdYRJn-hP-nneE"
        },
        "Post_129": {
            "text": "I think\nhashtag\n#\nsampling\nis one of the most common words used in\nhashtag\n#\nmachinelearning\n. Why?\n.\nBecause we don't live in an ideal world where we are omnipresent and all-powerful, and thus we don't have access to all the data in the world or don't have the computing power to train a model with 100TB of data.\nTherefore, we use sampling to compile a representative subset of the whole population.\nWith an emphasis on \"representative.\"\n.\nIt is easy to sample a random subset, but making it representative is hard.\nIt is essential to pay attention to this detail. Otherwise, your model will be biased and prone to failure when you deploy it into production.\n.\nSome popular sampling methods are:\n- stratified sampling\n- weighted sampling\n- reservoir sampling\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFM2vMSrpMqHw/feedshare-shrink_2048_1536/0/1621427623180?e=1707350400&v=beta&t=yNIz6gHvclclrNhMSAeILoYO9-lfz5SU7R4pqK7BYSE"
        },
        "Post_130": {
            "text": "CSV vs. Parquet. Which one is better?\n.\nSuch questions do not have strict answers.\nLet's observe this question from a different perspective.\nWe should ğ«ğğŸğ¨ğ«ğ¦ğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğªğ®ğğ¬ğ­ğ¢ğ¨ğ§ ğšğ¬ ğŸğ¨ğ¥ğ¥ğ¨ğ°ğ¬:\n- row vs. column major\n- text vs. binary\n.\nğ‘ğ¨ğ° ğ¯ğ¬. ğ‚ğ¨ğ¥ğ®ğ¦ğ§ ğŒğšğ£ğ¨ğ«:\nIt is a method of accessing the data. A row-major structure can easily read a row from a file, and a column-major file can quickly retrieve a column.\nAlso, it is faster to write row-major structures.\nCSV: row-major\nParquet: column-major\n.\nğ“ğğ±ğ­ ğ¯ğ¬. ğğ¢ğ§ğšğ«ğ²:\nA text file is human-readable, while a binary file consumes less storage and is faster to read.\nCSV: text\nParquet: binary\n.\nğŸ§ There is no correct answer on which file type you choose. It all depends on your use case. This also applies to other persistence mechanisms.\nğŸ“ƒ But, for example, you could use a CSV file to log real-time data. Meanwhile, you can schedule a different process that regularly reads the CSV file, transforms it into a parquet file, and stores it in your data warehouse for long purpose use-cases.\nThrough this, we can leverage the best of both worlds.\n.\nğŸ’¡ My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQHWPk1KX6LmNg/feedshare-shrink_800/0/1621353923984?e=1707350400&v=beta&t=i6CmUTnQQFmafD07zJdv2WzwWCH9oyTPfxJOYzvkIDM"
        },
        "Post_131": {
            "text": "Is active learning a game changer on how we label data?",
            "image": "https://media.licdn.com/dms/image/C5622AQElhwSF0A90sQ/feedshare-shrink_800/0/1571500709235?e=1707350400&v=beta&t=epxjuoC-EMwjU8NgC9rdchmXiG8oEydteyBUBtsXH2E"
        },
        "Post_132": {
            "text": "I find weak supervision labeling methods very intriguing.\n.\nğ—ªğ—µğ—®ğ˜ ğ—¶ğ˜€ ğ˜„ğ—²ğ—®ğ—¸ ğ˜€ğ˜‚ğ—½ğ—²ğ—¿ğ˜ƒğ—¶ğ˜€ğ—¶ğ—¼ğ—»?\nIt is a method of labeling your data through a set of labeling functions (LF).\nThe LF represents a set of heuristics based on which you can programmatically generate ground truth for your data.\n.\nğ—™ğ—¼ğ—¿ ğ—²ğ˜…ğ—®ğ—ºğ—½ğ—¹ğ—² ğŸ‘‡ğŸ»\nYou want to create a model that predicts the type of LinkedIn posts.\nBased on the hashtags of some posts, you can create your desired labels.\nNow you can train a generalized model that can predict the type of a post, whether it has hashtags or not.\n.\nğ—ªğ—µğ˜† ğ—¶ğ˜€ ğ—¶ğ˜ ğ˜€ğ—¼ ğ—½ğ—¼ğ˜„ğ—²ğ—¿ğ—³ğ˜‚ğ—¹?\nâ You can quickly scale your labeling infrastructure.\nâ It is cheap (or at least cheaper than hand labeling).\nâ The rules can quickly be shared among peers.\n.\nğ—•ğ˜‚ğ˜, ğ˜‚ğ—»ğ—³ğ—¼ğ—¿ğ˜ğ˜‚ğ—»ğ—®ğ˜ğ—²ğ—¹ğ˜†, ğ—»ğ—¼ğ˜ğ—µğ—¶ğ—»ğ—´ ğ—¶ğ˜€ ğ—½ğ—²ğ—¿ğ—³ğ—²ğ—°ğ˜...\nâ Using this method, your labels will most likely be noisy.\nâ Sometimes, you cannot write the heuristics programmatically.\nBoth cases,  if they happen, could be a complete killer for your ML model.\n.\nğŸ’¡ Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQGycMyQqF9LjA/feedshare-shrink_800/0/1561826703102?e=1707350400&v=beta&t=aVX2mE1iJw5z-Y0A7UX57JBSNvQxvpB9Kh3oB837hH4"
        },
        "Post_133": {
            "text": "Your model's performance heavily depends on the ğ—¾ğ˜‚ğ—®ğ—¹ğ—¶ğ˜ğ˜† and ğ—¾ğ˜‚ğ—®ğ—»ğ˜ğ—¶ğ˜ğ˜† of your training data and labels.\n.\nThe reality is that hand-labeling qualitative data is often complicated and expensive.\nSome major issues that might appear are:\nâ data privacy: Some annotators might not be allowed to look at the data, or the data cannot leave a particular place.\nâ label multiplicity: You use multiple sources and annotators, which might introduce a lot of noise.\nâ data lineage: Your new batch of annotated data, which you thought would save your model, actually burnt it down. The real issue is that you haven't versioned your data batches, so you cannot differentiate between the good and bad samples.\n.\nğŸ’¡ Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFhwQYT4N8fdg/feedshare-shrink_800/0/1577744773434?e=1707350400&v=beta&t=J-V9Izcy3FTxkXwZhhpBXoDmQyGL09S17FO6U8ke1gs"
        },
        "Post_134": {
            "text": "I am excited to see the drastic performance improvement in the latest version of Python.\nAs you can see, Python 3.11 is ~35% faster than Python 3.10.\nThat is incredibly powerful to the Python community and for everything built with Python in the future.\nhashtag\n#\npython\nhashtag\n#\nml\nhashtag\n#\nmlops"
        },
        "Post_135": {
            "text": "I was pretty excited when I found out that on scikit-learn-contrib you can find all kinds of extensions to the scikit-learn library. It contains various tools that are compatible with the scikit-learn API.\nSome of the most popular extensions are imbalaced-learn (tools to handle imbalanced datasets), category_encoders (various encoders for your categorical variables), hdbscan (one of the best clustering algorithms), etc.\nâ Check them out:\nhttps://lnkd.in/dPZmGZKv\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nsklearn"
        },
        "Post_136": {
            "text": "Check out the most common metrics you should know for\nhashtag\n#\nclustering\n.\n.\n.\n.\nFor unsupervised methods, such as clustering, we cannot use standard classification metrics because we don't have any ground truth.\n.\nThe most common metrics, where you don't require information about the ground truth labels, are:\n1. Silhouette score:\n- defined as the ratio between the mean distance of the intra-cluster and the mean distance to the nearest cluster\n- it ranges within [-1, 1], the higher, the better\n- tells us how well-assigned each point is:\n- '1' indicates that the sample is well-assigned to the cluster\n- '0' indicates that the sample is on the decision boundary\n- '-1' indicates that the sample is in the wrong cluster\n2. Calinski Harabaz index:\n- defined as the ratio between the within-cluster dispersion and the between-cluster dispersion\n- the higher, the better\n3. Davies-Bouldin Index:\n- defined as the average similarity measure of each cluster with its most similar cluster\n- it is helpful to decide on the number of clusters\n- it ranges within [0, +inf]. The smaller, the better\n4. Dunn's Index\n- defined as the minimum inter-cluster distance divided by the maximum cluster size\n- the higher, the better\n.\nThe metrics described above give a numerical interpretation of how well the clusters are separated. Because we don't have labels, it cannot determine the validity of the model's predictions.\nWe can use those methods to decide on the optimal number of clusters and to compare the performance of various unsupervised algorithms.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nclustering\nhashtag\n#\nmathematics"
        },
        "Post_137": {
            "text": "I recently found an intuitive way to understand the main families of clustering methods.\n.\n.\nYou can split all the clustering methods into:\n- ğŸğ¥ğšğ­\n- ğ¡ğ¢ğğ«ğšğ«ğœğ¡ğ¢ğœğšğ¥ (solves the resolution problem within your data)\nand into:\n- ğœğğ§ğ­ğ«ğ¨ğ¢ğ-ğ›ğšğ¬ğğ/ğ©ğšğ«ğšğ¦ğğ­ğ«ğ¢ğœ (it does NOT perform well when you don't know the shape of the data or the data has many dimensions)\n- ğğğ§ğ¬ğ¢ğ­ğ²-ğ›ğšğ¬ğğ/ğ§ğ¨ğ§-ğ©ğšğ«ğšğ¦ğğ­ğ«ğ¢ğœ (robust to noise)\n.\nThose two types of families are orthogonal.\nTherefore, you can have hierarchical density-based algorithms.\nThose are one of the best-performing clustering algorithms (e.g., HDBSCAN).\n.\nAs a bonus:\nâ Check out this excellent clustering cheatsheet from sklearn:\nhttps://lnkd.in/dXaSCiXG\nâ Also, this video which explains what I wrote above in more detail:\nhttps://lnkd.in/dUDfuYxP\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nclustering\nhashtag\n#\nmathematics"
        },
        "Post_138": {
            "text": "Everybody says that the best way to learn something is by doing projects. Read below why this is not true.\n.\n.\nI am not against projects or pro courses/books.\nBut, if you learn only by putting it into practice, you won't be able to connect the dots in the long run.\n.\nWhy?\nBecause while you are developing something, you want concrete answers. Therefore, you probably won't take your time to focus on the fundamentals.\n.\nTo scale what you are learning, you have to master the fundamentals of the topic, and only after that should you add more complexity on top of it.\nOtherwise, you will memorize something, and in the long run, you will forget it.\n.\nI am not against project learning. I am pro projects, and I think it is a great source to understand why a piece of theory is useful.\nBut, I am trying to say that it is not enough.\n.\nSo, how can you get the best out of both worlds?\nWell, there should be a balance between practice and theory.\n.\nStart with a brief theoretical introduction to the topic you want to learn.\nThen, implement a project. Understand what is important and ask the right questions.\nIn the end, go again to that piece of theory and try to answer all your questions. Try to fill the gaps.\n.\nUnfortunately, learning is not linear.\nMy suggested learning path would be something like this:\ntheory -> practice -> theory -> practice -> ...\nAt every iteration, you will get a deeper understanding of the domain.\nhashtag\n#\nlearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nsoftware"
        },
        "Post_139": {
            "text": "Check out a great video that intuitively explains the math behind the convolution operation.\nSay no to complicated math formulas, jargon, and the desire to look smart.\nIt is all about intuition and the beauty behind math.\nThe video is by no other than 3Blue1Brown.\nHere is the link:\nhttps://lnkd.in/dnastedV\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\ndatascience"
        },
        "Post_140": {
            "text": "This is a good rule of thumb to judge the strength of the\nhashtag\n#\ncorrelation\nbetween two variables:\n.\n.\nStrong: 0.7 <= | c | <= 1.0\nModerate: 0.3 <= | c | < 0.7\nWeak: 0.0 <= | c | < 0.3\nc = correlation values, ranging between [-1, 1]\n.\nThis is only a heuristic. It might vary in different contexts.\nBut it is a good starting point to highlight the most critical variables of your specific problem.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\ncorrelation\nhashtag\n#\nmathematics"
        },
        "Post_141": {
            "text": "This is hypothesis testing in a nutshell.\n.\n.\nIt contains a null hypothesis (H0) and an alternative hypothesis (H1).\n.\nThe null hypothesis (H0) is true before you collect any data.\nH0 usually states there is no effect or that two groups are equal.\nH0 contains an equal sign of some kind - either =, â‰¤, or â‰¥.\nH1  is what we would like to prove to be true.\nH1 contains the opposition of the null - either â‰ , >, or <.\nThe H0 and H1 are competing and non-overlapping hypotheses.\n.\nOne example of hypothesis testing.\nWe implemented a new design for our website. The end goal is to have more traffic. To measure the improvement of the new change, we can use hypothesis testing as follow:\nH0: avg_daily_visits_new â‰¤ avg_daily_visits_old\nH1: avg_daily_visits_new > avg_daily_visits_old\navg_daily_visits_x = the average number of daily visits on the website for design X\n.\nThere are two types of errors:\nğ“ğ²ğ©ğ ğˆ ğğ«ğ«ğ¨ğ«: we believe the alternative to be true, but in reality, the null is true (denoted by the symbol Î±)\nğ“ğ²ğ©ğ ğˆğˆ ğğ«ğ«ğ¨ğ«: we believe the null to be true, but in reality, the alternative is true (denoted by the symbol Î²)\n.\nAnother example:\nWe want to test the parachutes before using them, which can either work or not.\nH0: the parachute doesn't open (safer to assume none of the parachutes work until enough data suggests otherwise)\nH1: The alternative is that the parachute opens, which makes sense as it is what we would like to prove.\nIn this case, a Type I error would be that we think the parachute will open (H1) when it doesn't work (H0). Such an error would result in killing somebody.\nTherefore, Type I errors are worst than Type II errors. The threshold Î± of accepting Type I error varies on different applications.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nhypothesistesting\nhashtag\n#\nmathematics"
        },
        "Post_142": {
            "text": "This AI robot wakes up and takes a deep breath\nâ†“\nCheck out\nhttps://AlphaSignal.ai\nto get a summary of top publications and breakthroughs in Machine Learning.\n-\nhashtag\n#\ndeeplearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nneuralnetworks\nhashtag\n#\nnlp\nhashtag\n#\nrobotics\nhashtag\n#\nai\nhashtag\n#\ndatascience"
        },
        "Post_143": {
            "text": "Let's understand what bootstrapping is all about.\n.\n.\nBootstrapping is a technique where we sample from a group with replacement (replacement=after we pick one item, we put it back, therefore, at the next event, there is an equal probability that we can pick it again).\n.\nWhy do we even care about this?\n.\nUsually, we have only one sample from our population, which is not enough to generate a sampling distribution (we need more samples to do that).\n.\nWe can use bootstrapping to simulate the creation of multiple samples. Therefore, we can easily simulate our sampling distribution.\n.\nIt is effortless to understand.\n1. We have our actual sample/observations.\n2. We create a new sample by sampling with replacement from our observations.\n3. We compute our desired statistic on the bootstrapped sample (e.g., mean).\n4. Repeat this many times (e.g., 10k times)\n5. We end up with 10k samples from which we can compute the sampling distribution, standard error, confidence intervals, etc.\n.\nThis technique is also used in well-known methods such as Random Forest and Stochastic Gradient Boosting.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nbootstrapping\nhashtag\n#\nmathematics"
        },
        "Post_144": {
            "text": "What is Concept Drift?\nAs data changes over time, so should your AI / ML model.\nA concept drift refers to a situation when your original target distribution has changed from your previous model.\nDownload our whitepaper for more:\nhttps://lnkd.in/etihQ8am\nIn the image below, you can observe an example of our monitoring in real-life. The teal and red bars, from the image below, represent the upper and lower bounds for the metrics (i.e. the intervals within which the data and model behave as expected). coreControl has out-of-the-box statistical tools to detect drift, such as KL divergence, KS statistics, tree methods, etc. After the monitoring system detects any value outside the allowed interval, an alarm is triggered, and the coreControl monitoring system will notify you that a specific drift is detectedâ€‹â€‹.\nhashtag\n#\nai\nhashtag\n#\nml\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nmlops\nhashtag\n#\ncoreai"
        },
        "Post_145": {
            "text": "What is the difference between the Law of Large Numbers and the Central Limit Theorems?\n.\n.\n1. ğ‹ğšğ° ğ¨ğŸ ğ‹ğšğ«ğ ğ ğğ®ğ¦ğ›ğğ«ğ¬: the larger the sample size, the closer our statistic gets to the parameter. The larger the sample size, the more significant the chance that the sample is a good representation of the population.\n2. ğ“ğ¡ğ ğ‚ğğ§ğ­ğ«ğšğ¥ ğ‹ğ¢ğ¦ğ¢ğ­ ğ“ğ¡ğğ¨ğ«ğğ¦: if our sample size is large enough, the sample mean will be normally distributed. That is why we use the normal distribution when comparing different samples from a population.\n.\nThose two theorems say that if the sample is large enough, we can estimate the statistics of the population (1) by leveraging normal distributions (2).\nBoth theorems are intuitive and important to know to master your\nhashtag\n#\ndata\nproperly.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nmathematics"
        },
        "Post_146": {
            "text": "The only moment when coding looks cool to the outside world is when you do it quickly and look like a hacker.\nhashtag\n#\npython\nhashtag\n#\nprogramming\nhashtag\n#\nsoftwareengineering"
        },
        "Post_147": {
            "text": "What is the difference between the standard deviation and the standard error?\n.\n.\nâ Standard Error:\nâ€¢ The standard deviation of the statistics distribution\nâ€¢ For example, we have five samples from a population with the sample mean equal to 3, 5, 7, 10, 50. The standard deviation of the mean of the samples is ~19.7, which is called the standard error.\nâ€¢ The standard error decreases as we increase the sample size. This is true because as we have bigger samples, the mean of a sample is closer to the mean of the population, and therefore, it won't vary so much.\n.\nâ Standard Deviation:\nâ€¢ It is a general measure of spread that can be used for any set of points.\nâ€¢ In this scenario, the standard error is a particular case of the standard deviation.\nâ€¢ The standard error is just a naming convention for the standard deviation of the sample means (conventionally known as the statistical distribution of the given samples).\n.\nğğ¨ğ­ğ: Instead of the sample mean we could have used any other aggregate method that holds the Central Limit Theorem.\nhashtag\n#\nstandardeviation\nhashtag\n#\nstandarderror\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata"
        },
        "Post_148": {
            "text": "What is the importance of finding the best\nhashtag\n#\nsimilary\nmeasure for your\nhashtag\n#\nML\nalgorithm?\n.\n.\nThe similarity measure measures how much alike two data objects are.\nThis similarity score is the very basic building block in algorithms such as:\n- Recommendation engines,\n- Clustering,\n- Different classification problems, etc.\nIf the distance is small, the features have a high degree of similarity. At the same time, a considerable distance will be a low degree of similarity (or otherwise, the main idea is that we can numerically observe the 'equality' between two objects).\n.\nHere is a list of the most popular methods out there.\nThe smaller, the more significant the similarity between the two points (this is the case when we use distances as a similarity score):\n- Euclidean distance (geometric distance)\n- Manhattan distance (geometric distance)\n- Minkowski distance (a generalization of Euclidean & Manhattan)\n- Chebyshev distance (geometric distance)\n- Mahalanobis distance (distance between a point and a distribution)\nThe bigger, the more significant the similarity between the two points:\n- Cosine Similarity (the angle between two vectors)\n- Jaccard Similarity / Dice Score (used between sets)\n- Pearson/Spearman correlation (the trend between two vectors)\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nsimilarity\nhashtag\n#\nmetrics\nhashtag\n#\nmathematics"
        },
        "Post_149": {
            "text": "How often do you feel lost trying to understand what is going on in your\nhashtag\n#\ndata\nhashtag\n#\npipeline\n?\n.\n.\nIn my case, I often can't tell 100% the specific output of every stage of a pipeline without using a debugger.\nThis is amplified by the fact that we are often using Python.\nUsually, in Python code (or another dynamic programming language), you always have all kinds of surprises, as the variables can hold anything without any compiling errors.\n.\nI recently stumbled into Mage AI, a tool that solves this problem.\n.\nIt allows you to quickly\nhashtag\n#\nbuild\n,\nhashtag\n#\norchestrate\n, and\nhashtag\n#\nvisualize\ndata pipelines.\nIt gives you an intuitive GUI from which you can manage your entire data pipeline.\nThe sweet spot about this tool is that it allows you to plot the output of every stage within your data pipeline without writing any code.\nAnd...\nYou can quickly hook unit tests to every stage of the pipeline.\nThis is awesome.\nNo more guessing around. Now you can quickly validate and visualize every step of your data flow.\n.\nIt is more than an orchestration tool.\nAs a bonus, with a few lines of code, you can easily integrate it with Airflow.\n.\nCheck out their GitHub:\nhttps://lnkd.in/dcaSqbmv\n.\n.\nğŸ‘‹ If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dxV7Dnqv"
        },
        "Post_150": {
            "text": "Data, Data, Data... AI\nhashtag\n#\nmlops\nhashtag\n#\ndata\nhashtag\n#\ndatacentricai\nhashtag\n#\naiinnovation\nhashtag\n#\nbi\nhashtag\n#\nai2bi\nhashtag\n#\nai"
        },
        "Post_151": {
            "text": "Failing is the fastest way to succeed.\nYou thought this was a motivational speech?\nMaybe.\n.\nBut this concept also applies in\nhashtag\n#\nML\nwhen you want to find the best solution.\nThe faster you fail in your experimentation process, the quicker you understand what you did wrong.\nTherefore, the quicker you can adapt and develop better models.\n.\nDon't be afraid to fail.\nFailing is the fastest way to move forward.\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmlops\nhashtag\n#\ntraining"
        },
        "Post_152": {
            "text": "An AI system was asked to regenerate Human Evolutionâ€¦ A project by Fabio Comparelli.\nAll imagery generated with the latest AI tools, midjourney, stable diffusion etc. Prob. the most interesting (and disturbing) movie output Iâ€™ve seen up to now with these specific technologies. As itâ€™s able to transcend the mere visual trickery, which is fun and technically mind blowing, but often remains flat as a hat ğŸ˜‰\nAlso note that projects like these do involve a great level of effort and experimenting of the human artist, before youâ€™d actually make it to an end result like this. All movie frames are created by AI though and reflect, through their datasets, existing perceptions of our past and a seemingly unavoidable futureâ€¦\nIâ€™ve added a link to Comparelliâ€™s website down below."
        },
        "Post_153": {
            "text": "Group By vs. Window Functions. Two methods every\nhashtag\n#\nML\nengineer should know.\n.\n.\nGroup By aggregates multiple rows into a single row.\nWindow Functions aggregate multiple rows and output one value for each row.\nBoth are inter-rows operations.\n.\nGroup By performs inter-rows operations, but the information is compressed into a single data point (e.g., the average sales of a shop).\nWindow Functions perform inter-rows operations, but the number of rows stays the same (e.g., the average shop sales for the last three days).\nBoth operations are precious in doing your data preprocessing in the right way.\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nsql"
        },
        "Post_154": {
            "text": "How can you think about your future self and colleagues when writing\nhashtag\n#\ngit\nhashtag\n#\ncommits\n?\n.\n.\n.\nYou may think writing git commits messages is trivial or unimportant, but that is not the case. The reader needs to understand what happened all over the development history.\n.\nTo fully leverage the message from a\nhashtag\n#\ngit\nhashtag\n#\ncommit\n, it should follow the following format:\n\"\"\"\n<type>[optional scope]: <description>\n[optional body]\n[optional footer(s)]\n\"\"\"\nChoose your favorite editor:\nâ†’ git config --global core.editor \"nano\"\nStart writing:\nâ†’ git commit\n.\nThe <type>: feat, fix, docs, style, refactor, text, etc.\nThe <scope>: a function, file, or module you worked on (optional).\nThe <description>:\n- Should be no greater than 50 characters.\n- Should begin with a capital letter and not end with a period.\n- Use an imperative tone: 'If applied, this commit will <commit description>.'\nThe <body>:\n- Separate the subject from the body with a blank line.\n- Detailed explanation and context of the new changes.\n- Use the body to explain the what and why of a git commit, not the how.\n- Optional - Add the body if the changes are complex enough.\nThe <footer>: reference issue tracker IDs (optional).\n.\nOk. So why all of this is important?\nBy respecting this format, you can:\n- Automatically generate CHANGELOGs.\n- Automatically compute a semantic version bump.\n- Communicate the changes' nature to teammates, the public, and other stakeholders.\n- Trigger build and publish processes.\n- Make it easier for people to contribute to your projects.\nI think that is pretty cool. You can do all of that just by respecting a few writing conventions.\nhashtag\n#\nmachinelearning\nhashtag\n#\nsoftware\nhashtag\n#\ncoding\nhashtag\n#\ngit\nhashtag\n#\ngoodpractices"
        },
        "Post_155": {
            "text": "For all the\nhashtag\n#\nSQL\nfans out there. Can you train\nhashtag\n#\nML\nmodels directly using\nhashtag\n#\nSQL\n?\n.\n.\n.\nIt is crazy, but it turns out you can!\nI recently learned about\nhashtag\n#\nMindsDB\n, which lets you train and use models directly from your\nhashtag\n#\nSQL\nqueries.\nUsing this tool, you can pour your data into the model and save its predictions using\nhashtag\n#\nSQL\ndirectly interacting with your database. No other data pipelines are required.\nIt is as easy as running:\n\"\"\"\nCREATE PREDICTOR mindsdb.customer_churn_predictor\nFROM files\n(SELECT * FROM churn)\nPREDICT Churn;\n\"\"\"\nUnder the hood, by default, they use a proprietary\nhashtag\n#\nAutoML\nengine to find the best solution for your data.\nYou can customize this behavior with your models and hyper-parameters, but internally they support many operations.\nFor example, when customizing, you can use your trained model from\nhashtag\n#\nMLFlow\nto make predictions directly into your database.\nThis tool can help you avoid writing unnecessary data and machine learning pipelines when your infrastructure heavily depends on SQL databases.\nYou can use this tool as SasS or self-hosted.\n.\nCheck out their docs:\nhttps://docs.mindsdb.com/\n.\n.\nğŸ‘‹ If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dxV7Dnqv"
        },
        "Post_156": {
            "text": "As a\nhashtag\n#\nML\nengineer, did you ever feel overwhelmed by everything you have to know?\n.\n.\nProbably, you are not the only one.\nI often have to relearn all kinds of concepts if I didn't use them for a long time.\nThe problem is that sometimes you can't find that priceless piece of information that will revive your\nhashtag\n#\nML\nmodel.\nSometimes, the sea of information, the internet, is just too daunting.\nRecently, I read a book that is meant, in such moments, to light up your day.\nIt is called \"Why Does My Neural Network Not Learn\" by Mark Schutera and Frank Hafner.\nIt includes best practices from applying\nhashtag\n#\ndeeplearning\nin\nhashtag\n#\nresearch\nand\nhashtag\n#\ndevelopment\n.\nBecause it is a compact book, I think it is excellent to quickly refresh your memory on specific concepts while developing your\nhashtag\n#\nML\nsystem.\nI think it is perfect for\nhashtag\n#\nML\npractitioners to use it as their daily knowledge index.\nâ˜•  I completely resonated with this quote:\n\"What applause is for the musician, reviews are for the writer, and coffee for the engineer.\"\nğŸ‘‹ If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_157": {
            "text": "Have you ever wondered how hard it is to switch from\nhashtag\n#\nPandas\n+\nhashtag\n#\nSklearn\nto\nhashtag\n#\nSpark\n?\n.\n.\nWell... Not that hard.\nThere are a few things to understand while using\nhashtag\n#\nSpark\n, like pure functions and lazy initialization.\nIt is a different ecosystem, but within\nhashtag\n#\nPySpark\n, we have access to Spark DataFrames & Spark ML interfaces.\nThe guys from Spark tried to keep their interface as close as possible to Pandas and Sklearn (at least for PySpark).\nYou can see that for yourself in the following article, where I created an end-to-end tutorial for churn prediction using only\nhashtag\n#\nSpark\n.\nThe tutorial will take you from how you can load a data source to how to do feature engineering and run models with cross-validation.\n.\nğŸ‰ I want to thank\nTowards AI\nfor considering this article in their \"Top 3 Best Articles\" of their weekly newsletter.\nTheir newsletter is a great way to stay in touch with the latest updates from the AI world:\nhttps://lnkd.in/gG4upkCD\n.\nâ†’ Check out the article:\nhttps://lnkd.in/grQdudF5\n.\nğŸ‘‹ If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_158": {
            "text": "This is how you can quickly write context managers (resources accessed by the \"with\" keyword)  in\nhashtag\n#\nPython\n.\n.\n.\nThe most known way to write ğœğ¨ğ§ğ­ğğ±ğ­ ğ¦ğšğ§ğšğ ğğ«ğ¬ in\nhashtag\n#\nPython\nis using ğœğ¥ğšğ¬ğ¬ğğ¬ that implement the __ğğ§ğ­ğğ«__() and __ğğ±ğ¢ğ­__() methods.\nI recently stumbled into a lighter way to achieve the same behavior.\nYou can use the @ğœğ¨ğ§ğ­ğğ±ğ­ğ¦ğšğ§ğšğ ğğ« ğğğœğ¨ğ«ğšğ­ğ¨ğ« over any ğŸğ®ğ§ğœğ­ğ¢ğ¨ğ§, as shown in the pinned image.\nâ™ Besides decorating the function, there are four things that you have to consider within your function:\n1. Acquire your resource within the function.\n2. Use a \"try...except...finally\" block.\n3. Return your resource using the \"yield\" keyword within the \"try\" block (not by using \"return\").\n4. Release your resource within the \"finally\" block.\nâ™ When you call the function with the \"with\" keyword,\nhashtag\n#\nPython\nwill do the following:\n1. Execute all the code until the \"yield\" statement.\n2. Return your resource.\n3. When the \"with\" block ends, it will execute the code starting from the \"yield\" statement until the end of the function.\nAnd that is all that you need to know!\nâ™ You may ask, why does this even matter?\nWell is just syntactic sugar!\nThis method is better than classes when you don't have a lot of logic and want to keep things clean while using the \"with\" statement.\nOtherwise, if your logic is pretty concise, your code would be over 50% boilerplate code, using the standard syntax that uses classes.\nAlso, from my perspective, the logic flow is easier to follow within a single function.\nğŸ‘‹ If you want to read my weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_159": {
            "text": "Fully autonomous cars are still not quite ready for scale.\nâ€¦ but by the time I retire, itâ€™s highly likely Iâ€™ll be able to get in my car in the evening, tell it where to go, read a bit, watch a movie, go to sleepâ€¦ and then wake up the next morning at my destination.\nI find this fascinating.\nVolkswagen just released this thought-provoking concept design. They call is â€˜Gen Travelâ€™. It envisions what future travel might look like once fully autonomous fully electric technology is ready.\nI particularly love the seats that turn into beds, the ambient lighting, and the 360 degree views.\nâ€¦ and imagine all the hours we will be able to free up.\nHard not to be excited about the possibilities ahead.\nhashtag\n#\ntechnology\nhashtag\n#\ninnovation\nhashtag\n#\nmobility\nhashtag\n#\nautomotive"
        },
        "Post_160": {
            "text": "From Web 2.0 to Web3 ğŸš€\nThe main distinction between Web 2.0 and Web 3 is that Web 3.0 is built on decentralization â›“\nFurthermore, users will own their content and have complete control over using the internet ğŸ”‘\nThe possibilities for this new technology to completely change how users, developers, and brands all interact are significant ğŸ“Š\nâ€œI've become so numb I can't feel you there\nBecome so tired So much more aware I'm becoming thisâ€¦â€ ğŸ¶\nâ˜€ï¸ğŸ«ğŸŒğŸ˜´"
        },
        "Post_161": {
            "text": "Yet another post of an\nhashtag\n#\nAI\ngenerating some incredible images.\nI generated a random image with the stable diffusion model, open-sourced by\nstability.ai\n.\nWhat they did is incredible!\nI would personally put this image on my wall.\nI guess this is amazing and scary at the same time.\nğŸ‘‹ If you want to read my weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_162": {
            "text": "How do you write an excellent\nhashtag\n#\nREADME\nfile that catches people's attention?\n.\n.\nYou know the saying, \"The cover sells the book.\"\nThis is also true for public repositories.\n\"The README sells the code.\"\nEvery README, to look professional, should cover at least the following:\n1.  What your project does:\nA concise, single-paragraph describing your project + A screenshot or even an animated GIF.\n2.  How to install it:\nA code block in your README that shows step-by-step what to type in the shell to set up the whole project (From my experience, this is the most crucial step.).\n3.  Example usage:\nAdd a few snapshots that will show the user how to use the core of your work.\n4.  How to set up the dev environment:\nDescribe how to install the development dependencies and how to run the tests.\n5.  How to contribute:\nA description of the development process.\n6.  Change log\nShare the changes to the project + Give credit to contributors publicly.\n7.  License, Author Information, and Acknowledgements\nWrite a statement about the project's license, the contact information of the author, and acknowledgments to contributors or other 3rd party vendors.\nOther possible sections for your\nhashtag\n#\nML\nproject:\n- Data (describe the datasets you used and how to download them)\n- Results (if you trained any models, show your evaluation results in a table)\nğŸ‘‹ If you want to read my weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_163": {
            "text": "Excited that I have the chance to work on coreControl ğŸ‰.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nai"
        },
        "Post_164": {
            "text": "From research to production and back for continuous improvement - Training and maintaining AI Models requires managing several MLOps functions:\nâ–ª\tExperiment Management / KPIs\nâ–ª\tData Versioning\nâ–ª\tGithub version\nâ–ª\tModel Serving\nâ–ª\tModel Monitoring\nâ–ª\tData Monitoring\nâ–ª\tFinOps and Infrastructure\nRead more:\nhttps://lnkd.in/etihQ8am\nhashtag\n#\nmlops\nhashtag\n#\nfinops\nhashtag\n#\nversioncontrol\nhashtag\n#\naiops\nhashtag\n#\nai\nhashtag\n#\ncontinouslearning\nhashtag\n#\nmodelsearch\nhashtag\n#\nmonitoring"
        },
        "Post_165": {
            "text": "If you are interested in\nhashtag\n#\nai\n&\nhashtag\n#\nscience\nand psychology & the metaphysical, you need to check out this podcast.\nIt is a great discussion that takes into consideration both planes.\nCheers to\nLex Fridman\nfor his unique content and Jordan Peterson for his out-of-the-box concepts."
        },
        "Post_166": {
            "text": "Here's my conversation with Jordan Peterson about Nietzsche, Dostoevsky, Putin, the search for meaning, the corrupting effects of fame & power, and advice on how to think and how to live. This conversation was intense, challenging, and fascinating."
        },
        "Post_167": {
            "text": "ğŸ˜¿ You don't have enough labeled data?\nğŸ¤– Check out this Python package that will change your ML game.\nğ¦ğ¨ğğ€ğ‹ is a Python tool/package for ğšğœğ­ğ¢ğ¯ğ ğ¥ğğšğ«ğ§ğ¢ğ§ğ .\nBased on previous data samples, the package offers the developer out-of-the-box strategies to ğ¥ğšğ›ğğ¥ ğ®ğ§ğ¬ğğğ§ ğğšğ­ğš in a semi-supervised fashion.\nThe package is built around ğ­ğ°ğ¨ ğ¦ğšğ¢ğ§ ğœğ¨ğ§ğœğğ©ğ­ğ¬:\n- an estimator (e.g., RandomForestClassifier)\n- a query strategy\n\"\"\"\nlearner = ActiveLearner(\nestimator=RandomForestClassifier(),\nquery_strategy=uncertainty_sampling\n)\n\"\"\"\nAn ActiveLearner class is a wrapper over a given estimator. The ğğ¬ğ¬ğğ§ğ­ğ¢ğšğ¥ method it exposes is ğªğ®ğğ«ğ²(ğ—).\nYou can quickly ğ¥ğšğ›ğğ¥ your ğ§ğğ° ğ¢ğ§ğœğ¨ğ¦ğ¢ğ§ğ  ğğšğ­ğš using the given query strategy. The package supports out of the box, the following processes (but you can implement your strategy):\n- max uncertainty sampling\n- max margin sampling\n- entropy sampling\nOne use-case where the tool can easily be implemented and adds value is ğğ¨ğ¨ğ¥-ğ›ğšğ¬ğğ ğ¬ğšğ¦ğ©ğ¥ğ¢ğ§ğ .\nğŸ“ The ğğ±ğšğ¦ğ©ğ¥ğ from modAL states the following:\nWe split the good old iris dataset into a ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ¬ğğ­ ğ‹ with only three random examples and into an ğ®ğ§ğ¥ğšğ›ğğ¥ğğ ğ¬ğğ­ ğ• (we drop the targets for the sake of the example).\nWe used a simple KNeighborsClassifier with three neighbors.\nInitially, the model was ğ­ğ«ğšğ¢ğ§ğğ ğ¨ğ§ğ¥ğ² ğ¨ğ§ ğ­ğ¡ğ«ğğ ğ¬ğšğ¦ğ©ğ¥ğğ¬ and performed poorly.\nAfter, ğ¢ğ­ğğ«ğšğ­ğ¢ğ¯ğğ¥ğ², we ğšğğğğ ğŸğŸ ğ¬ğšğ¦ğ©ğ¥ğğ¬, one by one. We can see in the graph below that only after four samples were ğ¥ğšğ›ğğ¥ğğ ğ›ğ² ğ¦ğ¨ğğ€ğ‹ did the ğšğœğœğ®ğ«ğšğœğ² ğ ğ«ğ¨ğ° ğ«ğšğ©ğ¢ğğ¥ğ².\nThe performance is fascinating.\nIt has support for Sklearn, Keras and Pytorch.\nI strongly consider using this package in my current projects.\nâ“What is your opinion about it? Would you trust such a method for your semi-supervised use cases?\nğŸ Python Package:\nhttps://lnkd.in/d9MeJ9X2\nâœ… Pool-based sampling detailed example:\nhttps://lnkd.in/dXpD3czr"
        },
        "Post_168": {
            "text": "ğŸ‘€ This Is what you need to know to build an MLOps end-to-end architecture:\nğŸ¦¾ I wrote a Medium article in which I presented 7 simple steps to consider when designing your MLOps infrastructure to ensure scalability, shareability, and reusability.\n1. ML development\n2. Training operationalization\n3. Continuous training\n4. Model deployment\n5. Prediction serving\n6. Continuous monitoring\n7. Data & model management\nğŸ“¢ If you enjoyed this article and want to read more similar content, please support me by following me on Medium -\nhttps://lnkd.in/dvB39TmV\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\nhashtag\n#\nsoftwarearchitecture"
        },
        "Post_169": {
            "text": "Is climate change a myth or reality?\nThis is an animation that shows the temperature change by country from the years 1880-2021.\nI saw some discussions around whether this size of sample data is reasonable for us to have an assumption of climate change. Compare with the age of the earth, the data we have only covered less than 0.0000038% of the time, however, the first reliable mercury thermometer has been invented just around 300 years ago.\nWe always say that let data speak for itself, and in an ideal condition, we collected sufficient related unbiased data to make a correct conclusion/ prediction.\nThe reality is that most of the time, we just use the data we can get as much as possibleÂ  (with quality control), form a â€˜biasedâ€™ understanding, then make the â€˜bestâ€™ business decision we could.\nUsually, we just try to use the imperfect dataset to make a decision that benefits the current situation the most. Nothing is perfect in this process. But I guess this is also the charm of data science that we can always do better.\nAnimation by Antti Lipponen, source is NASA GISTEMP.\nFor ML/ AI/ Data Science learning materials, please check my previous posts.\nI share my learning journey into Data Science with my amazing LinkedIn friends, please follow me and let's grow together!\nAlex Wang\nhashtag\n#\ndatascience\nhashtag\n#\ndataanalysis"
        },
        "Post_170": {
            "text": "Smallest chef in the world makes your food in front of you. Would you like to go?\nImmersive dining experience from Le Petit Chef utilising 3D mapping and projection technology. 35 locations across the world\nFive-course meal will set you back Â£50-110 depending on location.\nMore info:\nhttps://lepetitchef.com/\n>>> We need more innovation everywhere <<<\nWeekly innovation newsletter sign up:\nhttps://lnkd.in/gkMMwdws\nhashtag\n#\nfood\nhashtag\n#\nimmersiveexperience\nhashtag\n#\ninnovation"
        },
        "Post_171": {
            "text": "Check out this Python syntactic sugar method that could benefit you in producing cleaner code.\nğŸ‘‡\nğŸ“ƒ In theÂ standard fashion, we must loop through the whole list when we want to find an object. When we see it, we break out of the loop. To keep the state of the query, we create a flag. At the end of the for loop, if we do not find the query object, we perform a specific action.\nğŸ“ƒ You can compress this logic into a single \"else\" statement using theÂ Python syntactic sugar. We wrote the for loop in the standard way. If we find the query object and hit the break statement, we won't execute the \"else\" block. Otherwise, we will complete the \"else\" block if we don't reach the break statement.\nâœ… I think this is a fantastic way to write fewer lines of code. Therefore, less code to manage.\nâŒ But, I also think your code can quickly get obscure and hard to read if you abuse this method.\nâ” What is your opinion about this syntactic sugar method?\nhashtag\n#\npython\nhashtag\n#\ncleancode\nhashtag\n#\nprogramming\nhashtag\n#\nlearn"
        },
        "Post_172": {
            "text": "Agari biodegradable bottles  ğŸŒ±\nWe need more of such innovations. What do you think?\nInvented by product designer Ari Jonsonn\nhashtag\n#\ninnovation\nhashtag\n#\ntech\nhashtag\n#\ngreen\nhashtag\n#\nsustainable\nhashtag\n#\ngreentech"
        },
        "Post_173": {
            "text": "ğŸ‘€ I have recently read a blog on Udacity about how to actively find jobs that you are passionate about, aka your dream job.\nğŸ“ƒ They present a method called the Visionary Approach. It consists of four main steps:\n1. Choose your target company (domain, size, etc.);\n2. Research as much as possible about it (both the company and the people that work at it). Build a targeted network;\n3. Learn where you can fit into that companies plan and build/learn something valuable to them;\n4. Show off your new expertise.\nâœ¨ To me was very insightful.\nâ”What is your opinion about this approach?\nLink to the blog post:\nhttps://lnkd.in/dHNqCzpM\nhashtag\n#\nwork\nhashtag\n#\njob\nhashtag\n#\nreading\nhashtag\n#\nselfgrowth\nhashtag\n#\ninsights"
        },
        "Post_174": {
            "text": "Moving from BI to AI can be easy with CoreAI.\nWe build AI pipeline and give your organization and data analysts a head start, as well as becoming independent in generating more accurate and up to date models later.\nIf you want to stay up-to-date with us, follow our Page.\nThis is a short clip showing how:\nhttps://lnkd.in/erxh8BKg"
        },
        "Post_175": {
            "text": "Machine Learning in action ğŸ¤£\nhashtag\n#\nHappyWeekend\nCredit: @alvinfoo"
        },
        "Post_176": {
            "text": "What are you waiting for?\nThat's the fundamental question\nhashtag\n#\nWarrenBuffett\nposes in this amazing clip. And the honest answer for many aspiring business owners is fear of the unknown.\n\"I just don't know where to begin.\"\nWell, I have some exciting news today that goes right to the heart of this dilemma!\nI'll be the host and an executive producer of a new unscripted series on CNBC called â€œBusiness Huntersâ€ that will guide everyday Americans through the journey of buying their own small business.Â There's simply never been a show like this before.Â  Â We willÂ pull back the curtain on how to find the right business within budget, breakdown the numbers, and negotiate a deal â€“ all while revealing the joy and sacrifices in making the transition from employee to employer.Â ToÂ navigate the process from beginning to end,Â I'll be drawing upon theÂ expertise of business broker and entrepreneur, Mayumi Muller.\nComing on the heels of the greatest mass exodus of American workers in modern history, â€œBusiness Huntersâ€ is being developed by award-winning producer and Chairman of Worldwide Television at MGM, Mark Burnett and Evolution Media. (See link to press release in the comments)\nOn a personal note, I hope this show will serve as a bridge from thought to action for millions of people around the world who are cradling a dream but are somehow stuck. I know how hard it is to come from nothing, muster the courage to quit the grind, leave your fears behind, and bet on yourself.Â Â I believe â€œBusiness Huntersâ€ will empower a whole new generation of aspiring business owners by demystifying this opaque process.\nI'm so grateful and honored that Mark Burnett and Barry Poznick at MGM, and Mark Hoffman and Denise Contis at CNBC have entrusted me to host this life-changing show.\nWatch out for â€œBusiness Huntersâ€ coming this Fall!\nhashtag\n#\nBusinessHunters\nhashtag\n#\nCNBC\nhashtag\n#\nentrepreneurship"
        },
        "Post_177": {
            "text": "Itâ€™s time we change the conversation ğŸ’›"
        },
        "Post_178": {
            "text": "An alternative way to imagine PCA:\nMapping from one plane to another is basically matrix regression. This might help you create an intuition of PCA.\nhashtag\n#\ndatascience\nhashtag\n#\nmachinelearning\ncredit:\nhttps://lnkd.in/giEeva_B\nby Karl Rohe"
        },
        "Post_179": {
            "text": "Someone just sent $300,000,000 of\nhashtag\n#\nBitcoin\nin a single transaction.\nTransaction fee: $0.12\nNo government, bank, or third party could have stopped this if they wanted to.\nThis is the power of Bitcoin."
        },
        "Post_180": {
            "text": "Have you ever wondered how GitHub Copilot ğŸ¤– is performing? Then check out this article where I share my coding experience with it.\nI think it is a great addition to my developing tool stack ğŸ¦¾.\nhashtag\n#\npython\nhashtag\n#\ndeveloper\nhashtag\n#\nsoftwareengineer\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence\nhashtag\n#\npycharm"
        },
        "Post_181": {
            "text": "An excellent illustration of how Convolutional Neural Networks (CNN) work\nFrom one picture, you end up with four understandable images. The reduction in resolution is a fundamental step in CNN to accelerate processing time\nCredit: Kensuke Koike\nhashtag\n#\nartificialintelligence"
        },
        "Post_182": {
            "text": "The data-Centric approach is taking over the AI development world!\nOne of its key elements is MLOPS.\nIn this blog post, we shared some of our experiences of how important it is!\nNot only for enterprises but small, medium, and startup companies."
        },
        "Post_183": {
            "text": "How cool is the live wallpaper - yes or no?"
        },
        "Post_184": {
            "text": "La carte de visite du futur !\nPar å·ç”°åå¤¢ :\nbit.ly/37nbxGx"
        },
        "Post_185": {
            "text": "Our brains are wired to compare. From the Predictably Irrational videobook by\nLIT Videobooks\nWatch more at\nhttps://lnkd.in/dwpdUDPy"
        },
        "Post_186": {
            "text": "Back in my day...\n.\n.\n.\n.\n.\n.\n.\nOriginal: Mundher Alshabi"
        },
        "Post_187": {
            "text": "ğŸ’¡ A sealed class cannot be inherited. Any attempt to do so will result in a compile error.\nğŸ‘‰ You can still attach additional behavior to immutable classes via extension methods, but you're limited to their public members in your implementation (unless both the class and the extension live in the same assembly, which gives you access to internal members as well).\nhashtag\n#\ndotnet\nhashtag\n#\ncsharp"
        },
        "Post_188": {
            "text": "I started using GitHub Copilot a few days ago.\nIt is a great addition to your IDE for your daily coding hours.\nI tested it by writing some Pandas code in Python. After using it for a few hours it picked up on my coding style. It is a lot more than just an autocompletion tool. I just typed a few letters and it suggested the whole line of code, with logic and syntax sugar. Of course, the great part is that its suggestions are also correct. You can accept the autocomplete with the tab key.\nSay goodbye to copy-paste from StackOverflow and hello to the tab key.\nhashtag\n#\npython\nhashtag\n#\npandas\nhashtag\n#\ncoding\nhashtag\n#\ngithubcopilot"
        },
        "Post_189": {
            "text": "LED fan holographic projection\nAmazing technology"
        },
        "Post_190": {
            "text": "We spend too much time thinking about the black lines, and not enough on the green ones.\nItâ€™s never too late to make a career pivot or learn something new.\nWhatâ€™s the secret?\nâœ… Learn about â€œnew pathsâ€ - follow people in new fields, watch videos, read more\nâœ… Surround yourself with people who encourage and empower you\nâœ… Believe in yourself, even when people say otherwise\nFollow me on Instagram for more ğŸ‘‰\nhttps://lnkd.in/e2jEHwk\nImage source:\nTim Urban"
        },
        "Post_191": {
            "text": "Check out this Medium article about a short introduction to Yacht. A Python framework based on Machine Learning for automating your orders, as buy and hold investors, into the stock market.\nhashtag\n#\ntradingbot\nhashtag\n#\nreinforcementlearning\nhashtag\n#\ndeeplearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nfinance"
        },
        "Post_192": {
            "text": "Penguins at 5x speed. I have no idea why, but at this late hour, I find this both hilarious and mesmerizing. Earth life is weird and beautiful."
        },
        "Post_193": {
            "text": "Came across this wonderful video posted by\nThi Hien Nguyen\n.\nA perspective is just a partial interpretation of reality. Not only in life, it is very much applicable in Data Science too ğŸ˜ƒ\nWhenever I come across stats being used with *absolute* certainty, I always remind myself that Statistics is not Arithmetic. 2 + 2 is not always 4.\nInterpreting data is much more nuanced. Data aggregations or visualizations have to be interpreted with care.\nAlways think: what is hiding in plain sight.\nCheck out the resources in the first comment.\nğŸ‘‰ For more content on Machine Learning for Developers, subscribe to\nhashtag\n#\nML4Devs\nnewsletter.\nhashtag\n#\nBigData\nhashtag\n#\nDataEngineering\nhashtag\n#\nDataScience\nhashtag\n#\nMachineLearning\nhashtag\n#\nMLOps\nhashtag\n#\nCloudComputing\nhashtag\n#\nMicroservices"
        },
        "Post_194": {
            "text": "hashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nfinance"
        },
        "Post_195": {
            "text": "Check out my Medium article about how to make time series stationary without losing memory."
        },
        "Post_196": {
            "text": "Fractionally Differentiated Features to Preserve Memory in Stationary Time Series\nHow to Properly Preprocess Data for Machine Learning Trading Bots\nby\nPaul Iusztin\nğŸ”µ\nhttps://lnkd.in/e5JgTD-2\nAI is everywhere, but the question is ğŸŸ  how much do you love it?\nhttps://lnkd.in/dd3EKnfv\nhashtag\n#\nart\nhashtag\n#\nmachinelearning\nhashtag\n#\ndeeplearning\nhashtag\n#\nartificialintelligence\nhashtag\n#\ndatascience\nhashtag\n#\niiot\nhashtag\n#\ndevops\nhashtag\n#\ndata\nhashtag\n#\nMLsoGood\nhashtag\n#\ncode\nhashtag\n#\npython\nhashtag\n#\nbigdata\nhashtag\n#\nMLart\nhashtag\n#\nalgorithm\nhashtag\n#\nprogrammer\nhashtag\n#\npytorch\nhashtag\n#\nDataScientist\nhashtag\n#\nAnalytics\nhashtag\n#\nAI\nhashtag\n#\nVR\nhashtag\n#\niot\nhashtag\n#\nTechCult\nhashtag\n#\nDigitalart\nhashtag\n#\nDigitalArtMarket\nhashtag\n#\nlearning\nhashtag\n#\nml\nhashtag\n#\ntrading"
        },
        "Post_197": {
            "text": "The carbon is pumped deep underground, where it turns into rock.\nLearn more about this incredible new plant:\nhttp://ow.ly/e8rj50HwPcR\nhashtag\n#\ndavosagenda"
        },
        "Post_198": {
            "text": "We are working with the zero-code smart contract generation platform MyWish to enable anyone to write their own Elrond smart contracts without any programming skills. The platform offers a number of predefined smart contracts templates that allow you to create your own token, host your own token sale and airdrop, perhaps even get married on the blockchain.\nhashtag\n#\nmission10\nDay 20.\nhttps://lnkd.in/e9KafJ_5"
        },
        "Post_199": {
            "text": "â€œKeep focused on the step in front of you. Nothing else.â€\n~Bear Grylls"
        },
        "Post_200": {
            "text": "Anyone can learn to be a great leader â€“ itâ€™s a practice and the first requirement is you have to want to be one. Our team invites you into our virtual classroom to unpack the concepts in \"Leaders Eat Last.\" Join us:\nhttps://bit.ly/3ecNb2s"
        },
        "Post_201": {
            "text": "Amazing! Researchers from\nUniversity of California, San Francisco\nused\nhashtag\n#\nAI\n+\nNVIDIA\nGPUs to give a paralyzed man the ability to communicate by translating his brain signals into computer-generated writing. Read more:\nhttps://nvda.ws/3exANKm"
        },
        "Post_202": {
            "text": "Amazing innovation\nI am ready to hit the road - would you?"
        },
        "Post_203": {
            "text": "Surgical robots are coming.\nUK robotics start-up CMR Surgical just raised $600m, the largest funding round in MedTech history. ğŸ”¥\nTheir mission?\nTo make keyhole surgery universally accessible and affordable through next-gen surgical robots.\nDemand for robotic surgery is growing rapidly, and CMR partners with hospitals around the world to provide surgeons with unprecedented precision and accuracy through robot assistants.\nThey launched their first system, Versius (seen below), back in 2019, and their pioneering team in Cambridge already counts 700 people. This new funding will help them scale globally.\nHuge congrats to the whole team at CMR. ğŸ‘\nhashtag\n#\ntechnology\nhashtag\n#\ninnovation\nhashtag\n#\nmedtech"
        },
        "Post_204": {
            "text": "The Pena Palace in Sintra, Portugal ğŸ°\nğŸ“·: DoublekickOnesnare"
        },
        "Post_205": {
            "text": "A powerful picture, You Change yourself and nobody will do this for youğŸ’ª\nhashtag\n#\njerinlovescreatures\nPC ğŸ‘‰ FB"
        },
        "Post_206": {
            "text": "Ego is a drug. Use in moderation, if at all."
        },
        "Post_207": {
            "text": "World of Warcraft and Fortnite have been played for over 141 billion hours. Maybe Earth is an online video game played by an alien civilization, each human controlled by an alien gamer. Population of Earth increases to account for the growing popularity of the game."
        },
        "Post_208": {
            "text": "Companies\n-2019 You NEED to be in office.\n-2020 Please work from home.\n-2021 You NEED to come back to office.\nGreat Companies\n-2019 You CAN work from home 1-2 days a week.\n-2020 Please work from home.\n-2021 work from where it WORKS for you.\nFlexibility to work from home is going to be the key difference in the coming years.\nAfter 2020, if companies are not able to trust their employees, they will NEVER be.\nGreat work cultures are built on TRUST.\nAgree?\nhashtag\n#\nworkfromhome\nhashtag\n#\ncareer\nhashtag\n#\nculture"
        },
        "Post_209": {
            "text": "Iâ€™ve never seen a set of Machine Learning Cheat Sheets look so cool and clearly explained. You need to keep those handy!\nThanks to\nAqeel Anwar\n, whoâ€™s keeping these doc versions updated, for creating them.\nSubjects include:\nğŸ”¹Bias Variance Trade-off\nğŸ”¹Imbalanced Data in Classification\nğŸ”¹Principal Component Analysis\nğŸ”¹Bayesâ€™ Theorem and Classifier\nğŸ”¹Regression Analysis\nğŸ”¹Regularization in ML\nğŸ”¹Convolutional Neural Network\nğŸ”¹Famous CNNs\nğŸ”¹Ensemble Methods in Machine Learning\nğŸ”¹STAR Method for Behavioral Interview prep\nğŸ”¹How to Answer Behavioral Questions\nHope you enjoy!\nhashtag\n#\ndatascience\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence\nClick\nhashtag\n#\nlinkedangle\nand follow for more content\nFollow\nGreg Coquillo"
        },
        "Post_210": {
            "text": "Your brain works differently when you take breaks ğŸ§ âš ï¸\nMicrosoft researched the impact of back-to-back video call meetings on our stress levels. The cool blues show consistent levels with breaks - the warmer reds show increasing stress levels with no breaks ğŸš¨\nThe takeaways ğŸ‘‡ğŸ½\n1ï¸âƒ£ Breaks between meetings allow the brain to â€œreset,â€ reducing a cumulative buildup of stress across meetings.\n2ï¸âƒ£ Back-to-back meetings can decrease your ability to focus and engage.\n3ï¸âƒ£ Transitioning between meetings can be a source of high stress.\nMy best ideas come when I'm not in meetings or in an office. I've taken up DJ-ing and gymming every day to get away from my laptop and honestly, it's when I have my best ideas.\nHave you found your stress levels increased during the last year of virtual meetings? and how do you spend your 'breaks'? ğŸ¤¯"
        },
        "Post_211": {
            "text": "Source:\nhttps://lnkd.in/ejFRgYA"
        },
        "Post_212": {
            "text": "Check out part two in this series on leveraging\nMathWorks\nto support workflows on NVIDIA DRIVE Sim, powered by\nNVIDIA Omniverse\n:\nhttps://nvda.ws/3tJd7rf"
        },
        "Post_213": {
            "text": "Electromyography has the potential to revolutionize how we interact with computers.\nImagine typing without a keyboard, controlling visual interfaces without a mouse, and navigating AR/VR environments much more intuitively.\nThe signals in our wrists are so clear that EMG can detect finger motion of just a millimeter.\nThis video highlights some of the groundbreaking research by the incredible CTRL-Labs team who joined us last year. ğŸ¤©\nMind-blowing potential.\nhashtag\n#\ninnovation\nhashtag\n#\ntechnology\nhashtag\n#\nfacebook"
        },
        "Post_214": {
            "text": "Come to see how you can be a part of a place of innovation, with projects like:\nâ¡ï¸\nlek3.co\n, charging electric cars anywhere. Quick and easy.\nâ¡ï¸\n2park.io\n, smart parking at your fingertips and\nâ¡ï¸ SafeFleet, innovating the telematics industry\nWe're waiting for you! :)\nâ¡ï¸ â¡ï¸ â¡ï¸\nhttps://lnkd.in/g-sEMiW\nhashtag\n#\niot\nhashtag\n#\ndesign\nhashtag\n#\nfuture\nhashtag\n#\nhr\nhashtag\n#\nteam\nhashtag\n#\nsmartcities\nhashtag\n#\nprojects\nhashtag\n#\nelectriccars\nhashtag\n#\ntelematics\nhashtag\n#\nparks\nhashtag\n#\nfuturism\nhashtag\n#\ncharging\nhashtag\n#\nsoftware\nhashtag\n#\nprojecting\nhashtag\n#\nsafefleet\nhashtag\n#\nrecruting\nhashtag\n#\nlek3\nhashtag\n#\nlek3co"
        }
    }
}